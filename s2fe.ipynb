{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 21:58:25.601012: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLEASE EDIT configuration.txt BEFORE EXECUTION\n",
      ".wav files might be generated in path. The program will automatically delete them. If execuetion stops unexpectedly, please delete them yourself\n",
      "Video dataset at /home/gamal/Datasets/Dataset1/Video\n",
      "Number of cpus to use for multiprocessing :  8\n",
      "------------------- ABOUT TO START --------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "# Loading configurations\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "\n",
    "insert_amd_env_vars =  int(configParser.get('COMMON', 'insert_amd_env_vars'))\n",
    "HSA_OVERRIDE_GFX_VERSION =  configParser.get('COMMON', 'HSA_OVERRIDE_GFX_VERSION')\n",
    "ROCM_PATH =  configParser.get('COMMON', 'ROCM_PATH')\n",
    "\n",
    "if(insert_amd_env_vars != 0):\n",
    "    os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = HSA_OVERRIDE_GFX_VERSION\n",
    "    os.environ[\"ROCM_PATH\"] = ROCM_PATH\n",
    "\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import pathlib\n",
    "import configparser\n",
    "import sqlite3 as sl\n",
    "import cv2\n",
    "from pydub import AudioSegment\n",
    "import math\n",
    "import pickle\n",
    "import shutil\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Process,Queue\n",
    "import itertools\n",
    "from threading import Thread\n",
    "import soundfile as sf\n",
    "from deepface import DeepFace\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "\n",
    "start_time = time.time()    # To measure execution time in seconds\n",
    "\n",
    "\n",
    "print(\"PLEASE EDIT configuration.txt BEFORE EXECUTION\")\n",
    "print(\".wav files might be generated in path. The program will automatically delete them. If execuetion stops unexpectedly, please delete them yourself\")\n",
    "\n",
    "\n",
    "\n",
    "datasetPathVideo =  configParser.get('COMMON', 'datasetPathVideo')\n",
    "datasetPathAudio =  configParser.get('extractAudio', 'datasetPathAudio')\n",
    "p =  configParser.get('extractAudio', 'dbChunk')\n",
    "ttwbdf =  int(configParser.get('extractAudio', 'time_to_wait_before_deleting_files'))\n",
    "cuda =  int(configParser.get('COMMON', 'cuda'))\n",
    "cpus =  int(configParser.get('COMMON', 'cpus'))\n",
    "datasetPathDatabase =  configParser.get('COMMON', 'datasetPathDatabase') + '/dataset.db'\n",
    "model_weights_path =  configParser.get('train_s2fe', 'model_weights_path')\n",
    "\n",
    "# TODO dataset.db in configuration too\n",
    "\n",
    "print(\"Video dataset at \" + datasetPathVideo )\n",
    "print(\"Number of cpus to use for multiprocessing : \", cpus)\n",
    "\n",
    "\n",
    "  # Connection to databases\n",
    "con = sl.connect(datasetPathDatabase,check_same_thread=False)\n",
    "print('------------------- ABOUT TO START --------------------')\n",
    "\n",
    "REQUIRED_SAMPLE_RATE = 16000\n",
    "FACE_EMBEDDING_SIZE = 2622\n",
    "AUDIO_MAX_LEN = 246000\n",
    "len_to_input = 16000\n",
    "NO_OF_VIDEOS = int(configParser.get('train_s2fe', 'no_of_videos'))\n",
    "validation_split = float(configParser.get('train_s2fe', 'validation_split')) \n",
    "BATCH_SIZE = int(configParser.get('train_s2fe', 'batch_size'))\n",
    "save_freq = int(configParser.get('train_s2fe', 'save_freq'))\n",
    "no_of_epochs = int(configParser.get('train_s2fe', 'no_of_epochs'))\n",
    "\n",
    "\n",
    "def read_audio_file(file_path):\n",
    "  with open(file_path, \"rb\") as f:\n",
    "      audio_wave, sample_rate = sf.read(f)\n",
    "  if sample_rate != REQUIRED_SAMPLE_RATE:\n",
    "      raise ValueError(\n",
    "          f\"sample rate (={sample_rate}) of your files must be {REQUIRED_SAMPLE_RATE}\"\n",
    "      )\n",
    "  return audio_wave\n",
    "\n",
    " \n",
    "def extractAudio(row):\n",
    "    absPathVideo = row[0][1]   # for this one video\n",
    "\n",
    "    absPathAudio = y = absPathVideo.replace(datasetPathVideo,datasetPathAudio)  # for this one audio\n",
    "    absPathAudio = os.path.splitext(absPathAudio)[0]\n",
    "    absPathAudio_w = absPathAudio   # without the end\n",
    "    absPathAudio = absPathAudio + \"_audio.wav\"  # full path to extracted audio from the video\n",
    "\n",
    "    #Create Directory\n",
    "    pathlib.Path(os.path.dirname(absPathAudio)).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    # Extract audio monochannel and with 16khz and put it in absPathAudio\n",
    "    command = \"ffmpeg -nostats -loglevel 0 -y -i '\" + absPathVideo + \"' -acodec pcm_s16le -ab 160k -ac 1 -ar 16000 -vn '\" + absPathAudio + \"'\"\n",
    "    subprocess.call(command, shell=True)\n",
    "\n",
    "\n",
    "    # Get original duration of video\n",
    "    audio = AudioSegment.from_file(absPathVideo)\n",
    "    audio_length_og = math.floor(audio.duration_seconds)\n",
    "    #print(audio_length_og)\n",
    "    \n",
    "\n",
    "\n",
    "    # Will either truncate or loop the original video to reach audio_length (3,6,12 or 24)\n",
    "    audio_length_list = [6,12,24]\n",
    "    for audio_length in audio_length_list:\n",
    "        path_var_len_audio =  absPathAudio_w + \"audio\" + str(audio_length) + \"s.wav\"    # path to the variable length audio\n",
    "        path_var_len_audio_temp =  absPathAudio_w + \"audio_temp\" + str(audio_length) + \"s.wav\"  # path to a temp version of the variable length audio\n",
    "\n",
    "        if(audio_length_og > audio_length):\n",
    "            # Truncate    \n",
    "\n",
    "            command = \"ffmpeg -nostats -loglevel 0 -y -ss 0 -t \"+str(audio_length)+\" -i \\\"\" + absPathAudio + \"\\\" \\\"\" + path_var_len_audio + \"\\\"\"\n",
    "            subprocess.call(command, shell=True)\n",
    "\n",
    "\n",
    "        else:\n",
    "            # Loop then truncaate\n",
    "            #print(\"lesa\")\n",
    "            twoDigitLenStr = f\"{audio_length:02}\"\n",
    "            #print(twoDigitLenStr)\n",
    "            command = \"ffmpeg -nostats -loglevel 0 -y -stream_loop -1 -i '\" + absPathAudio + \"' -t \\\"00:00:\"+twoDigitLenStr+\".000\\\" -codec:a \\\"aac\\\" -f \\\"wav\\\" -c copy '\"+ path_var_len_audio_temp + \"'\"\n",
    "            subprocess.call(command, shell=True)\n",
    "            command = \"ffmpeg -nostats -loglevel 0 -y -ss 0 -t \"+str(audio_length)+\" -i \\\"\" + path_var_len_audio_temp + \"\\\" \\\"\" + path_var_len_audio + \"\\\"\"\n",
    "            subprocess.call(command, shell=True)\n",
    "\n",
    "\n",
    "\n",
    "            # Will delete those files after a little bit\n",
    "        ftd = [absPathAudio,path_var_len_audio,os.path.basename(path_var_len_audio),path_var_len_audio_temp]\n",
    "        audio_wave = read_audio_file(path_var_len_audio)\n",
    "        \n",
    "        tDelete = Thread(target=delFiles, args=(ftd,))   # spawn a process\n",
    "        tDelete.start()\n",
    "    return audio_wave\n",
    "        \n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "# Function to delete audio temp files\n",
    "def delFiles(filesToDelete):\n",
    "    time.sleep(ttwbdf)  # wait a bit\n",
    "    for file in filesToDelete:  \n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except OSError:\n",
    "            pass\n",
    "        \n",
    "\n",
    "\n",
    "from random import randint\n",
    "from scipy.fft import fft, ifft\n",
    "\n",
    "def get_video(offset):\n",
    "    \n",
    "    try:\n",
    "        #print('offset:' + str(offset))\n",
    "        #print('offset:' + str(offset))\n",
    "        data = con.execute(\"\"\"SELECT V.ID, V.VIDEO_PATH, F.FACE_PATH FROM VIDEO V \n",
    "                        INNER JOIN FACE F ON F.ID = V.ID\n",
    "                        LIMIT 1 OFFSET \"\"\" + str(offset))\n",
    "\n",
    "\n",
    "        \n",
    "        #print(data.fetchall())\n",
    "        dataGotten = data.fetchall()\n",
    "        if (len(dataGotten)) == 0:\n",
    "            raise ValueError('No video was fetched')\n",
    "\n",
    "        #print(dataGotten[0][0])\n",
    "        audio_wave = extractAudio(dataGotten)\n",
    "        audio_wave = audio_wave[0:len_to_input]\n",
    "        audio_wave = tf.constant(audio_wave, dtype=tf.float64)\n",
    "\n",
    "        #audio = tf.squeeze(audio_wave, axis=-1)\n",
    "        fft = tf.signal.fft(\n",
    "            tf.cast(tf.complex(real=audio_wave, imag=tf.zeros_like(audio_wave)), tf.complex64)\n",
    "        )\n",
    "        #fft = tf.expand_dims(fft, axis=-1)\n",
    "        #fft = tf.squeeze(fft, axis=1)\n",
    "        #print()\n",
    "        fft = fft[0:(audio_wave.shape[0] // 2)]\n",
    "        fft = tf.math.abs(fft)\n",
    "        #fft = tf.expand_dims(fft, axis=-1)\n",
    "\n",
    "        embedding_objs = DeepFace.represent(dataGotten[0][2],enforce_detection=False)\n",
    "        return fft,tf.constant(embedding_objs[0]['embedding'], dtype=tf.float64)\n",
    "    except Exception as e:\n",
    "        print('Error getting video, retrying ...:' + str(offset))\n",
    "        return get_video(randint(1, NO_OF_VIDEOS - 1))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8000,), dtype=float32, numpy=\n",
       " array([2.6001568 , 2.2688131 , 2.3923457 , ..., 0.33533525, 0.51442325,\n",
       "        0.63863385], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2622,), dtype=float64, numpy=\n",
       " array([ 0.01234863,  0.0142761 ,  0.00426896, ..., -0.00633709,\n",
       "         0.01058111,  0.01097511])>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#np.set_printoptions(threshold=1000000)\n",
    "get_video(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def valid_generator(n):\n",
    "\n",
    "    \n",
    "    n =  int(NO_OF_VIDEOS * (1-validation_split))\n",
    "    # initialize counter\n",
    "    value = -1\n",
    "\n",
    "    \n",
    "    result_array = np.arange(n, NO_OF_VIDEOS)\n",
    "    np.random.shuffle(result_array)\n",
    "\n",
    "    # loop until counter is less than n\n",
    "    while value < len(result_array) - 1:\n",
    "        #print(value)\n",
    "        #print(len(result_array))\n",
    "        value = value + 1\n",
    "        yield get_video(result_array[value])\n",
    "\n",
    "\n",
    "def data_generator(n):\n",
    "\n",
    "    n =  int(NO_OF_VIDEOS * (1-validation_split))\n",
    "    # initialize counter\n",
    "    value = -1\n",
    "\n",
    "    \n",
    "    result_array = np.arange(1, n)\n",
    "    np.random.shuffle(result_array)\n",
    "\n",
    "    # loop until counter is less than n\n",
    "    while value < len(result_array) - 1:\n",
    "        #print(value)\n",
    "        #print(n)\n",
    "        value = value + 1\n",
    "        yield get_video(result_array[value])\n",
    "\n",
    "        # increment the counter\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for x in valid_generator(20):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "SAMPLING_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 8000)]       0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 8000)        32000       ['input[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 8000, 1)      0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 8000, 16)     64          ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 8000, 16)     0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 8000, 16)     784         ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 8000, 16)     32          ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 8000, 16)     0           ['conv1d_2[0][0]',               \n",
      "                                                                  'conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 8000, 16)     0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 4000, 16)     0           ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 4000, 32)     1568        ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 4000, 32)     0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 4000, 32)     3104        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 4000, 32)     544         ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4000, 32)     0           ['conv1d_5[0][0]',               \n",
      "                                                                  'conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 4000, 32)     0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 2000, 32)    0           ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 2000, 64)     6208        ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 2000, 64)     0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 2000, 64)     12352       ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 2000, 64)     0           ['conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 2000, 64)     12352       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 2000, 64)     2112        ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 2000, 64)     0           ['conv1d_9[0][0]',               \n",
      "                                                                  'conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 2000, 64)     0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 1000, 64)    0           ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 1000, 128)    24704       ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 1000, 128)    0           ['conv1d_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 1000, 128)    49280       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 1000, 128)    0           ['conv1d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 1000, 128)    49280       ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 1000, 128)    8320        ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 1000, 128)    0           ['conv1d_13[0][0]',              \n",
      "                                                                  'conv1d_10[0][0]']              \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 1000, 128)    0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 500, 128)    0           ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 500, 128)     49280       ['max_pooling1d_3[0][0]']        \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 500, 128)     0           ['conv1d_15[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 500, 128)     49280       ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 500, 128)     0           ['conv1d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 500, 128)     49280       ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 500, 128)     16512       ['max_pooling1d_3[0][0]']        \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 500, 128)     0           ['conv1d_17[0][0]',              \n",
      "                                                                  'conv1d_14[0][0]']              \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 500, 128)     0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 250, 128)    0           ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling1d (AveragePool  (None, 83, 128)     0           ['max_pooling1d_4[0][0]']        \n",
      " ing1D)                                                                                           \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 10624)        0           ['average_pooling1d[0][0]']      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          2720000     ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 256)         1024        ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          32896       ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 128)         512         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 2622)         338238      ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,459,726\n",
      "Trainable params: 3,442,958\n",
      "Non-trainable params: 16,768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def residual_block(x, filters, conv_num=3, activation=\"relu\"):\n",
    "    # Shortcut\n",
    "    s = keras.layers.Conv1D(filters, 1, padding=\"same\")(x)\n",
    "    for i in range(conv_num - 1):\n",
    "        x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.Activation(activation)(x)\n",
    "    x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "    x = keras.layers.Add()([x, s])\n",
    "    x = keras.layers.Activation(activation)(x)\n",
    "    return keras.layers.MaxPool1D(pool_size=2, strides=2)(x)\n",
    "\n",
    "\n",
    "def build_model(input_shape, num_outputs):\n",
    "    inputs = keras.layers.Input(shape=input_shape, name=\"input\")\n",
    "    inputs2 = keras.layers.BatchNormalization()(inputs)\n",
    "    inputs3 = keras.layers.Reshape(target_shape=(input_shape,1))(inputs2)\n",
    "    \n",
    "    x = residual_block(inputs3, 16, 2)\n",
    "    x = residual_block(x, 32, 2)\n",
    "    x = residual_block(x, 64, 3)\n",
    "    x = residual_block(x, 128, 3)\n",
    "    x = residual_block(x, 128, 3)\n",
    "\n",
    "    x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    outputs = keras.layers.Dense(num_outputs, activation=\"linear\", name=\"output\")(x)\n",
    "\n",
    "    return keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "model = build_model((len_to_input//2), 2622)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model using Adam's default learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(None),  dtype=tf.float64),\n",
    "    tf.TensorSpec(shape=(None), dtype=tf.float64),\n",
    ")\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(data_generator, args=[NO_OF_VIDEOS], \n",
    "                                         output_signature=output_signature)\n",
    "\n",
    "dataset_valid = tf.data.Dataset.from_generator(valid_generator, args=[NO_OF_VIDEOS], \n",
    "                                         output_signature=output_signature)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "dataset_valid = dataset_valid.batch(BATCH_SIZE)\n",
    "dataset_valid = dataset_valid.prefetch(tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_batches = int(NO_OF_VIDEOS / BATCH_SIZE)\n",
    "train_dataset = dataset.take(num_train_batches)\n",
    "valid_dataset = dataset_valid.take(num_train_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load weights, using random ones.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.load_weights(model_weights_path)\n",
    "    print('Loaded weights')\n",
    "except:\n",
    "    print('Could not load weights, using random ones.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath= model_weights_path,\n",
    "    save_weights_only=True,\n",
    "    save_freq = save_freq,\n",
    "\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_accumulator import GradientAccumulateOptimizer\n",
    "LEARNING_RATE = 0.0001\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "optimizer = GradientAccumulateOptimizer(accum_steps=5, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 0.01010858  0.0079855   0.00182851 ... -0.00792103  0.01460108\n",
      "  0.0016924 ], shape=(2622,), dtype=float64)\n",
      "1/1 [==============================] - 0s 105ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.02355498,  0.01846472, -0.05685584, ...,  0.08099563,\n",
       "         0.09888003,  0.04346417]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = get_video(5)\n",
    "print(bb[1])\n",
    "model.predict(tf.expand_dims(bb[0], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 0.01234863  0.0142761   0.00426896 ... -0.00633709  0.01058111\n",
      "  0.01097511], shape=(2622,), dtype=float64)\n",
      "1/1 [==============================] - 0s 78ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.06879631, -0.15874718,  0.00033301, ...,  0.01943459,\n",
       "        -0.03036234,  0.02665952]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = get_video(1)\n",
    "print(bb[1])\n",
    "model.predict(tf.expand_dims(bb[0], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "2/2 [==============================] - 25s 11s/step - loss: 0.0036 - val_loss: 0.0051\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 24s 11s/step - loss: 0.0041 - val_loss: 0.0052\n",
      "Epoch 3/1000\n",
      "1/2 [==============>...............] - ETA: 13s - loss: 0.0030Error getting video, retrying ...:9\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0030 - val_loss: 0.0052\n",
      "Epoch 4/1000\n",
      "1/2 [==============>...............] - ETA: 12s - loss: 0.0034Error getting video, retrying ...:7\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0067 - val_loss: 0.0052\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 25s 11s/step - loss: 0.0035 - val_loss: 0.0052\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 25s 11s/step - loss: 0.0021 - val_loss: 0.0051\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 25s 11s/step - loss: 0.0024 - val_loss: 0.0051\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 25s 11s/step - loss: 0.0020 - val_loss: 0.0051\n",
      "Epoch 9/1000\n",
      "1/2 [==============>...............] - ETA: 13s - loss: 0.0017Error getting video, retrying ...:8\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0016 - val_loss: 0.0050\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0049 - val_loss: 0.0050\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 25s 11s/step - loss: 0.0017 - val_loss: 0.0049\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 25s 11s/step - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 13/1000\n",
      "1/2 [==============>...............] - ETA: 13s - loss: 0.0015Error getting video, retrying ...:16\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0030 - val_loss: 0.0049\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0026 - val_loss: 0.0048\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0016 - val_loss: 0.0047\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 25s 11s/step - loss: 0.0022 - val_loss: 0.0046\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0022 - val_loss: 0.0046\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0020 - val_loss: 0.0045\n",
      "Epoch 19/1000\n",
      "1/2 [==============>...............] - ETA: 13s - loss: 0.0017Error getting video, retrying ...:3\n",
      "2/2 [==============================] - 26s 12s/step - loss: 0.0016 - val_loss: 0.0046\n",
      "Epoch 20/1000\n",
      "1/2 [==============>...............] - ETA: 13s - loss: 0.0013Error getting video, retrying ...:8\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0012 - val_loss: 0.0046\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0016 - val_loss: 0.0047\n",
      "Epoch 22/1000\n",
      "Error getting video, retrying ...:10\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0015 - val_loss: 0.0047\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 25s 11s/step - loss: 0.0012 - val_loss: 0.0048\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0013 - val_loss: 0.0048\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0011 - val_loss: 0.0048\n",
      "Epoch 26/1000\n",
      "Error getting video, retrying ...:10\n",
      "1/2 [==============>...............] - ETA: 13s - loss: 9.7169e-04Error getting video, retrying ...:6\n",
      "2/2 [==============================] - 26s 12s/step - loss: 0.0015 - val_loss: 0.0048\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0018 - val_loss: 0.0048\n",
      "Epoch 28/1000\n",
      "Error getting video, retrying ...:8\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0025 - val_loss: 0.0048\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0016 - val_loss: 0.0048\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 25s 11s/step - loss: 0.0013 - val_loss: 0.0049\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 26s 11s/step - loss: 0.0011 - val_loss: 0.0049\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 24s 11s/step - loss: 9.8503e-04 - val_loss: 0.0049\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 25s 12s/step - loss: 0.0010 - val_loss: 0.0050\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 25s 11s/step - loss: 0.0011 - val_loss: 0.0050\n",
      "Epoch 35/1000\n"
     ]
    }
   ],
   "source": [
    "history_model = model.fit(train_dataset,validation_data = valid_dataset\n",
    "                          , epochs=no_of_epochs,callbacks=[model_checkpoint_callback])\n",
    "history_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fedb2f46c70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
