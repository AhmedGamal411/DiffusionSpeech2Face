{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "# Loading configurations\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "\n",
    "insert_amd_env_vars =  int(configParser.get('COMMON', 'insert_amd_env_vars'))\n",
    "HSA_OVERRIDE_GFX_VERSION =  configParser.get('COMMON', 'HSA_OVERRIDE_GFX_VERSION')\n",
    "ROCM_PATH =  configParser.get('COMMON', 'ROCM_PATH')\n",
    "\n",
    "if(insert_amd_env_vars != 0):\n",
    "    os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = HSA_OVERRIDE_GFX_VERSION\n",
    "    os.environ[\"ROCM_PATH\"] = ROCM_PATH\n",
    "    \n",
    "#import os\n",
    "#os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "#os.environ[\"AUTOGRAPH_VERBOSITY\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import configparser\n",
    "\n",
    "# Loading configurations\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "\n",
    "\n",
    "video_path =  configParser.get('test_imagen','video_path')\n",
    "\n",
    "ttwbdf =  int(configParser.get('test_imagen', 'time_to_wait_before_deleting_files'))\n",
    "\n",
    "\n",
    "cuda =  int(configParser.get('COMMON', 'cuda'))\n",
    "cpus =  int(configParser.get('COMMON', 'cpus'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO document jupyter\n",
    "import pickle\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from textwrap import wrap\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_folder = output_folder = r'imagen_testing_folder' \n",
    "if not os.path.exists(audio_folder):\n",
    "    os.makedirs(audio_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size =  int(configParser.get('COMMON', 'resizeImageTo'))\n",
    "audio_length_used =  configParser.get('test_imagen', 'audio_length_used') \n",
    "model_filename =  configParser.get('test_imagen', 'model_filename') + '_'  + audio_length_used +  's.pt'\n",
    "openl3_mode =  configParser.get('test_imagen', 'openl3_mode')\n",
    "folder =  configParser.get('test_imagen', 'folder')\n",
    "number_of_images =  configParser.get('test_imagen', 'number_of_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "age =  int(configParser.get('test_imagen', 'age'))\n",
    "ethnicity =  str(configParser.get('test_imagen', 'ethnicity'))\n",
    "gender =  str(configParser.get('test_imagen', 'gender'))\n",
    "language =  str(configParser.get('test_imagen', 'language'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGotten = [['1',video_path,age,ethnicity,gender,[],[],language,[]]]\n",
    "df = pd.DataFrame(dataGotten,columns = ['ID','VIDEO_PATH','AGE','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','caption_l','image_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 21:57:09.580038: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-08 21:57:09.603974: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-08 21:57:09.604502: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-08 21:57:10.140878: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "#gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#for gpu in gpus:\n",
    "#  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-07-08 21:57:13 optimizers:54] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo W 2023-07-08 21:57:13 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...\n",
      "Name: SPEAKER_EMB, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v1.9.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../.cache/torch/pyannote/models--pyannote--embedding/snapshots/20b2db779562a3141f5eadd34a0232dbcd56d620/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.0.0+rocm5. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v1.9.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../.cache/torch/pyannote/models--pyannote--embedding/snapshots/20b2db779562a3141f5eadd34a0232dbcd56d620/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.0.0+rocm5. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-07-08 21:57:18 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /manifests/combined_fisher_swbd_voxceleb12_librispeech/train.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    augmentor:\n",
      "      noise:\n",
      "        manifest_path: /manifests/noise/rir_noise_manifest.json\n",
      "        prob: 0.5\n",
      "        min_snr_db: 0\n",
      "        max_snr_db: 15\n",
      "      speed:\n",
      "        prob: 0.5\n",
      "        sr: 16000\n",
      "        resample_type: kaiser_fast\n",
      "        min_speed_rate: 0.95\n",
      "        max_speed_rate: 1.05\n",
      "    num_workers: 15\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2023-07-08 21:57:18 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /manifests/combined_fisher_swbd_voxceleb12_librispeech/dev.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    num_workers: 15\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-07-08 21:57:18 features:291] PADDING: 16\n",
      "[NeMo I 2023-07-08 21:57:18 save_restore_connector:249] Model EncDecSpeakerLabelModel was successfully restored from /home/gamal/.cache/huggingface/hub/models--nvidia--speakerverification_en_titanet_large/snapshots/4e0b2d387a805da7c208b13d5898ee09de8ec1e9/speakerverification_en_titanet_large.nemo.\n",
      "[NeMo I 2023-07-08 21:57:18 cloud:58] Found existing object /home/gamal/.cache/torch/NeMo/NeMo_1.19.0/speakerverification_speakernet/a8330fa516557b963a89ccbf0fcbe2f2/speakerverification_speakernet.nemo.\n",
      "[NeMo I 2023-07-08 21:57:18 cloud:64] Re-using file from: /home/gamal/.cache/torch/NeMo/NeMo_1.19.0/speakerverification_speakernet/a8330fa516557b963a89ccbf0fcbe2f2/speakerverification_speakernet.nemo\n",
      "[NeMo I 2023-07-08 21:57:18 common:913] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-07-08 21:57:18 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /ws/manifests/raid/combined/train_manifest.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    time_length: 8\n",
      "    augmentor:\n",
      "      noise:\n",
      "        manifest_path: /ws/manifests/raid/musan/musan_music_noise_manifest_dur8.json\n",
      "        prob: 0.2\n",
      "        min_snr_db: 5\n",
      "        max_snr_db: 15\n",
      "    num_workers: 4\n",
      "    \n",
      "[NeMo W 2023-07-08 21:57:18 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /ws/manifests/raid/voxceleb/small_manifest.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    time_length: 8\n",
      "    num_workers: 1\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-07-08 21:57:18 features:291] PADDING: 16\n",
      "[NeMo I 2023-07-08 21:57:19 save_restore_connector:249] Model EncDecSpeakerLabelModel was successfully restored from /home/gamal/.cache/torch/NeMo/NeMo_1.19.0/speakerverification_speakernet/a8330fa516557b963a89ccbf0fcbe2f2/speakerverification_speakernet.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/gamal/vsc/DiffusionSpeech2Face/testing_imagen_pyannote_titanet.py\", line 118, in extract_pyannote_titanet_embeddings\n",
      "    emb0 = inference0(path_var_len_audio).data\n",
      "  File \"/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/pyannote/audio/core/inference.py\", line 425, in __call__\n",
      "    return self.slide(waveform, sample_rate, hook=hook)\n",
      "  File \"/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/pyannote/audio/core/inference.py\", line 275, in slide\n",
      "    self.model.example_output,\n",
      "  File \"/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/functools.py\", line 967, in __get__\n",
      "    val = self.func(instance)\n",
      "  File \"/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/pyannote/audio/core/model.py\", line 215, in example_output\n",
      "    return map_with_specifications(\n",
      "  File \"/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/pyannote/audio/utils/multi_task.py\", line 55, in map_with_specifications\n",
      "    return func(*iterables, specifications=specifications)\n",
      "  File \"/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/pyannote/audio/core/model.py\", line 201, in __example_output\n",
      "    _, num_frames, dimension = example_output.shape\n",
      "ValueError: not enough values to unpack (expected 3, got 2)\n"
     ]
    }
   ],
   "source": [
    "from testing_imagen_speechbrain import extract_speechbrain_embeddings\n",
    "from testing_imagen_pyannote_titanet import extract_pyannote_titanet_embeddings\n",
    "import multiprocessing\n",
    "\n",
    "audio_embs =  configParser.get('COMMON', 'audio_embs') \n",
    "from pydub import AudioSegment\n",
    "import math\n",
    "# Get original duration of video\n",
    "audio = AudioSegment.from_file(video_path)\n",
    "audio_length_og = math.floor(audio.duration_seconds)\n",
    "\n",
    "\n",
    "video_filename = os.path.basename(video_path)\n",
    "\n",
    "absPathAudio_w = os.path.abspath(audio_folder) + \"/\" + video_filename\n",
    "absPathAudio = os.path.splitext(absPathAudio_w)[0]+'_audio.wav'\n",
    "absPathAudio_w = os.path.splitext(absPathAudio_w)[0]\n",
    "\n",
    "\n",
    "import subprocess\n",
    "# Extract audio monochannel and with 16khz and put it in absPathAudio\n",
    "\n",
    "command = \"ffmpeg -nostats -loglevel 0 -y -i '\" + video_path + \"' -acodec pcm_s16le -ab 160k -ac 1 -ar 16000 -vn '\" + absPathAudio  + \"'\"\n",
    "subprocess.call(command, shell=True)\n",
    "\n",
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "# Function to delete audio temp files\n",
    "def delFiles(filesToDelete):\n",
    "    time.sleep(ttwbdf)  # wait a bit\n",
    "    for file in filesToDelete:  \n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "\n",
    "# Will either truncate or loop the original video to reach audio_length (6,12 or 24)\n",
    "audio_length_list = [24]\n",
    "for audio_length in audio_length_list:\n",
    "    path_var_len_audio =  absPathAudio_w + \"audio\" + str(audio_length) + \"s.wav\"    # path to the variable length audio\n",
    "    path_var_len_audio_temp =  absPathAudio_w + \"audio_temp\" + str(audio_length) + \"s.wav\"  # path to a temp version of the variable length audio\n",
    "\n",
    "    if(audio_length_og > audio_length):\n",
    "        # Truncate    \n",
    "\n",
    "        command = \"ffmpeg -nostats -loglevel 0 -y -ss 0 -t \"+str(audio_length)+\" -i \\\"\" + absPathAudio + \"\\\" \\\"\" + path_var_len_audio + \"\\\"\"\n",
    "        subprocess.call(command, shell=True)\n",
    "\n",
    "\n",
    "    else:\n",
    "        # Loop then truncaate\n",
    "        #print(\"lesa\")\n",
    "        twoDigitLenStr = f\"{audio_length:02}\"\n",
    "        #print(twoDigitLenStr)\n",
    "        command = \"ffmpeg -nostats -loglevel 0 -y -stream_loop -1 -i '\" + absPathAudio + \"' -t \\\"00:00:\"+twoDigitLenStr+\".000\\\" -codec:a \\\"aac\\\" -f \\\"wav\\\" -c copy '\"+ path_var_len_audio_temp + \"'\"\n",
    "        subprocess.call(command, shell=True)\n",
    "        command = \"ffmpeg -nostats -loglevel 0 -y -ss 0 -t \"+str(audio_length)+\" -i \\\"\" + path_var_len_audio_temp + \"\\\" \\\"\" + path_var_len_audio + \"\\\"\"\n",
    "        subprocess.call(command, shell=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    q = multiprocessing.Queue()\n",
    "\n",
    "    proc = multiprocessing.Process(target=extract_speechbrain_embeddings, args=(q,path_var_len_audio,))\n",
    "    proc.start()\n",
    "    proc.join()\n",
    "\n",
    "\n",
    "\n",
    "    df['SPEAKER_EMB'] = q.get()\n",
    "    print(df['SPEAKER_EMB'].head())\n",
    "    proc = multiprocessing.Process(target=extract_pyannote_titanet_embeddings, args=(q,path_var_len_audio,audio_embs,audio_length,openl3_mode,))\n",
    "    proc.start()\n",
    "    proc.join()\n",
    "\n",
    "    df['AUDIO_EMB'] = q.get()\n",
    "    print(df['SPEAKER_EMB'].head())\n",
    "\n",
    "    \n",
    "\n",
    "    ftd = [absPathAudio,path_var_len_audio,os.path.basename(path_var_len_audio),path_var_len_audio_temp]\n",
    "    tDelete = Thread(target=delFiles, args=(ftd,))   # spawn a process\n",
    "    tDelete.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df[[\"image_path\",\"caption_e\",\"caption_g\",\"caption_l\"]]\n",
    "data_frame = df3\n",
    "data_frame['SPEAKER_EMB'] = df['SPEAKER_EMB']\n",
    "data_frame['AUDIO_EMB'] = df['AUDIO_EMB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speaker_emb_preprocess(speaker_emb2):\n",
    "\n",
    "    speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "    speaker_emb2 = speaker_emb2.squeeze()\n",
    "    speaker_emb2 = np.pad(speaker_emb2, (288), 'constant', constant_values=(0))\n",
    "    speaker_emb2 = np.tile(speaker_emb2, (1, 1))\n",
    "    speaker_emb2 = speaker_emb2 / 200.0\n",
    "    speaker_emb2 = np.array(speaker_emb2).tolist()\n",
    "    return speaker_emb2\n",
    "\n",
    "\n",
    "def audio_emb_preprocess2(speaker_emb2):\n",
    "    speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "    a = speaker_emb2 #np.zeros(shape=(24, 512))\n",
    "    b = np.zeros(shape=(a.shape[0], 768-a.shape[1]))\n",
    "    c = np.zeros(shape=(250-a.shape[0], 768))\n",
    "    arr = np.concatenate((a, b), axis=1)\n",
    "    arr = np.concatenate((arr, c), axis=0)\n",
    "    arr = arr / (1 if(audio_embs == 'wav2vec') else 10 if(audio_embs == 'openl3')  else 1 if(audio_embs == 'pyannoteTitaNet') else 1)\n",
    "    speaker_emb2 = np.array(arr).tolist()\n",
    "    return speaker_emb2\n",
    "\n",
    "from PIL import Image\n",
    "def getImage(face_path):\n",
    "    im = Image.open(face_path)\n",
    "    im.load() # required for png.split()\n",
    "\n",
    "    im2 = Image.new(\"RGB\", im.size, (255, 255, 255))\n",
    "    im2.paste(im, mask=im.split()[3]) # 3 is the alpha channel\n",
    "    im3 = np.array(im2)\n",
    "    im4 = np.rollaxis(im3,2)\n",
    "    return im4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def process_age(age):\n",
    "    if(age > 100 or age < 0):\n",
    "        x = np.zeros(768)\n",
    "        x[767] = 1\n",
    "        return x\n",
    "    else:\n",
    "        x = np.ones(768) * (age / 100.0)\n",
    "        x[767] = 0\n",
    "        return x\n",
    "\n",
    "def process_gender(gender):\n",
    "    if(gender == 'man'):\n",
    "        return np.ones(768)\n",
    "    elif(gender == \"woman\"):\n",
    "        return np.ones(768) * -1\n",
    "    else:\n",
    "        return np.zeros(768)\n",
    "    \n",
    "# TODO\n",
    "def process_ethnicity(eth):\n",
    "    x = np.zeros(768)\n",
    "    if(eth == \"indian\"):\n",
    "        x[0] = 1\n",
    "    elif(eth == \"asian\"):\n",
    "        x[16]=1\n",
    "    elif(eth == \"latino hispanic\"):\n",
    "        x[2]=1\n",
    "    elif(eth == \"black\"):\n",
    "        x[3]=1\n",
    "    elif(eth == \"middle eastern\"):\n",
    "        x[4]=1\n",
    "    elif(eth == \"white\"):\n",
    "        x[5]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x     \n",
    "\n",
    "def process_language(lan):\n",
    "    x = np.zeros(768)\n",
    "    if(lan == \"Arabic\"):\n",
    "        x[0] = 1\n",
    "    elif(lan == \"Portuguese\"):\n",
    "        x[16]=1\n",
    "    elif(lan == \"Romansh_Sursilvan\"):\n",
    "        x[2]=1\n",
    "    elif(lan == \"Japanese\"):\n",
    "        x[3]=1\n",
    "    elif(lan == \"Ukranian\"):\n",
    "        x[4]=1\n",
    "    elif(lan == \"German\"):\n",
    "        x[5]=1   \n",
    "    elif(lan == \"Chinese_China\"):\n",
    "        x[6]=1   \n",
    "    elif(lan == \"Welsh\"):\n",
    "        x[7]=1  \n",
    "    elif(lan == \"English\"):\n",
    "        x[8]=1\n",
    "    elif(lan == \"Kabyle\"):\n",
    "        x[9]=1 \n",
    "    elif(lan == \"Kyrgyz\"):\n",
    "        x[10]=1\n",
    "    elif(lan == \"Georgian\"):\n",
    "        x[11]=1\n",
    "    elif(lan == \"Persian\"):\n",
    "        x[12]=1 \n",
    "    elif(lan == \"French\"):\n",
    "        x[13]=1\n",
    "    elif(lan == \"Interlingua\"):\n",
    "        x[14]=1\n",
    "    elif(lan == \"Swedish\"):\n",
    "        x[15]=1\n",
    "    elif(lan == \"Spanish\"):\n",
    "        x[16]=1 \n",
    "    elif(lan == \"Dhivehi\"):\n",
    "        x[17]=1\n",
    "    elif(lan == \"Kinyarwanda\"):\n",
    "        x[18]=1 \n",
    "    elif(lan == \"Tatar\"):\n",
    "        x[19]=1\n",
    "    elif(lan == \"Hakha_Chin\"):\n",
    "        x[20]=1 \n",
    "    elif(lan == \"Tamil\"):\n",
    "        x[21]=1 \n",
    "    elif(lan == \"Greek\"):\n",
    "        x[22]=1\n",
    "    elif(lan == \"Latvian\"):\n",
    "        x[23]=1 \n",
    "    elif(lan == \"Russian\"):\n",
    "        x[24]=1\n",
    "    elif(lan == \"Breton\"):\n",
    "        x[25]=1\n",
    "    elif(lan == \"Catalan\"):\n",
    "        x[26]=1    \n",
    "    elif(lan == \"Maltese\"):\n",
    "        x[27]=1 \n",
    "    elif(lan == \"Slovenian\"):\n",
    "        x[28]=1    \n",
    "    elif(lan == \"Indonesian\"):\n",
    "        x[29]=1    \n",
    "    elif(lan == \"Dutch\"):\n",
    "        x[30]=1\n",
    "    elif(lan == \"Chinese_Taiwan\"):\n",
    "        x[31]=1 \n",
    "    elif(lan == \"Sakha\"):\n",
    "        x[32]=1 \n",
    "    elif(lan == \"Polish\"):\n",
    "        x[33]=1 \n",
    "    elif(lan == \"Czech\"):\n",
    "        x[34]=1 \n",
    "    elif(lan == \"Romanian\"):\n",
    "        x[35]=1 \n",
    "    elif(lan == \"Mangolian\"):\n",
    "        x[36]=1 \n",
    "    elif(lan == \"Italian\"):\n",
    "        x[37]=1 \n",
    "    elif(lan == \"Chinese_Hongkong\"):\n",
    "        x[38]=1 \n",
    "    elif(lan == \"Estonian\"):\n",
    "        x[39]=1 \n",
    "    elif(lan == \"Basque\"):\n",
    "        x[40]=1 \n",
    "    elif(lan == \"Esperanto\"):\n",
    "        x[41]=1 \n",
    "    elif(lan == \"Frisian\"):\n",
    "        x[42]=1 \n",
    "    elif(lan == \"Turkish\"):\n",
    "        x[43]=1 \n",
    "    elif(lan == \"Chuvash\"):\n",
    "        x[44]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(age,ethnicity,gender,language,speaker_emb,audio_emb):\n",
    "    speaker_emb = np.asarray(speaker_emb, dtype=np.float32)\n",
    "    speaker_emb = speaker_emb.squeeze()\n",
    "    audio_emb = np.asarray(audio_emb, dtype=np.float32)\n",
    "    audio_emb = audio_emb.squeeze()\n",
    "    #print(speaker_emb.shape)\n",
    "    h = np.vstack((age, ethnicity))\n",
    "    h = np.vstack((h, gender))\n",
    "    h = np.vstack((h, language))\n",
    "    h = np.vstack((h, speaker_emb))\n",
    "    h = np.vstack((h, audio_emb))\n",
    "\n",
    "    j = np.zeros(768)\n",
    "    j = np.tile(j,(256-h.shape[0],1))\n",
    "    h = np.vstack((h, j))\n",
    "    #print(h.shape)\n",
    "    return h.tolist()\n",
    "\n",
    "from PIL import Image\n",
    "def process_image_path(path):\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['SPEAKER_EMB'] = data_frame['SPEAKER_EMB'].apply(lambda x: speaker_emb_preprocess(x))\n",
    "data_frame['AUDIO_EMB'] = data_frame['AUDIO_EMB'].apply(lambda x: audio_emb_preprocess2(x))\n",
    "data_frame['caption_a'] = data_frame['caption_a'].apply(lambda x: process_age(x))\n",
    "data_frame['caption_g'] = data_frame['caption_g'].apply(lambda x: process_gender(x))\n",
    "data_frame['caption_l'] = data_frame['caption_l'].apply(lambda x: process_language(x))\n",
    "data_frame['caption_e'] = data_frame['caption_e'].apply(lambda x: process_ethnicity(x))\n",
    "\n",
    "data_frame['INPUT'] = data_frame['SPEAKER_EMB']\n",
    "\n",
    "for index, row in data_frame.iterrows():\n",
    "    x = process_input(data_frame.loc[index,\"caption_a\"],data_frame.loc[index,\"caption_e\"]\n",
    "                        ,data_frame.loc[index,\"caption_g\"],\n",
    "                    data_frame.loc[index,\"caption_l\"],data_frame.loc[index,\"SPEAKER_EMB\"],\n",
    "                    data_frame.loc[index,\"AUDIO_EMB\"])\n",
    "    x = [x]\n",
    "    #AADFS = AADFS\n",
    "    data_frame.loc[index,\"INPUT\"] = x\n",
    "\n",
    "data_frame = data_frame.drop(['caption_e', 'caption_g','caption_l','SPEAKER_EMB','AUDIO_EMB'], axis=1)\n",
    "\n",
    "\n",
    "data_frame['image_path'] = data_frame['image_path'].apply(lambda x: process_image_path(x))\n",
    "\n",
    "input = data_frame['INPUT'].to_numpy()\n",
    "input = np.array([np.array(xi) for xi in input])\n",
    "input[np.isnan(input)] = 0\n",
    "input[input > 10] = 10\n",
    "input[input < -10] = -10\n",
    "\n",
    "output = data_frame['image_path'].to_numpy()\n",
    "output = np.array([np.array(xi) for xi in output])\n",
    "output.squeeze().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "input = torch.from_numpy(input)\n",
    "input = input.to(torch.float)\n",
    "\n",
    "output = torch.from_numpy(output)\n",
    "output = output.to(torch.float)\n",
    "\n",
    "input = input.squeeze()\n",
    "output = output.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = input[None,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_dim =  int(configParser.get('COMMON', 'unet_dim'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from imagen_pytorch import Unet, Imagen, ImagenTrainer\n",
    "from imagen_pytorch.data import Dataset\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = unet_dim,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = 3,\n",
    "    layer_attns = (False, True, True, True),\n",
    "    layer_cross_attns = (False, True, True, True)\n",
    ")\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = unet_dim,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = (2, 4, 8, 8),\n",
    "    layer_attns = (False, False, False, True),\n",
    "    layer_cross_attns = (False, False, False, True)\n",
    ")\n",
    "\n",
    "#unet = Unet(\n",
    "#    dim = 32,\n",
    "#    dim_mults = (1, 2, 4, 8),\n",
    "#    num_resnet_blocks = 1,\n",
    "#    layer_attns = (False, False, False, True),\n",
    "#    layer_cross_attns = False\n",
    "#)\n",
    "\n",
    "# imagen, which contains the unet above\n",
    "\n",
    "#imagen = Imagen(\n",
    "#    unets = unet,\n",
    "#    image_sizes = 32,\n",
    "#    timesteps = 1000\n",
    "#)\n",
    "\n",
    "imagen = Imagen(\n",
    "    unets = (unet1, unet2),\n",
    "    image_sizes = (image_size, image_size),\n",
    "    timesteps = 1000,\n",
    "    cond_drop_prob = 0.1\n",
    ").cuda()\n",
    "\n",
    "trainer = ImagenTrainer(\n",
    "    imagen = imagen,\n",
    "    split_valid_from_train = True # whether to split the validation dataset from the training\n",
    ").cuda()\n",
    "\n",
    "print('Using model file ' + model_filename)\n",
    "trainer.load(model_filename)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "for i in range(number_of_images):\n",
    "    images = trainer.sample(text_embeds=input,stop_at_unet_number=1,batch_size = 1, return_pil_images = True) # returns List[Image]\n",
    "    t = time.time()\n",
    "    images[0].save(folder + '/' + str(t) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78dccc615e96ab04385280185a87a524fe0822daf5dbad2f4bf2e7d7b28366a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
