{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
      "env: ROCM_PATH=/opt/rocm\n"
     ]
    }
   ],
   "source": [
    "%env HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
    "%env ROCM_PATH=/opt/rocm\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"AUTOGRAPH_VERBOSITY\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipyfilechooser import FileChooser\n",
    "import configparser\n",
    "\n",
    "# Loading configurations\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "\n",
    "use_video_in_configuration =  int(configParser.get('fineTuneStableDiffusionTesting', \n",
    "                                                   'use_video_in_configuration'))\n",
    "\n",
    "video_path =  configParser.get('fineTuneStableDiffusionTesting', \n",
    "                                                   'video_path')\n",
    "\n",
    "ttwbdf =  int(configParser.get('fineTuneStableDiffusionTesting', 'time_to_wait_before_deleting_files'))\n",
    "\n",
    "\n",
    "dev_mode =  int(configParser.get('fineTuneStableDiffusionTesting', 'dev_mode'))\n",
    "\n",
    "cuda =  int(configParser.get('COMMON', 'cuda'))\n",
    "cpus =  int(configParser.get('COMMON', 'cpus'))\n",
    "\n",
    "\n",
    "if(use_video_in_configuration == 0):\n",
    "    fc = FileChooser('/')\n",
    "    \n",
    "\n",
    "\n",
    "    # Create and display a FileChooser widget\n",
    "\n",
    "    # Set a file filter patern\n",
    "    fc.filter_pattern = '*.mp4'\n",
    "    display(fc)\n",
    "\n",
    "    # Print the selected filename\n",
    "    print(fc.selected_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(use_video_in_configuration == 0):\n",
    "    video_path = fc.selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO document jupyter\n",
    "import pickle\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from textwrap import wrap\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cpus to use for multiprocessing :  8\n",
      "Will try to use cuda, if no cuda is present please set cuda = 0 in configuration.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of cpus to use for multiprocessing : \", cpus)\n",
    "if(cuda == 0):\n",
    "    print(\"Not using cuda\")\n",
    "else:\n",
    "    print(\"Will try to use cuda, if no cuda is present please set cuda = 0 in configuration.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_folder = output_folder = r'imagen_testing_folder' \n",
    "if not os.path.exists(audio_folder):\n",
    "    os.makedirs(audio_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size =  int(configParser.get('COMMON', 'resizeImageTo'))\n",
    "audio_length_used =  configParser.get('train_imagen', 'audio_length_used') \n",
    "model_filename =  configParser.get('train_imagen', 'model_filename') + '_'  + audio_length_used +  's.pt'\n",
    "openl3_mode =  configParser.get('extractOpenL3', 'openl3_mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGotten = [['1',video_path,'50','black','man',[],[],'English',[]]]\n",
    "df = pd.DataFrame(dataGotten,columns = ['ID','VIDEO_PATH','AGE','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','caption_l','image_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:862.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torchaudio\n",
    "import speechbrain as sb\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "import openl3\n",
    "import openl3\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "import speechbrain as sb\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "\n",
    "model = openl3.models.load_audio_embedding_model(input_repr=\"mel128\", content_type=\"music\",\n",
    "                                                embedding_size=512)\n",
    "\n",
    "\n",
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n",
    "\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import math\n",
    "# Get original duration of video\n",
    "audio = AudioSegment.from_file(video_path)\n",
    "audio_length_og = math.floor(audio.duration_seconds)\n",
    "audio_length_og\n",
    "\n",
    "video_filename = os.path.basename(video_path)\n",
    "\n",
    "absPathAudio_w = os.path.abspath(audio_folder) + \"/\" + video_filename\n",
    "absPathAudio = os.path.splitext(absPathAudio_w)[0]+'_audio.wav'\n",
    "absPathAudio_w = os.path.splitext(absPathAudio_w)[0]\n",
    "\n",
    "\n",
    "import subprocess\n",
    "# Extract audio monochannel and with 16khz and put it in absPathAudio\n",
    "\n",
    "command = \"ffmpeg -nostats -loglevel 0 -y -i '\" + video_path + \"' -acodec pcm_s16le -ab 160k -ac 1 -ar 16000 -vn '\" + absPathAudio  + \"'\"\n",
    "subprocess.call(command, shell=True)\n",
    "\n",
    "import torchaudio\n",
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Function to delete audio temp files\n",
    "def delFiles(filesToDelete):\n",
    "    time.sleep(ttwbdf)  # wait a bit\n",
    "    for file in filesToDelete:  \n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "\n",
    "# Will either truncate or loop the original video to reach audio_length (6,12 or 24)\n",
    "audio_length_list = [24]\n",
    "for audio_length in audio_length_list:\n",
    "    path_var_len_audio =  absPathAudio_w + \"audio\" + str(audio_length) + \"s.wav\"    # path to the variable length audio\n",
    "    path_var_len_audio_temp =  absPathAudio_w + \"audio_temp\" + str(audio_length) + \"s.wav\"  # path to a temp version of the variable length audio\n",
    "\n",
    "    if(audio_length_og > audio_length):\n",
    "        # Truncate    \n",
    "\n",
    "        command = \"ffmpeg -nostats -loglevel 0 -y -ss 0 -t \"+str(audio_length)+\" -i \\\"\" + absPathAudio + \"\\\" \\\"\" + path_var_len_audio + \"\\\"\"\n",
    "        subprocess.call(command, shell=True)\n",
    "\n",
    "\n",
    "    else:\n",
    "        # Loop then truncaate\n",
    "        #print(\"lesa\")\n",
    "        twoDigitLenStr = f\"{audio_length:02}\"\n",
    "        #print(twoDigitLenStr)\n",
    "        command = \"ffmpeg -nostats -loglevel 0 -y -stream_loop -1 -i '\" + absPathAudio + \"' -t \\\"00:00:\"+twoDigitLenStr+\".000\\\" -codec:a \\\"aac\\\" -f \\\"wav\\\" -c copy '\"+ path_var_len_audio_temp + \"'\"\n",
    "        subprocess.call(command, shell=True)\n",
    "        command = \"ffmpeg -nostats -loglevel 0 -y -ss 0 -t \"+str(audio_length)+\" -i \\\"\" + path_var_len_audio_temp + \"\\\" \\\"\" + path_var_len_audio + \"\\\"\"\n",
    "        subprocess.call(command, shell=True)\n",
    "\n",
    "    # Extract speaker embeddings\n",
    "    signal, fs = torchaudio.load(path_var_len_audio)\n",
    "    embeddings = classifier.encode_batch(signal)\n",
    "    embeddingsPickle = pickle.dumps(embeddings.cpu().detach().numpy())\n",
    "\n",
    "    audio, sr = sf.read(path_var_len_audio)\n",
    "\n",
    "    hop_size = -1\n",
    "    if(openl3_mode == 'imagen'):\n",
    "        hop_size = audio_length/250\n",
    "    elif(openl3_mode == 'stable'):\n",
    "        hop_size = 24/50\n",
    "    else:\n",
    "        raise ValueError('openl3_mode in configuration must either be stable or imagen') \n",
    "    \n",
    "    emb, ts = openl3.get_audio_embedding(audio, sr,hop_size=hop_size,verbose=0,model=model)\n",
    "\n",
    "    embeddingsPickle2 = pickle.dumps(emb)\n",
    "\n",
    "    df['SPEAKER_EMB'] = embeddingsPickle\n",
    "    df['AUDIO_EMB'] = embeddingsPickle2\n",
    "\n",
    "    \n",
    "\n",
    "    ftd = [absPathAudio,path_var_len_audio,os.path.basename(path_var_len_audio),path_var_len_audio_temp]\n",
    "    tDelete = Thread(target=delFiles, args=(ftd,))   # spawn a process\n",
    "    tDelete.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df[[\"image_path\",\"caption_e\",\"caption_g\",\"caption_l\"]]\n",
    "data_frame = df3\n",
    "data_frame['SPEAKER_EMB'] = df['SPEAKER_EMB']\n",
    "data_frame['AUDIO_EMB'] = df['AUDIO_EMB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speaker_emb_preprocess(speaker_emb2):\n",
    "\n",
    "    speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "    speaker_emb2 = speaker_emb2.squeeze()\n",
    "    speaker_emb2 = np.pad(speaker_emb2, (288), 'constant', constant_values=(0))\n",
    "    speaker_emb2 = np.tile(speaker_emb2, (1, 1))\n",
    "    speaker_emb2 = speaker_emb2 / 200.0\n",
    "    speaker_emb2 = np.array(speaker_emb2).tolist()\n",
    "    return speaker_emb2\n",
    "\n",
    "\n",
    "def audio_emb_preprocess2(speaker_emb2):\n",
    "    speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "    a = speaker_emb2 #np.zeros(shape=(24, 512))\n",
    "    b = np.zeros(shape=(a.shape[0], 768-a.shape[1]))\n",
    "    c = np.zeros(shape=(250-a.shape[0], 768))\n",
    "    arr = np.concatenate((a, b), axis=1)\n",
    "    arr = np.concatenate((arr, c), axis=0)\n",
    "    arr = arr / 10.0\n",
    "    speaker_emb2 = np.array(arr).tolist()\n",
    "    return speaker_emb2\n",
    "\n",
    "from PIL import Image\n",
    "def getImage(face_path):\n",
    "    im = Image.open(face_path)\n",
    "    im.load() # required for png.split()\n",
    "\n",
    "    im2 = Image.new(\"RGB\", im.size, (255, 255, 255))\n",
    "    im2.paste(im, mask=im.split()[3]) # 3 is the alpha channel\n",
    "    im3 = np.array(im2)\n",
    "    im4 = np.rollaxis(im3,2)\n",
    "    return im4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def process_gender(gender):\n",
    "    if(gender == 'man'):\n",
    "        return np.ones(768)\n",
    "    elif(gender == \"woman\"):\n",
    "        return np.ones(768) * -1\n",
    "    else:\n",
    "        return np.zeros(768)\n",
    "    \n",
    "# TODO\n",
    "def process_ethnicity(eth):\n",
    "    x = np.zeros(768)\n",
    "    if(eth == \"indian\"):\n",
    "        x[0] = 1\n",
    "    elif(eth == \"asian\"):\n",
    "        x[16]=1\n",
    "    elif(eth == \"latino hispanic\"):\n",
    "        x[2]=1\n",
    "    elif(eth == \"black\"):\n",
    "        x[3]=1\n",
    "    elif(eth == \"middle eastern\"):\n",
    "        x[4]=1\n",
    "    elif(eth == \"white\"):\n",
    "        x[5]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x     \n",
    "\n",
    "def process_language(lan):\n",
    "    x = np.zeros(768)\n",
    "    if(lan == \"Arabic\"):\n",
    "        x[0] = 1\n",
    "    elif(lan == \"Portuguese\"):\n",
    "        x[16]=1\n",
    "    elif(lan == \"Romansh_Sursilvan\"):\n",
    "        x[2]=1\n",
    "    elif(lan == \"Japanese\"):\n",
    "        x[3]=1\n",
    "    elif(lan == \"Ukranian\"):\n",
    "        x[4]=1\n",
    "    elif(lan == \"German\"):\n",
    "        x[5]=1   \n",
    "    elif(lan == \"Chinese_China\"):\n",
    "        x[6]=1   \n",
    "    elif(lan == \"Welsh\"):\n",
    "        x[7]=1  \n",
    "    elif(lan == \"English\"):\n",
    "        x[8]=1\n",
    "    elif(lan == \"Kabyle\"):\n",
    "        x[9]=1 \n",
    "    elif(lan == \"Kyrgyz\"):\n",
    "        x[10]=1\n",
    "    elif(lan == \"Georgian\"):\n",
    "        x[11]=1\n",
    "    elif(lan == \"Persian\"):\n",
    "        x[12]=1 \n",
    "    elif(lan == \"French\"):\n",
    "        x[13]=1\n",
    "    elif(lan == \"Interlingua\"):\n",
    "        x[14]=1\n",
    "    elif(lan == \"Swedish\"):\n",
    "        x[15]=1\n",
    "    elif(lan == \"Spanish\"):\n",
    "        x[16]=1 \n",
    "    elif(lan == \"Dhivehi\"):\n",
    "        x[17]=1\n",
    "    elif(lan == \"Kinyarwanda\"):\n",
    "        x[18]=1 \n",
    "    elif(lan == \"Tatar\"):\n",
    "        x[19]=1\n",
    "    elif(lan == \"Hakha_Chin\"):\n",
    "        x[20]=1 \n",
    "    elif(lan == \"Tamil\"):\n",
    "        x[21]=1 \n",
    "    elif(lan == \"Greek\"):\n",
    "        x[22]=1\n",
    "    elif(lan == \"Latvian\"):\n",
    "        x[23]=1 \n",
    "    elif(lan == \"Russian\"):\n",
    "        x[24]=1\n",
    "    elif(lan == \"Breton\"):\n",
    "        x[25]=1\n",
    "    elif(lan == \"Catalan\"):\n",
    "        x[26]=1    \n",
    "    elif(lan == \"Maltese\"):\n",
    "        x[27]=1 \n",
    "    elif(lan == \"Slovenian\"):\n",
    "        x[28]=1    \n",
    "    elif(lan == \"Indonesian\"):\n",
    "        x[29]=1    \n",
    "    elif(lan == \"Dutch\"):\n",
    "        x[30]=1\n",
    "    elif(lan == \"Chinese_Taiwan\"):\n",
    "        x[31]=1 \n",
    "    elif(lan == \"Sakha\"):\n",
    "        x[32]=1 \n",
    "    elif(lan == \"Polish\"):\n",
    "        x[33]=1 \n",
    "    elif(lan == \"Czech\"):\n",
    "        x[34]=1 \n",
    "    elif(lan == \"Romanian\"):\n",
    "        x[35]=1 \n",
    "    elif(lan == \"Mangolian\"):\n",
    "        x[36]=1 \n",
    "    elif(lan == \"Italian\"):\n",
    "        x[37]=1 \n",
    "    elif(lan == \"Chinese_Hongkong\"):\n",
    "        x[38]=1 \n",
    "    elif(lan == \"Estonian\"):\n",
    "        x[39]=1 \n",
    "    elif(lan == \"Basque\"):\n",
    "        x[40]=1 \n",
    "    elif(lan == \"Esperanto\"):\n",
    "        x[41]=1 \n",
    "    elif(lan == \"Frisian\"):\n",
    "        x[42]=1 \n",
    "    elif(lan == \"Turkish\"):\n",
    "        x[43]=1 \n",
    "    elif(lan == \"Chuvash\"):\n",
    "        x[44]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO add age\n",
    "def process_input(ethnicity,gender,language,speaker_emb,audio_emb):\n",
    "    speaker_emb = np.asarray(speaker_emb, dtype=np.float32)\n",
    "    speaker_emb = speaker_emb.squeeze()\n",
    "    audio_emb = np.asarray(audio_emb, dtype=np.float32)\n",
    "    audio_emb = audio_emb.squeeze()\n",
    "    #print(speaker_emb.shape)\n",
    "    h = np.vstack((ethnicity, gender))\n",
    "    h = np.vstack((h, language))\n",
    "    h = np.vstack((h, speaker_emb))\n",
    "    h = np.vstack((h, audio_emb))\n",
    "\n",
    "    j = np.zeros(768)\n",
    "    j = np.tile(j,(256-h.shape[0],1))\n",
    "    h = np.vstack((h, j))\n",
    "    #print(h.shape)\n",
    "    return h.tolist()\n",
    "\n",
    "from PIL import Image\n",
    "def process_image_path(path):\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in data_frame.iterrows():\n",
    "    x = speaker_emb_preprocess(data_frame.loc[index,\"SPEAKER_EMB\"])\n",
    "    x = [x]\n",
    "    data_frame.loc[index,\"SPEAKER_EMB\"] = x\n",
    "\n",
    "#CHECK\n",
    "for index, row in data_frame.iterrows():\n",
    "    x = audio_emb_preprocess2(data_frame.loc[index,\"AUDIO_EMB\"])\n",
    "    x = [x]\n",
    "    #AADFS = AADFS\n",
    "    data_frame.loc[index,\"AUDIO_EMB\"] = x\n",
    "\n",
    "for index, row in data_frame.iterrows():\n",
    "    x = process_gender(data_frame.loc[index,\"caption_g\"])\n",
    "    x = [x]\n",
    "    #AADFS = AADFS\n",
    "    data_frame.loc[index,\"caption_g\"] = x\n",
    "\n",
    "for index, row in data_frame.iterrows():\n",
    "    x = process_language(data_frame.loc[index,\"caption_l\"])\n",
    "    x = [x]\n",
    "    #AADFS = AADFS\n",
    "    data_frame.loc[index,\"caption_l\"] = x\n",
    "    \n",
    "for index, row in data_frame.iterrows():\n",
    "    x = process_ethnicity(data_frame.loc[index,\"caption_e\"])\n",
    "    x = [x]\n",
    "    #AADFS = AADFS\n",
    "    data_frame.loc[index,\"caption_e\"] = x\n",
    "\n",
    "data_frame['INPUT'] = data_frame['SPEAKER_EMB']\n",
    "\n",
    "for index, row in data_frame.iterrows():\n",
    "    x = process_input(data_frame.loc[index,\"caption_e\"],data_frame.loc[index,\"caption_g\"],\n",
    "                    data_frame.loc[index,\"caption_l\"],data_frame.loc[index,\"SPEAKER_EMB\"],\n",
    "                    data_frame.loc[index,\"AUDIO_EMB\"])\n",
    "    x = [x]\n",
    "    #AADFS = AADFS\n",
    "    data_frame.loc[index,\"INPUT\"] = x\n",
    "\n",
    "data_frame = data_frame.drop(['caption_e', 'caption_g','caption_l','SPEAKER_EMB','AUDIO_EMB'], axis=1)\n",
    "\n",
    "for index, row in data_frame.iterrows():\n",
    "    x = process_image_path(data_frame.loc[index,\"image_path\"])\n",
    "    x = [x]\n",
    "    #AADFS = AADFS\n",
    "    data_frame.loc[index,\"image_path\"] = x\n",
    "\n",
    "input = data_frame['INPUT'].to_numpy()\n",
    "input = np.array([np.array(xi) for xi in input])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output = data_frame['image_path'].to_numpy()\n",
    "output = np.array([np.array(xi) for xi in output])\n",
    "output.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "input = torch.from_numpy(input)\n",
    "input = input.to(torch.float)\n",
    "\n",
    "output = torch.from_numpy(output)\n",
    "output = output.to(torch.float)\n",
    "\n",
    "input = input.squeeze()\n",
    "output = output.squeeze()\n",
    "\n",
    "#my_dataset = TensorDataset(output,input) # create your datset\n",
    "#my_dataloader = DataLoader(my_dataset) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n",
      "Using model file imagen_two_unets_24s.pt\n",
      "checkpoint loaded from imagen_two_unets_24s.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95050cb377b740c9a2010cecede964ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02735f373804087998b456661b4d060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mUsing model file \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m model_filename)\n\u001b[1;32m     52\u001b[0m trainer\u001b[39m.\u001b[39mload(model_filename)\n\u001b[0;32m---> 54\u001b[0m images \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49msample(text_embeds\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m[\u001b[39m1\u001b[39;49m],stop_at_unet_number\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,batch_size \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, return_pil_images \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m) \u001b[39m# returns List[Image]\u001b[39;00m\n\u001b[1;32m     55\u001b[0m images[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mtest.png\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/trainer.py:136\u001b[0m, in \u001b[0;36mcast_torch_tensor.<locals>.inner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m args, kwargs_values \u001b[39m=\u001b[39m all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n\u001b[1;32m    134\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mtuple\u001b[39m(\u001b[39mzip\u001b[39m(kwargs_keys, kwargs_values)))\n\u001b[0;32m--> 136\u001b[0m out \u001b[39m=\u001b[39m fn(model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/trainer.py:196\u001b[0m, in \u001b[0;36mimagen_sample_in_chunks.<locals>.inner\u001b[0;34m(self, max_batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[1;32m    194\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, max_batch_size \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    195\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exists(max_batch_size):\n\u001b[0;32m--> 196\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    198\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimagen\u001b[39m.\u001b[39munconditional:\n\u001b[1;32m    199\u001b[0m         batch_size \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/trainer.py:960\u001b[0m, in \u001b[0;36mImagenTrainer.sample\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39muse_tqdm\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    959\u001b[0m \u001b[39mwith\u001b[39;00m context():\n\u001b[0;32m--> 960\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimagen\u001b[39m.\u001b[39;49msample(\u001b[39m*\u001b[39;49margs, device \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    962\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/imagen_pytorch.py:111\u001b[0m, in \u001b[0;36meval_decorator.<locals>.inner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m was_training \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtraining\n\u001b[1;32m    110\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m--> 111\u001b[0m out \u001b[39m=\u001b[39m fn(model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    112\u001b[0m model\u001b[39m.\u001b[39mtrain(was_training)\n\u001b[1;32m    113\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m<@beartype(imagen_pytorch.imagen_pytorch.Imagen.sample) at 0x7fb64c8f6a60>:42\u001b[0m, in \u001b[0;36msample\u001b[0;34m(__beartype_func, __beartype_conf, __beartype_get_violation, __beartype_getrandbits, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/imagen_pytorch.py:2441\u001b[0m, in \u001b[0;36mImagen.sample\u001b[0;34m(self, texts, text_masks, text_embeds, video_frames, cond_images, cond_video_frames, post_cond_video_frames, inpaint_images, inpaint_masks, inpaint_resample_times, init_images, skip_steps, batch_size, cond_scale, lowres_sample_noise_level, start_at_unet_number, start_image_or_video, stop_at_unet_number, return_all_unet_outputs, return_pil_images, device, use_tqdm, use_one_unet_in_gpu)\u001b[0m\n\u001b[1;32m   2437\u001b[0m     \u001b[39m# shape of stage\u001b[39;00m\n\u001b[1;32m   2439\u001b[0m     shape \u001b[39m=\u001b[39m (batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannels, \u001b[39m*\u001b[39mframe_dims, image_size, image_size)\n\u001b[0;32m-> 2441\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample_loop(\n\u001b[1;32m   2442\u001b[0m         unet,\n\u001b[1;32m   2443\u001b[0m         shape,\n\u001b[1;32m   2444\u001b[0m         text_embeds \u001b[39m=\u001b[39;49m text_embeds,\n\u001b[1;32m   2445\u001b[0m         text_mask \u001b[39m=\u001b[39;49m text_masks,\n\u001b[1;32m   2446\u001b[0m         cond_images \u001b[39m=\u001b[39;49m cond_images,\n\u001b[1;32m   2447\u001b[0m         inpaint_images \u001b[39m=\u001b[39;49m inpaint_images,\n\u001b[1;32m   2448\u001b[0m         inpaint_masks \u001b[39m=\u001b[39;49m inpaint_masks,\n\u001b[1;32m   2449\u001b[0m         inpaint_resample_times \u001b[39m=\u001b[39;49m inpaint_resample_times,\n\u001b[1;32m   2450\u001b[0m         init_images \u001b[39m=\u001b[39;49m unet_init_images,\n\u001b[1;32m   2451\u001b[0m         skip_steps \u001b[39m=\u001b[39;49m unet_skip_steps,\n\u001b[1;32m   2452\u001b[0m         cond_scale \u001b[39m=\u001b[39;49m unet_cond_scale,\n\u001b[1;32m   2453\u001b[0m         lowres_cond_img \u001b[39m=\u001b[39;49m lowres_cond_img,\n\u001b[1;32m   2454\u001b[0m         lowres_noise_times \u001b[39m=\u001b[39;49m lowres_noise_times,\n\u001b[1;32m   2455\u001b[0m         noise_scheduler \u001b[39m=\u001b[39;49m noise_scheduler,\n\u001b[1;32m   2456\u001b[0m         pred_objective \u001b[39m=\u001b[39;49m pred_objective,\n\u001b[1;32m   2457\u001b[0m         dynamic_threshold \u001b[39m=\u001b[39;49m dynamic_threshold,\n\u001b[1;32m   2458\u001b[0m         use_tqdm \u001b[39m=\u001b[39;49m use_tqdm,\n\u001b[1;32m   2459\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mvideo_kwargs\n\u001b[1;32m   2460\u001b[0m     )\n\u001b[1;32m   2462\u001b[0m     outputs\u001b[39m.\u001b[39mappend(img)\n\u001b[1;32m   2464\u001b[0m \u001b[39mif\u001b[39;00m exists(stop_at_unet_number) \u001b[39mand\u001b[39;00m stop_at_unet_number \u001b[39m==\u001b[39m unet_number:\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/imagen_pytorch.py:2248\u001b[0m, in \u001b[0;36mImagen.p_sample_loop\u001b[0;34m(self, unet, shape, noise_scheduler, lowres_cond_img, lowres_noise_times, text_embeds, text_mask, cond_images, cond_video_frames, post_cond_video_frames, inpaint_images, inpaint_masks, inpaint_resample_times, init_images, skip_steps, cond_scale, pred_objective, dynamic_threshold, use_tqdm)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     img \u001b[39m=\u001b[39m img \u001b[39m*\u001b[39m \u001b[39m~\u001b[39minpaint_masks \u001b[39m+\u001b[39m noised_inpaint_images \u001b[39m*\u001b[39m inpaint_masks\n\u001b[1;32m   2246\u001b[0m self_cond \u001b[39m=\u001b[39m x_start \u001b[39mif\u001b[39;00m unet\u001b[39m.\u001b[39mself_cond \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2248\u001b[0m img, x_start \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample(\n\u001b[1;32m   2249\u001b[0m     unet,\n\u001b[1;32m   2250\u001b[0m     img,\n\u001b[1;32m   2251\u001b[0m     times,\n\u001b[1;32m   2252\u001b[0m     t_next \u001b[39m=\u001b[39;49m times_next,\n\u001b[1;32m   2253\u001b[0m     text_embeds \u001b[39m=\u001b[39;49m text_embeds,\n\u001b[1;32m   2254\u001b[0m     text_mask \u001b[39m=\u001b[39;49m text_mask,\n\u001b[1;32m   2255\u001b[0m     cond_images \u001b[39m=\u001b[39;49m cond_images,\n\u001b[1;32m   2256\u001b[0m     cond_scale \u001b[39m=\u001b[39;49m cond_scale,\n\u001b[1;32m   2257\u001b[0m     self_cond \u001b[39m=\u001b[39;49m self_cond,\n\u001b[1;32m   2258\u001b[0m     lowres_cond_img \u001b[39m=\u001b[39;49m lowres_cond_img,\n\u001b[1;32m   2259\u001b[0m     lowres_noise_times \u001b[39m=\u001b[39;49m lowres_noise_times,\n\u001b[1;32m   2260\u001b[0m     noise_scheduler \u001b[39m=\u001b[39;49m noise_scheduler,\n\u001b[1;32m   2261\u001b[0m     pred_objective \u001b[39m=\u001b[39;49m pred_objective,\n\u001b[1;32m   2262\u001b[0m     dynamic_threshold \u001b[39m=\u001b[39;49m dynamic_threshold,\n\u001b[1;32m   2263\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mvideo_kwargs\n\u001b[1;32m   2264\u001b[0m )\n\u001b[1;32m   2266\u001b[0m \u001b[39mif\u001b[39;00m has_inpainting \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_last_resample_step \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39mall(is_last_timestep)):\n\u001b[1;32m   2267\u001b[0m     renoised_img \u001b[39m=\u001b[39m noise_scheduler\u001b[39m.\u001b[39mq_sample_from_to(img, times_next, times)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/imagen_pytorch.py:2139\u001b[0m, in \u001b[0;36mImagen.p_sample\u001b[0;34m(self, unet, x, t, noise_scheduler, t_next, text_embeds, text_mask, cond_images, cond_video_frames, post_cond_video_frames, cond_scale, self_cond, lowres_cond_img, lowres_noise_times, pred_objective, dynamic_threshold)\u001b[0m\n\u001b[1;32m   2133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_video:\n\u001b[1;32m   2134\u001b[0m     video_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m   2135\u001b[0m         cond_video_frames \u001b[39m=\u001b[39m cond_video_frames,\n\u001b[1;32m   2136\u001b[0m         post_cond_video_frames \u001b[39m=\u001b[39m post_cond_video_frames,\n\u001b[1;32m   2137\u001b[0m     )\n\u001b[0;32m-> 2139\u001b[0m (model_mean, _, model_log_variance), x_start \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_mean_variance(\n\u001b[1;32m   2140\u001b[0m     unet,\n\u001b[1;32m   2141\u001b[0m     x \u001b[39m=\u001b[39;49m x,\n\u001b[1;32m   2142\u001b[0m     t \u001b[39m=\u001b[39;49m t,\n\u001b[1;32m   2143\u001b[0m     t_next \u001b[39m=\u001b[39;49m t_next,\n\u001b[1;32m   2144\u001b[0m     noise_scheduler \u001b[39m=\u001b[39;49m noise_scheduler,\n\u001b[1;32m   2145\u001b[0m     text_embeds \u001b[39m=\u001b[39;49m text_embeds,\n\u001b[1;32m   2146\u001b[0m     text_mask \u001b[39m=\u001b[39;49m text_mask,\n\u001b[1;32m   2147\u001b[0m     cond_images \u001b[39m=\u001b[39;49m cond_images,\n\u001b[1;32m   2148\u001b[0m     cond_scale \u001b[39m=\u001b[39;49m cond_scale,\n\u001b[1;32m   2149\u001b[0m     lowres_cond_img \u001b[39m=\u001b[39;49m lowres_cond_img,\n\u001b[1;32m   2150\u001b[0m     self_cond \u001b[39m=\u001b[39;49m self_cond,\n\u001b[1;32m   2151\u001b[0m     lowres_noise_times \u001b[39m=\u001b[39;49m lowres_noise_times,\n\u001b[1;32m   2152\u001b[0m     pred_objective \u001b[39m=\u001b[39;49m pred_objective,\n\u001b[1;32m   2153\u001b[0m     dynamic_threshold \u001b[39m=\u001b[39;49m dynamic_threshold,\n\u001b[1;32m   2154\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mvideo_kwargs\n\u001b[1;32m   2155\u001b[0m )\n\u001b[1;32m   2157\u001b[0m noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn_like(x)\n\u001b[1;32m   2158\u001b[0m \u001b[39m# no noise when t == 0\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/imagen_pytorch.py:2069\u001b[0m, in \u001b[0;36mImagen.p_mean_variance\u001b[0;34m(self, unet, x, t, noise_scheduler, text_embeds, text_mask, cond_images, cond_video_frames, post_cond_video_frames, lowres_cond_img, self_cond, lowres_noise_times, cond_scale, model_output, t_next, pred_objective, dynamic_threshold)\u001b[0m\n\u001b[1;32m   2063\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_video:\n\u001b[1;32m   2064\u001b[0m     video_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m   2065\u001b[0m         cond_video_frames \u001b[39m=\u001b[39m cond_video_frames,\n\u001b[1;32m   2066\u001b[0m         post_cond_video_frames \u001b[39m=\u001b[39m post_cond_video_frames,\n\u001b[1;32m   2067\u001b[0m     )\n\u001b[0;32m-> 2069\u001b[0m pred \u001b[39m=\u001b[39m default(model_output, \u001b[39mlambda\u001b[39;49;00m: unet\u001b[39m.\u001b[39;49mforward_with_cond_scale(\n\u001b[1;32m   2070\u001b[0m     x,\n\u001b[1;32m   2071\u001b[0m     noise_scheduler\u001b[39m.\u001b[39;49mget_condition(t),\n\u001b[1;32m   2072\u001b[0m     text_embeds \u001b[39m=\u001b[39;49m text_embeds,\n\u001b[1;32m   2073\u001b[0m     text_mask \u001b[39m=\u001b[39;49m text_mask,\n\u001b[1;32m   2074\u001b[0m     cond_images \u001b[39m=\u001b[39;49m cond_images,\n\u001b[1;32m   2075\u001b[0m     cond_scale \u001b[39m=\u001b[39;49m cond_scale,\n\u001b[1;32m   2076\u001b[0m     lowres_cond_img \u001b[39m=\u001b[39;49m lowres_cond_img,\n\u001b[1;32m   2077\u001b[0m     self_cond \u001b[39m=\u001b[39;49m self_cond,\n\u001b[1;32m   2078\u001b[0m     lowres_noise_times \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlowres_noise_schedule\u001b[39m.\u001b[39;49mget_condition(lowres_noise_times),\n\u001b[1;32m   2079\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mvideo_kwargs\n\u001b[1;32m   2080\u001b[0m ))\n\u001b[1;32m   2082\u001b[0m \u001b[39mif\u001b[39;00m pred_objective \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnoise\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   2083\u001b[0m     x_start \u001b[39m=\u001b[39m noise_scheduler\u001b[39m.\u001b[39mpredict_start_from_noise(x, t \u001b[39m=\u001b[39m t, noise \u001b[39m=\u001b[39m pred)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/imagen_pytorch.py:70\u001b[0m, in \u001b[0;36mdefault\u001b[0;34m(val, d)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m exists(val):\n\u001b[1;32m     69\u001b[0m     \u001b[39mreturn\u001b[39;00m val\n\u001b[0;32m---> 70\u001b[0m \u001b[39mreturn\u001b[39;00m d() \u001b[39mif\u001b[39;00m callable(d) \u001b[39melse\u001b[39;00m d\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/imagen_pytorch.py:2069\u001b[0m, in \u001b[0;36mImagen.p_mean_variance.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2063\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_video:\n\u001b[1;32m   2064\u001b[0m     video_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m   2065\u001b[0m         cond_video_frames \u001b[39m=\u001b[39m cond_video_frames,\n\u001b[1;32m   2066\u001b[0m         post_cond_video_frames \u001b[39m=\u001b[39m post_cond_video_frames,\n\u001b[1;32m   2067\u001b[0m     )\n\u001b[0;32m-> 2069\u001b[0m pred \u001b[39m=\u001b[39m default(model_output, \u001b[39mlambda\u001b[39;00m: unet\u001b[39m.\u001b[39;49mforward_with_cond_scale(\n\u001b[1;32m   2070\u001b[0m     x,\n\u001b[1;32m   2071\u001b[0m     noise_scheduler\u001b[39m.\u001b[39;49mget_condition(t),\n\u001b[1;32m   2072\u001b[0m     text_embeds \u001b[39m=\u001b[39;49m text_embeds,\n\u001b[1;32m   2073\u001b[0m     text_mask \u001b[39m=\u001b[39;49m text_mask,\n\u001b[1;32m   2074\u001b[0m     cond_images \u001b[39m=\u001b[39;49m cond_images,\n\u001b[1;32m   2075\u001b[0m     cond_scale \u001b[39m=\u001b[39;49m cond_scale,\n\u001b[1;32m   2076\u001b[0m     lowres_cond_img \u001b[39m=\u001b[39;49m lowres_cond_img,\n\u001b[1;32m   2077\u001b[0m     self_cond \u001b[39m=\u001b[39;49m self_cond,\n\u001b[1;32m   2078\u001b[0m     lowres_noise_times \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlowres_noise_schedule\u001b[39m.\u001b[39;49mget_condition(lowres_noise_times),\n\u001b[1;32m   2079\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mvideo_kwargs\n\u001b[1;32m   2080\u001b[0m ))\n\u001b[1;32m   2082\u001b[0m \u001b[39mif\u001b[39;00m pred_objective \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnoise\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   2083\u001b[0m     x_start \u001b[39m=\u001b[39m noise_scheduler\u001b[39m.\u001b[39mpredict_start_from_noise(x, t \u001b[39m=\u001b[39m t, noise \u001b[39m=\u001b[39m pred)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/imagen_pytorch.py:1514\u001b[0m, in \u001b[0;36mUnet.forward_with_cond_scale\u001b[0;34m(self, cond_scale, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_with_cond_scale\u001b[39m(\n\u001b[1;32m   1509\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1510\u001b[0m     \u001b[39m*\u001b[39margs,\n\u001b[1;32m   1511\u001b[0m     cond_scale \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m,\n\u001b[1;32m   1512\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m   1513\u001b[0m ):\n\u001b[0;32m-> 1514\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1516\u001b[0m     \u001b[39mif\u001b[39;00m cond_scale \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1517\u001b[0m         \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/imagen_pytorch.py:1606\u001b[0m, in \u001b[0;36mUnet.forward\u001b[0;34m(self, x, time, lowres_cond_img, lowres_noise_times, text_embeds, text_mask, cond_images, self_cond, cond_drop_prob)\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[39m# calculate text embeds\u001b[39;00m\n\u001b[1;32m   1604\u001b[0m text_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_to_cond(text_embeds)\n\u001b[0;32m-> 1606\u001b[0m text_tokens \u001b[39m=\u001b[39m text_tokens[:, :\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_text_len]\n\u001b[1;32m   1608\u001b[0m \u001b[39mif\u001b[39;00m exists(text_mask):\n\u001b[1;32m   1609\u001b[0m     text_mask \u001b[39m=\u001b[39m text_mask[:, :\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_text_len]\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from imagen_pytorch import Unet, Imagen, ImagenTrainer\n",
    "from imagen_pytorch.data import Dataset\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = 32,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = 3,\n",
    "    layer_attns = (False, True, True, True),\n",
    "    layer_cross_attns = (False, True, True, True)\n",
    ")\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = 32,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = (2, 4, 8, 8),\n",
    "    layer_attns = (False, False, False, True),\n",
    "    layer_cross_attns = (False, False, False, True)\n",
    ")\n",
    "\n",
    "#unet = Unet(\n",
    "#    dim = 32,\n",
    "#    dim_mults = (1, 2, 4, 8),\n",
    "#    num_resnet_blocks = 1,\n",
    "#    layer_attns = (False, False, False, True),\n",
    "#    layer_cross_attns = False\n",
    "#)\n",
    "\n",
    "# imagen, which contains the unet above\n",
    "\n",
    "#imagen = Imagen(\n",
    "#    unets = unet,\n",
    "#    image_sizes = 32,\n",
    "#    timesteps = 1000\n",
    "#)\n",
    "\n",
    "imagen = Imagen(\n",
    "    unets = (unet1, unet2),\n",
    "    image_sizes = (image_size, image_size),\n",
    "    timesteps = 1000,\n",
    "    cond_drop_prob = 0.1\n",
    ").cuda()\n",
    "\n",
    "trainer = ImagenTrainer(\n",
    "    imagen = imagen,\n",
    "    split_valid_from_train = True # whether to split the validation dataset from the training\n",
    ").cuda()\n",
    "\n",
    "print('Using model file ' + model_filename)\n",
    "trainer.load(model_filename)\n",
    "\n",
    "images = trainer.sample(text_embeds=input,stop_at_unet_number=2,batch_size = 1, return_pil_images = True) # returns List[Image]\n",
    "images[0].save('test.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78dccc615e96ab04385280185a87a524fe0822daf5dbad2f4bf2e7d7b28366a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
