{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "# Loading configurations\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "\n",
    "insert_amd_env_vars =  int(configParser.get('COMMON', 'insert_amd_env_vars'))\n",
    "HSA_OVERRIDE_GFX_VERSION =  configParser.get('COMMON', 'HSA_OVERRIDE_GFX_VERSION')\n",
    "ROCM_PATH =  configParser.get('COMMON', 'ROCM_PATH')\n",
    "\n",
    "if(insert_amd_env_vars != 0):\n",
    "    os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = HSA_OVERRIDE_GFX_VERSION\n",
    "    os.environ[\"ROCM_PATH\"] = ROCM_PATH\n",
    "    \n",
    "#import os\n",
    "#os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "#os.environ[\"AUTOGRAPH_VERBOSITY\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import configparser\n",
    "\n",
    "# Loading configurations\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "\n",
    "\n",
    "video_path =  configParser.get('test_imagen','video_path')\n",
    "\n",
    "ttwbdf =  int(configParser.get('test_imagen', 'time_to_wait_before_deleting_files'))\n",
    "\n",
    "\n",
    "cuda =  int(configParser.get('COMMON', 'cuda'))\n",
    "cpus =  int(configParser.get('COMMON', 'cpus'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO document jupyter\n",
    "import pickle\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from textwrap import wrap\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_folder = output_folder = r'imagen_testing_folder' \n",
    "if not os.path.exists(audio_folder):\n",
    "    os.makedirs(audio_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size =  int(configParser.get('COMMON', 'resizeImageTo'))\n",
    "audio_length_used =  configParser.get('test_imagen', 'audio_length_used') \n",
    "model_filename =  configParser.get('test_imagen', 'model_filename') + '_'  + audio_length_used +  's.pt'\n",
    "openl3_mode =  configParser.get('test_imagen', 'openl3_mode')\n",
    "folder =  configParser.get('test_imagen', 'folder')\n",
    "number_of_images =  configParser.get('test_imagen', 'number_of_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age =  int(configParser.get('test_imagen', 'age'))\n",
    "ethnicity =  str(configParser.get('test_imagen', 'ethnicity'))\n",
    "gender =  str(configParser.get('test_imagen', 'gender'))\n",
    "language =  str(configParser.get('test_imagen', 'language'))\n",
    "image_guide_path =  str(configParser.get('test_imagen', 'image_guide_path'))\n",
    "blur_or_pixelate =  int(configParser.get('test_imagen', 'blur_or_pixelate'))\n",
    "\n",
    "boxBlurMin =  int(configParser.get('test_imagen', 'boxBlurMin'))\n",
    "boxBlurMax =  int(configParser.get('test_imagen', 'boxBlurMax'))\n",
    "\n",
    "gaussianBlurMin =  int(configParser.get('test_imagen', 'gaussianBlurMin'))\n",
    "gaussianBlurMax =  int(configParser.get('test_imagen', 'gaussianBlurMax'))\n",
    "\n",
    "pix_to_min =  int(configParser.get('test_imagen', 'pix_to_min'))\n",
    "pix_to_max =  int(configParser.get('test_imagen', 'pix_to_max'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGotten = [['1','1',video_path,age,ethnicity,gender,[],[],[],language,[]]]\n",
    "df = pd.DataFrame(dataGotten,columns = ['ID','FACE_ID','VIDEO_PATH','caption_a','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','AUDIO_FEATURES','caption_l','image_path'])\n",
    "df[\"image_guide\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#for gpu in gpus:\n",
    "#  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpus\n",
    "import pickle\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from testing_imagen_speechbrain import extract_speechbrain_embeddings\n",
    "from testing_imagen_pyannote_titanet import extract_pyannote_titanet_embeddings\n",
    "from testing_imagen_audio_features import extract_audio_features\n",
    "from testing_imagen_vision_transformer import extract_vision_transformer\n",
    "\n",
    "audio_embs =  configParser.get('COMMON', 'audio_embs') \n",
    "from pydub import AudioSegment\n",
    "import math\n",
    "# Get original duration of video\n",
    "audio = AudioSegment.from_file(video_path)\n",
    "audio_length_og = math.floor(audio.duration_seconds)\n",
    "\n",
    "\n",
    "video_filename = os.path.basename(video_path)\n",
    "\n",
    "absPathAudio_w = os.path.abspath(audio_folder) + \"/\" + video_filename\n",
    "absPathAudio = os.path.splitext(absPathAudio_w)[0]+'_audio.wav'\n",
    "absPathAudio_w = os.path.splitext(absPathAudio_w)[0]\n",
    "\n",
    "\n",
    "import subprocess\n",
    "# Extract audio monochannel and with 16khz and put it in absPathAudio\n",
    "\n",
    "command = \"ffmpeg -nostats -loglevel 0 -y -i '\" + video_path + \"' -acodec pcm_s16le -ab 160k -ac 1 -ar 16000 -vn '\" + absPathAudio  + \"'\"\n",
    "subprocess.call(command, shell=True)\n",
    "\n",
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "# Function to delete audio temp files\n",
    "def delFiles(filesToDelete):\n",
    "    time.sleep(ttwbdf)  # wait a bit\n",
    "    for file in filesToDelete:  \n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "\n",
    "# Will either truncate or loop the original video to reach audio_length (6,12 or 24)\n",
    "audio_length_list = [24]\n",
    "for audio_length in audio_length_list:\n",
    "    path_var_len_audio =  absPathAudio_w + \"audio\" + str(audio_length) + \"s.wav\"    # path to the variable length audio\n",
    "    path_var_len_audio_temp =  absPathAudio_w + \"audio_temp\" + str(audio_length) + \"s.wav\"  # path to a temp version of the variable length audio\n",
    "\n",
    "    if(audio_length_og > audio_length):\n",
    "        # Truncate    \n",
    "\n",
    "        command = \"ffmpeg -nostats -loglevel 0 -y -ss 0 -t \"+str(audio_length)+\" -i \\\"\" + absPathAudio + \"\\\" \\\"\" + path_var_len_audio + \"\\\"\"\n",
    "        subprocess.call(command, shell=True)\n",
    "\n",
    "\n",
    "    else:\n",
    "        # Loop then truncaate\n",
    "        #print(\"lesa\")\n",
    "        twoDigitLenStr = f\"{audio_length:02}\"\n",
    "        #print(twoDigitLenStr)\n",
    "        command = \"ffmpeg -nostats -loglevel 0 -y -stream_loop -1 -i '\" + absPathAudio + \"' -t \\\"00:00:\"+twoDigitLenStr+\".000\\\" -codec:a \\\"aac\\\" -f \\\"wav\\\" -c copy '\"+ path_var_len_audio_temp + \"'\"\n",
    "        subprocess.call(command, shell=True)\n",
    "        command = \"ffmpeg -nostats -loglevel 0 -y -ss 0 -t \"+str(audio_length)+\" -i \\\"\" + path_var_len_audio_temp + \"\\\" \\\"\" + path_var_len_audio + \"\\\"\"\n",
    "        subprocess.call(command, shell=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    q = multiprocessing.Queue()\n",
    "\n",
    "    proc = multiprocessing.Process(target=extract_speechbrain_embeddings, args=(q,path_var_len_audio,))\n",
    "    proc.start()\n",
    "    proc.join()\n",
    "\n",
    "\n",
    "\n",
    "    df['SPEAKER_EMB'] = q.get()\n",
    "    proc = multiprocessing.Process(target=extract_pyannote_titanet_embeddings, args=(q,path_var_len_audio,audio_embs,audio_length,openl3_mode,))\n",
    "    proc.start()\n",
    "    proc.join()\n",
    "\n",
    "    df['AUDIO_EMB'] = q.get()\n",
    "\n",
    "    proc = multiprocessing.Process(target=extract_audio_features, args=(q,path_var_len_audio,output_folder,))\n",
    "    proc.start()\n",
    "    proc.join()\n",
    "\n",
    "\n",
    "\n",
    "    with open(output_folder + '/' + 'audio_features.pickle', 'rb') as handle:\n",
    "        df['AUDIO_FEATURES'] = pickle.load(handle)\n",
    "\n",
    "\n",
    "    proc = multiprocessing.Process(target=extract_vision_transformer, args=(q,output_folder,\n",
    "                                                                            image_guide_path,output_folder,\n",
    "        image_size,boxBlurMin,boxBlurMax,gaussianBlurMin,gaussianBlurMax,pix_to_min,pix_to_max,))\n",
    "    proc.start()\n",
    "    proc.join()\n",
    "\n",
    "\n",
    "\n",
    "    with open(output_folder + '/' + 'image_features.pickle', 'rb') as handle:\n",
    "        df[\"image_guide\"] = pickle.load(handle)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ftd = [absPathAudio,path_var_len_audio,os.path.basename(path_var_len_audio),path_var_len_audio_temp]\n",
    "    tDelete = Thread(target=delFiles, args=(ftd,))   # spawn a process\n",
    "    tDelete.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df[[\"image_path\",\"caption_a\",\"caption_e\",\"caption_g\",\"caption_l\"]]\n",
    "data_frame = df3\n",
    "data_frame['SPEAKER_EMB'] = df['SPEAKER_EMB']\n",
    "data_frame['AUDIO_EMB'] = df['AUDIO_EMB']\n",
    "data_frame['AUDIO_FEATURES'] = df['AUDIO_FEATURES']\n",
    "data_frame['image_guide'] = df['image_guide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open(output_folder + '/' + 'filename.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_frame, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78dccc615e96ab04385280185a87a524fe0822daf5dbad2f4bf2e7d7b28366a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
