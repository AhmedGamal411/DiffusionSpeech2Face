{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
      "env: ROCM_PATH=/opt/rocm\n"
     ]
    }
   ],
   "source": [
    "%env HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
    "%env ROCM_PATH=/opt/rocm\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"AUTOGRAPH_VERBOSITY\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from ipyfilechooser import FileChooser\n",
    "import configparser\n",
    "\n",
    "# Loading configurations\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "\n",
    "\n",
    "video_path =  configParser.get('test_imagen','video_path')\n",
    "\n",
    "ttwbdf =  int(configParser.get('test_imagen', 'time_to_wait_before_deleting_files'))\n",
    "\n",
    "\n",
    "cuda =  int(configParser.get('COMMON', 'cuda'))\n",
    "cpus =  int(configParser.get('COMMON', 'cpus'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO document jupyter\n",
    "import pickle\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from textwrap import wrap\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_folder = output_folder = r'imagen_testing_folder' \n",
    "if not os.path.exists(audio_folder):\n",
    "    os.makedirs(audio_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size =  int(configParser.get('COMMON', 'resizeImageTo'))\n",
    "audio_length_used =  configParser.get('train_imagen', 'audio_length_used') \n",
    "model_filename =  configParser.get('train_imagen', 'model_filename') + '_'  + audio_length_used +  's.pt'\n",
    "openl3_mode =  configParser.get('extractOpenL3', 'openl3_mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age =  int(configParser.get('test_imagen', 'age'))\n",
    "ethnicity =  str(configParser.get('test_imagen', 'age'))\n",
    "gender =  str(configParser.get('test_imagen', 'gender'))\n",
    "language =  str(configParser.get('test_imagen', 'language'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGotten = [['1',video_path,age,ethnicity,gender,[],[],language,[]]]\n",
    "df = pd.DataFrame(dataGotten,columns = ['ID','VIDEO_PATH','AGE','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','caption_l','image_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:862.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torchaudio\n",
    "import speechbrain as sb\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "import openl3\n",
    "import openl3\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "import speechbrain as sb\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "\n",
    "model = openl3.models.load_audio_embedding_model(input_repr=\"mel128\", content_type=\"env\",\n",
    "                                                embedding_size=512)\n",
    "\n",
    "\n",
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n",
    "\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import math\n",
    "# Get original duration of video\n",
    "audio = AudioSegment.from_file(video_path)\n",
    "audio_length_og = math.floor(audio.duration_seconds)\n",
    "audio_length_og\n",
    "\n",
    "video_filename = os.path.basename(video_path)\n",
    "\n",
    "absPathAudio_w = os.path.abspath(audio_folder) + \"/\" + video_filename\n",
    "absPathAudio = os.path.splitext(absPathAudio_w)[0]+'_audio.wav'\n",
    "absPathAudio_w = os.path.splitext(absPathAudio_w)[0]\n",
    "\n",
    "\n",
    "import subprocess\n",
    "# Extract audio monochannel and with 16khz and put it in absPathAudio\n",
    "\n",
    "command = \"ffmpeg -nostats -loglevel 0 -y -i '\" + video_path + \"' -acodec pcm_s16le -ab 160k -ac 1 -ar 16000 -vn '\" + absPathAudio  + \"'\"\n",
    "subprocess.call(command, shell=True)\n",
    "\n",
    "import torchaudio\n",
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Function to delete audio temp files\n",
    "def delFiles(filesToDelete):\n",
    "    time.sleep(ttwbdf)  # wait a bit\n",
    "    for file in filesToDelete:  \n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "\n",
    "# Will either truncate or loop the original video to reach audio_length (6,12 or 24)\n",
    "audio_length_list = [24]\n",
    "for audio_length in audio_length_list:\n",
    "    path_var_len_audio =  absPathAudio_w + \"audio\" + str(audio_length) + \"s.wav\"    # path to the variable length audio\n",
    "    path_var_len_audio_temp =  absPathAudio_w + \"audio_temp\" + str(audio_length) + \"s.wav\"  # path to a temp version of the variable length audio\n",
    "\n",
    "    if(audio_length_og > audio_length):\n",
    "        # Truncate    \n",
    "\n",
    "        command = \"ffmpeg -nostats -loglevel 0 -y -ss 0 -t \"+str(audio_length)+\" -i \\\"\" + absPathAudio + \"\\\" \\\"\" + path_var_len_audio + \"\\\"\"\n",
    "        subprocess.call(command, shell=True)\n",
    "\n",
    "\n",
    "    else:\n",
    "        # Loop then truncaate\n",
    "        #print(\"lesa\")\n",
    "        twoDigitLenStr = f\"{audio_length:02}\"\n",
    "        #print(twoDigitLenStr)\n",
    "        command = \"ffmpeg -nostats -loglevel 0 -y -stream_loop -1 -i '\" + absPathAudio + \"' -t \\\"00:00:\"+twoDigitLenStr+\".000\\\" -codec:a \\\"aac\\\" -f \\\"wav\\\" -c copy '\"+ path_var_len_audio_temp + \"'\"\n",
    "        subprocess.call(command, shell=True)\n",
    "        command = \"ffmpeg -nostats -loglevel 0 -y -ss 0 -t \"+str(audio_length)+\" -i \\\"\" + path_var_len_audio_temp + \"\\\" \\\"\" + path_var_len_audio + \"\\\"\"\n",
    "        subprocess.call(command, shell=True)\n",
    "\n",
    "    # Extract speaker embeddings\n",
    "    signal, fs = torchaudio.load(path_var_len_audio)\n",
    "    embeddings = classifier.encode_batch(signal)\n",
    "    embeddingsPickle = pickle.dumps(embeddings.cpu().detach().numpy())\n",
    "\n",
    "    audio, sr = sf.read(path_var_len_audio)\n",
    "\n",
    "    hop_size = -1\n",
    "    if(openl3_mode == 'imagen'):\n",
    "        hop_size = audio_length/250\n",
    "    elif(openl3_mode == 'stable'):\n",
    "        hop_size = 24/50\n",
    "    else:\n",
    "        raise ValueError('openl3_mode in configuration must either be stable or imagen') \n",
    "    \n",
    "    emb, ts = openl3.get_audio_embedding(audio, sr,hop_size=hop_size,verbose=0,model=model)\n",
    "\n",
    "    embeddingsPickle2 = pickle.dumps(emb)\n",
    "\n",
    "    df['SPEAKER_EMB'] = embeddingsPickle\n",
    "    df['AUDIO_EMB'] = embeddingsPickle2\n",
    "\n",
    "    \n",
    "\n",
    "    ftd = [absPathAudio,path_var_len_audio,os.path.basename(path_var_len_audio),path_var_len_audio_temp]\n",
    "    tDelete = Thread(target=delFiles, args=(ftd,))   # spawn a process\n",
    "    tDelete.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df[[\"image_path\",\"caption_e\",\"caption_g\",\"caption_l\"]]\n",
    "data_frame = df3\n",
    "data_frame['SPEAKER_EMB'] = df['SPEAKER_EMB']\n",
    "data_frame['AUDIO_EMB'] = df['AUDIO_EMB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speaker_emb_preprocess(speaker_emb2):\n",
    "\n",
    "    speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "    speaker_emb2 = speaker_emb2.squeeze()\n",
    "    speaker_emb2 = np.pad(speaker_emb2, (288), 'constant', constant_values=(0))\n",
    "    speaker_emb2 = np.tile(speaker_emb2, (1, 1))\n",
    "    speaker_emb2 = speaker_emb2 / 200.0\n",
    "    speaker_emb2 = np.array(speaker_emb2).tolist()\n",
    "    return speaker_emb2\n",
    "\n",
    "\n",
    "def audio_emb_preprocess2(speaker_emb2):\n",
    "    speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "    a = speaker_emb2 #np.zeros(shape=(24, 512))\n",
    "    b = np.zeros(shape=(a.shape[0], 768-a.shape[1]))\n",
    "    c = np.zeros(shape=(250-a.shape[0], 768))\n",
    "    arr = np.concatenate((a, b), axis=1)\n",
    "    arr = np.concatenate((arr, c), axis=0)\n",
    "    arr = arr / 10.0\n",
    "    speaker_emb2 = np.array(arr).tolist()\n",
    "    return speaker_emb2\n",
    "\n",
    "from PIL import Image\n",
    "def getImage(face_path):\n",
    "    im = Image.open(face_path)\n",
    "    im.load() # required for png.split()\n",
    "\n",
    "    im2 = Image.new(\"RGB\", im.size, (255, 255, 255))\n",
    "    im2.paste(im, mask=im.split()[3]) # 3 is the alpha channel\n",
    "    im3 = np.array(im2)\n",
    "    im4 = np.rollaxis(im3,2)\n",
    "    return im4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def process_age(age):\n",
    "    if(age > 100 or age < 0):\n",
    "        x = np.zeros(768)\n",
    "        x[767] = 1\n",
    "        return x\n",
    "    else:\n",
    "        x = np.ones(768) * (age / 100.0)\n",
    "        x[767] = 0\n",
    "        return x\n",
    "\n",
    "def process_gender(gender):\n",
    "    if(gender == 'man'):\n",
    "        return np.ones(768)\n",
    "    elif(gender == \"woman\"):\n",
    "        return np.ones(768) * -1\n",
    "    else:\n",
    "        return np.zeros(768)\n",
    "    \n",
    "# TODO\n",
    "def process_ethnicity(eth):\n",
    "    x = np.zeros(768)\n",
    "    if(eth == \"indian\"):\n",
    "        x[0] = 1\n",
    "    elif(eth == \"asian\"):\n",
    "        x[16]=1\n",
    "    elif(eth == \"latino hispanic\"):\n",
    "        x[2]=1\n",
    "    elif(eth == \"black\"):\n",
    "        x[3]=1\n",
    "    elif(eth == \"middle eastern\"):\n",
    "        x[4]=1\n",
    "    elif(eth == \"white\"):\n",
    "        x[5]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x     \n",
    "\n",
    "def process_language(lan):\n",
    "    x = np.zeros(768)\n",
    "    if(lan == \"Arabic\"):\n",
    "        x[0] = 1\n",
    "    elif(lan == \"Portuguese\"):\n",
    "        x[16]=1\n",
    "    elif(lan == \"Romansh_Sursilvan\"):\n",
    "        x[2]=1\n",
    "    elif(lan == \"Japanese\"):\n",
    "        x[3]=1\n",
    "    elif(lan == \"Ukranian\"):\n",
    "        x[4]=1\n",
    "    elif(lan == \"German\"):\n",
    "        x[5]=1   \n",
    "    elif(lan == \"Chinese_China\"):\n",
    "        x[6]=1   \n",
    "    elif(lan == \"Welsh\"):\n",
    "        x[7]=1  \n",
    "    elif(lan == \"English\"):\n",
    "        x[8]=1\n",
    "    elif(lan == \"Kabyle\"):\n",
    "        x[9]=1 \n",
    "    elif(lan == \"Kyrgyz\"):\n",
    "        x[10]=1\n",
    "    elif(lan == \"Georgian\"):\n",
    "        x[11]=1\n",
    "    elif(lan == \"Persian\"):\n",
    "        x[12]=1 \n",
    "    elif(lan == \"French\"):\n",
    "        x[13]=1\n",
    "    elif(lan == \"Interlingua\"):\n",
    "        x[14]=1\n",
    "    elif(lan == \"Swedish\"):\n",
    "        x[15]=1\n",
    "    elif(lan == \"Spanish\"):\n",
    "        x[16]=1 \n",
    "    elif(lan == \"Dhivehi\"):\n",
    "        x[17]=1\n",
    "    elif(lan == \"Kinyarwanda\"):\n",
    "        x[18]=1 \n",
    "    elif(lan == \"Tatar\"):\n",
    "        x[19]=1\n",
    "    elif(lan == \"Hakha_Chin\"):\n",
    "        x[20]=1 \n",
    "    elif(lan == \"Tamil\"):\n",
    "        x[21]=1 \n",
    "    elif(lan == \"Greek\"):\n",
    "        x[22]=1\n",
    "    elif(lan == \"Latvian\"):\n",
    "        x[23]=1 \n",
    "    elif(lan == \"Russian\"):\n",
    "        x[24]=1\n",
    "    elif(lan == \"Breton\"):\n",
    "        x[25]=1\n",
    "    elif(lan == \"Catalan\"):\n",
    "        x[26]=1    \n",
    "    elif(lan == \"Maltese\"):\n",
    "        x[27]=1 \n",
    "    elif(lan == \"Slovenian\"):\n",
    "        x[28]=1    \n",
    "    elif(lan == \"Indonesian\"):\n",
    "        x[29]=1    \n",
    "    elif(lan == \"Dutch\"):\n",
    "        x[30]=1\n",
    "    elif(lan == \"Chinese_Taiwan\"):\n",
    "        x[31]=1 \n",
    "    elif(lan == \"Sakha\"):\n",
    "        x[32]=1 \n",
    "    elif(lan == \"Polish\"):\n",
    "        x[33]=1 \n",
    "    elif(lan == \"Czech\"):\n",
    "        x[34]=1 \n",
    "    elif(lan == \"Romanian\"):\n",
    "        x[35]=1 \n",
    "    elif(lan == \"Mangolian\"):\n",
    "        x[36]=1 \n",
    "    elif(lan == \"Italian\"):\n",
    "        x[37]=1 \n",
    "    elif(lan == \"Chinese_Hongkong\"):\n",
    "        x[38]=1 \n",
    "    elif(lan == \"Estonian\"):\n",
    "        x[39]=1 \n",
    "    elif(lan == \"Basque\"):\n",
    "        x[40]=1 \n",
    "    elif(lan == \"Esperanto\"):\n",
    "        x[41]=1 \n",
    "    elif(lan == \"Frisian\"):\n",
    "        x[42]=1 \n",
    "    elif(lan == \"Turkish\"):\n",
    "        x[43]=1 \n",
    "    elif(lan == \"Chuvash\"):\n",
    "        x[44]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(age,ethnicity,gender,language,speaker_emb,audio_emb):\n",
    "    speaker_emb = np.asarray(speaker_emb, dtype=np.float32)\n",
    "    speaker_emb = speaker_emb.squeeze()\n",
    "    audio_emb = np.asarray(audio_emb, dtype=np.float32)\n",
    "    audio_emb = audio_emb.squeeze()\n",
    "    #print(speaker_emb.shape)\n",
    "    h = np.vstack((age, ethnicity))\n",
    "    h = np.vstack((h, gender))\n",
    "    h = np.vstack((h, language))\n",
    "    h = np.vstack((h, speaker_emb))\n",
    "    h = np.vstack((h, audio_emb))\n",
    "\n",
    "    j = np.zeros(768)\n",
    "    j = np.tile(j,(256-h.shape[0],1))\n",
    "    h = np.vstack((h, j))\n",
    "    #print(h.shape)\n",
    "    return h.tolist()\n",
    "\n",
    "from PIL import Image\n",
    "def process_image_path(path):\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame['SPEAKER_EMB'] = data_frame['SPEAKER_EMB'].apply(lambda x: speaker_emb_preprocess(x))\n",
    "data_frame['AUDIO_EMB'] = data_frame['AUDIO_EMB'].apply(lambda x: audio_emb_preprocess2(x))\n",
    "data_frame['caption_a'] = data_frame['caption_a'].apply(lambda x: process_age(x))\n",
    "data_frame['caption_g'] = data_frame['caption_g'].apply(lambda x: process_gender(x))\n",
    "data_frame['caption_l'] = data_frame['caption_l'].apply(lambda x: process_language(x))\n",
    "data_frame['caption_e'] = data_frame['caption_e'].apply(lambda x: process_ethnicity(x))\n",
    "\n",
    "data_frame['INPUT'] = data_frame['SPEAKER_EMB']\n",
    "\n",
    "for index, row in data_frame.iterrows():\n",
    "    x = process_input(data_frame.loc[index,\"caption_a\"],data_frame.loc[index,\"caption_e\"]\n",
    "                        ,data_frame.loc[index,\"caption_g\"],\n",
    "                    data_frame.loc[index,\"caption_l\"],data_frame.loc[index,\"SPEAKER_EMB\"],\n",
    "                    data_frame.loc[index,\"AUDIO_EMB\"])\n",
    "    x = [x]\n",
    "    #AADFS = AADFS\n",
    "    data_frame.loc[index,\"INPUT\"] = x\n",
    "\n",
    "data_frame = data_frame.drop(['caption_e', 'caption_g','caption_l','SPEAKER_EMB','AUDIO_EMB'], axis=1)\n",
    "\n",
    "\n",
    "data_frame['image_path'] = data_frame['image_path'].apply(lambda x: process_image_path(x))\n",
    "\n",
    "input = data_frame['INPUT'].to_numpy()\n",
    "input = np.array([np.array(xi) for xi in input])\n",
    "\n",
    "output = data_frame['image_path'].to_numpy()\n",
    "output = np.array([np.array(xi) for xi in output])\n",
    "output.squeeze().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "input = torch.from_numpy(input)\n",
    "input = input.to(torch.float)\n",
    "\n",
    "output = torch.from_numpy(output)\n",
    "output = output.to(torch.float)\n",
    "\n",
    "input = input.squeeze()\n",
    "output = output.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = input[None,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n",
      "Using model file imagen_two_unets_24s.pt\n",
      "checkpoint loaded from imagen_two_unets_24s.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755ac5816ba643d1b1f589456bc60037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bf33ebc39249d7aa2286b84221a03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7648c70e63d47f3ab6ab8c57eb435be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from imagen_pytorch import Unet, Imagen, ImagenTrainer\n",
    "from imagen_pytorch.data import Dataset\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = 32,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = 3,\n",
    "    layer_attns = (False, True, True, True),\n",
    "    layer_cross_attns = (False, True, True, True)\n",
    ")\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = 32,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = (2, 4, 8, 8),\n",
    "    layer_attns = (False, False, False, True),\n",
    "    layer_cross_attns = (False, False, False, True)\n",
    ")\n",
    "\n",
    "#unet = Unet(\n",
    "#    dim = 32,\n",
    "#    dim_mults = (1, 2, 4, 8),\n",
    "#    num_resnet_blocks = 1,\n",
    "#    layer_attns = (False, False, False, True),\n",
    "#    layer_cross_attns = False\n",
    "#)\n",
    "\n",
    "# imagen, which contains the unet above\n",
    "\n",
    "#imagen = Imagen(\n",
    "#    unets = unet,\n",
    "#    image_sizes = 32,\n",
    "#    timesteps = 1000\n",
    "#)\n",
    "\n",
    "imagen = Imagen(\n",
    "    unets = (unet1, unet2),\n",
    "    image_sizes = (image_size, image_size),\n",
    "    timesteps = 1000,\n",
    "    cond_drop_prob = 0.1\n",
    ").cuda()\n",
    "\n",
    "trainer = ImagenTrainer(\n",
    "    imagen = imagen,\n",
    "    split_valid_from_train = True # whether to split the validation dataset from the training\n",
    ").cuda()\n",
    "\n",
    "print('Using model file ' + model_filename)\n",
    "trainer.load(model_filename)\n",
    "\n",
    "images = trainer.sample(text_embeds=input,stop_at_unet_number=2,batch_size = 1, return_pil_images = True) # returns List[Image]\n",
    "images[0].save('test.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os._exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78dccc615e96ab04385280185a87a524fe0822daf5dbad2f4bf2e7d7b28366a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
