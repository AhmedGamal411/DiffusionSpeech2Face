{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
      "env: ROCM_PATH=/opt/rocm\n"
     ]
    }
   ],
   "source": [
    "%env HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
    "%env ROCM_PATH=/opt/rocm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#TODO document jupyter\n",
    "import pickle\n",
    "import configparser\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "datasetPathDatabase =  configParser.get('COMMON', 'datasetPathDatabase') + '/dataset.db'\n",
    "image_size =  int(configParser.get('COMMON', 'resizeImageTo'))\n",
    "unet_dim =  int(configParser.get('COMMON', 'unet_dim'))\n",
    "audio_embs =  str(configParser.get('COMMON', 'audio_embs'))\n",
    "audio_length_used =  configParser.get('train_imagen', 'audio_length_used') \n",
    "model_filename =  configParser.get('train_imagen', 'model_filename') + '_'  + audio_length_used +  's.pt'\n",
    "sub_epochs=  int(configParser.get('train_imagen', 'sub_epochs') )\n",
    "batch_size=  int(configParser.get('train_imagen', 'batch_size') )\n",
    "sample_every=  int(configParser.get('train_imagen', 'sample_every') ) - 1\n",
    "save_model_every=  int(configParser.get('train_imagen', 'save_model_every') ) - 1\n",
    "epochs=  int(configParser.get('train_imagen', 'epochs') )\n",
    "\n",
    "continue_from_epoch = int(configParser.get('train_imagen', 'continue_from_epoch'))\n",
    "continue_from_offset = int(configParser.get('train_imagen', 'continue_from_offset'))\n",
    "continue_from_epoch_and_offset_flag = int(configParser.get('train_imagen', 'continue_from_epoch_and_offset_flag'))\n",
    "db_chunk = int(configParser.get('train_imagen', 'db_chunk'))\n",
    "\n",
    "con = sl.connect(datasetPathDatabase)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speaker_emb_preprocess(speaker_emb2):\n",
    "\n",
    "    speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "    speaker_emb2 = speaker_emb2.squeeze()\n",
    "    speaker_emb2 = np.pad(speaker_emb2, (288), 'constant', constant_values=(0))\n",
    "    speaker_emb2 = np.tile(speaker_emb2, (1, 1))\n",
    "    speaker_emb2 = speaker_emb2 / 200.0\n",
    "    speaker_emb2 = np.array(speaker_emb2).tolist()\n",
    "    return speaker_emb2\n",
    "\n",
    "\n",
    "def audio_emb_preprocess2(speaker_emb2):\n",
    "    speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "    a = speaker_emb2 #np.zeros(shape=(24, 512))\n",
    "    b = np.zeros(shape=(a.shape[0], 768-a.shape[1]))\n",
    "    c = np.zeros(shape=(250-a.shape[0], 768))\n",
    "    arr = np.concatenate((a, b), axis=1)\n",
    "    arr = np.concatenate((arr, c), axis=0)\n",
    "    arr = arr / (1 if(audio_embs == 'wav2vec') else 10 if(audio_embs == 'openl3')  else 1 if(audio_embs == 'pyannoteTitaNet') else 1)\n",
    "    #print(str(arr.max()))\n",
    "    speaker_emb2 = np.array(arr).tolist()\n",
    "    return speaker_emb2\n",
    "\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def process_age(age):\n",
    "    if(random.random() < 0.2):\n",
    "        x = np.zeros(768)\n",
    "        x[767] = 1\n",
    "        return x\n",
    "    else:\n",
    "        x = np.ones(768) * (age / 100.0)\n",
    "        x[767] = 0\n",
    "        return x\n",
    "\n",
    "def process_gender(gender):\n",
    "    if(random.random() < 0.2):\n",
    "        return np.zeros(768)\n",
    "    elif(gender == 'man'):\n",
    "        return np.ones(768)\n",
    "    elif(gender == \"woman\"):\n",
    "        return np.ones(768) * -1\n",
    "    else:\n",
    "        return np.zeros(768)\n",
    "    \n",
    "# TODO\n",
    "def process_ethnicity(eth):\n",
    "    x = np.zeros(768)\n",
    "    if(random.random() < 0.2):\n",
    "        x = x\n",
    "    elif(eth == \"indian\"):\n",
    "        x[0] = 1\n",
    "    elif(eth == \"asian\"):\n",
    "        x[16]=1\n",
    "    elif(eth == \"latino hispanic\"):\n",
    "        x[2]=1\n",
    "    elif(eth == \"black\"):\n",
    "        x[3]=1\n",
    "    elif(eth == \"middle eastern\"):\n",
    "        x[4]=1\n",
    "    elif(eth == \"white\"):\n",
    "        x[5]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x     \n",
    "\n",
    "def process_language(lan):\n",
    "    x = np.zeros(768)\n",
    "    if(random.random() < 0.2):\n",
    "        x = x\n",
    "    elif(lan == \"Arabic\"):\n",
    "        x[0] = 1\n",
    "    elif(lan == \"Portuguese\"):\n",
    "        x[16]=1\n",
    "    elif(lan == \"Romansh_Sursilvan\"):\n",
    "        x[2]=1\n",
    "    elif(lan == \"Japanese\"):\n",
    "        x[3]=1\n",
    "    elif(lan == \"Ukranian\"):\n",
    "        x[4]=1\n",
    "    elif(lan == \"German\"):\n",
    "        x[5]=1   \n",
    "    elif(lan == \"Chinese_China\"):\n",
    "        x[6]=1   \n",
    "    elif(lan == \"Welsh\"):\n",
    "        x[7]=1  \n",
    "    elif(lan == \"English\"):\n",
    "        x[8]=1\n",
    "    elif(lan == \"Kabyle\"):\n",
    "        x[9]=1 \n",
    "    elif(lan == \"Kyrgyz\"):\n",
    "        x[10]=1\n",
    "    elif(lan == \"Georgian\"):\n",
    "        x[11]=1\n",
    "    elif(lan == \"Persian\"):\n",
    "        x[12]=1 \n",
    "    elif(lan == \"French\"):\n",
    "        x[13]=1\n",
    "    elif(lan == \"Interlingua\"):\n",
    "        x[14]=1\n",
    "    elif(lan == \"Swedish\"):\n",
    "        x[15]=1\n",
    "    elif(lan == \"Spanish\"):\n",
    "        x[16]=1 \n",
    "    elif(lan == \"Dhivehi\"):\n",
    "        x[17]=1\n",
    "    elif(lan == \"Kinyarwanda\"):\n",
    "        x[18]=1 \n",
    "    elif(lan == \"Tatar\"):\n",
    "        x[19]=1\n",
    "    elif(lan == \"Hakha_Chin\"):\n",
    "        x[20]=1 \n",
    "    elif(lan == \"Tamil\"):\n",
    "        x[21]=1 \n",
    "    elif(lan == \"Greek\"):\n",
    "        x[22]=1\n",
    "    elif(lan == \"Latvian\"):\n",
    "        x[23]=1 \n",
    "    elif(lan == \"Russian\"):\n",
    "        x[24]=1\n",
    "    elif(lan == \"Breton\"):\n",
    "        x[25]=1\n",
    "    elif(lan == \"Catalan\"):\n",
    "        x[26]=1    \n",
    "    elif(lan == \"Maltese\"):\n",
    "        x[27]=1 \n",
    "    elif(lan == \"Slovenian\"):\n",
    "        x[28]=1    \n",
    "    elif(lan == \"Indonesian\"):\n",
    "        x[29]=1    \n",
    "    elif(lan == \"Dutch\"):\n",
    "        x[30]=1\n",
    "    elif(lan == \"Chinese_Taiwan\"):\n",
    "        x[31]=1 \n",
    "    elif(lan == \"Sakha\"):\n",
    "        x[32]=1 \n",
    "    elif(lan == \"Polish\"):\n",
    "        x[33]=1 \n",
    "    elif(lan == \"Czech\"):\n",
    "        x[34]=1 \n",
    "    elif(lan == \"Romanian\"):\n",
    "        x[35]=1 \n",
    "    elif(lan == \"Mangolian\"):\n",
    "        x[36]=1 \n",
    "    elif(lan == \"Italian\"):\n",
    "        x[37]=1 \n",
    "    elif(lan == \"Chinese_Hongkong\"):\n",
    "        x[38]=1 \n",
    "    elif(lan == \"Estonian\"):\n",
    "        x[39]=1 \n",
    "    elif(lan == \"Basque\"):\n",
    "        x[40]=1 \n",
    "    elif(lan == \"Esperanto\"):\n",
    "        x[41]=1 \n",
    "    elif(lan == \"Frisian\"):\n",
    "        x[42]=1 \n",
    "    elif(lan == \"Turkish\"):\n",
    "        x[43]=1 \n",
    "    elif(lan == \"Chuvash\"):\n",
    "        x[44]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_input(age,ethnicity,gender,language,speaker_emb,audio_emb):\n",
    "    speaker_emb = np.asarray(speaker_emb, dtype=np.float32)\n",
    "    speaker_emb = speaker_emb.squeeze()\n",
    "    audio_emb = np.asarray(audio_emb, dtype=np.float32)\n",
    "    audio_emb = audio_emb.squeeze()\n",
    "    #print(speaker_emb.shape)\n",
    "    h = np.vstack((age, ethnicity))\n",
    "    h = np.vstack((h, gender))\n",
    "    h = np.vstack((h, language))\n",
    "    h = np.vstack((h, speaker_emb))\n",
    "    h = np.vstack((h, audio_emb))\n",
    "\n",
    "    j = np.zeros(768)\n",
    "    j = np.tile(j,(256-h.shape[0],1))\n",
    "    h = np.vstack((h, j))\n",
    "    #print(h.shape)\n",
    "    return h.tolist()\n",
    "\n",
    "from PIL import Image\n",
    "def make_square(im, min_size=image_size, fill_color=(255, 255, 255, 0)):\n",
    "    x, y = im.size\n",
    "    size = max(min_size, x, y)\n",
    "    new_im = Image.new('RGB', (size, size), fill_color)\n",
    "    new_im.paste(im, (int((size - x) / 2), int((size - y) / 2)))\n",
    "    return new_im\n",
    "\n",
    "def process_image_path(path):\n",
    "    \n",
    "    image = Image.open(path).convert('RGB')\n",
    "\n",
    "    w_s = image_size / (1+2 * 0.5)\n",
    "    h_s = image_size / (1+2 * 0.4)\n",
    "\n",
    "    image = image.crop((0.4*w_s,0.0*h_s,1.6*w_s,1.4*h_s))\n",
    "\n",
    "    image = make_square(image)\n",
    "    print('saving')\n",
    "    image.save(str(random.random()) + '.png')\n",
    "\n",
    "    \n",
    "\n",
    "    #print(np.array(image,np.float32).shape)\n",
    "    pix = np.array(image,np.float32)\n",
    "    pix = np.moveaxis(pix, -1, 0)\n",
    "    \n",
    "    pix = pix / 255\n",
    "    return pix.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in epoch 1\n",
      "Training in offset 0\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "Training Unet No. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2023-06-07 14:30:31.275033: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-07 14:30:31.317379: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n",
      "training with dataset of 46 samples and validating with randomly splitted 2 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/gamal/vsc/DiffusionSpeech2Face/train_imagen_batch.py\", line 126, in train_batch_unet1\n",
      "    loss = trainer.train_step(unet_number = 1, max_batch_size = batch_size)\n",
      "  File \"/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/trainer.py\", line 611, in train_step\n",
      "    loss = self.step_with_dl_iter(self.train_dl_iter, unet_number = unet_number, **kwargs)\n",
      "  File \"/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/trainer.py\", line 629, in step_with_dl_iter\n",
      "    loss = self.forward(**{**kwargs, **model_input})\n",
      "  File \"/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/trainer.py\", line 136, in inner\n",
      "    out = fn(model, *args, **kwargs)\n",
      "  File \"/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/trainer.py\", line 988, in forward\n",
      "    self.accelerator.backward(loss)\n",
      "  File \"/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/accelerate/accelerator.py\", line 1634, in backward\n",
      "    self.scaler.scale(loss).backward(**kwargs)\n",
      "  File \"/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.53 GiB (GPU 0; 22.15 GiB total capacity; 15.30 GiB already allocated; 2.56 GiB free; 18.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Unet No. 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2023-06-07 14:30:38.222879: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-07 14:30:38.264542: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n",
      "training with dataset of 46 samples and validating with randomly splitted 2 samples\n",
      "valid loss: 0.5843320488929749\n",
      "unet 1 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4251e0afb3e3437e94fd276955ac04cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fefbb6734fca4ed1b952a683065e6254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 90\u001b[0m\n\u001b[1;32m     88\u001b[0m proc \u001b[39m=\u001b[39m multiprocessing\u001b[39m.\u001b[39mProcess(target\u001b[39m=\u001b[39mtrain_imagen_batch\u001b[39m.\u001b[39mtrain_batch_unet2, args\u001b[39m=\u001b[39m(\u001b[39minput\u001b[39m,output,model_filename,sub_epochs,batch_size,sample_every,save_model_every,image_size,unet_dim))\n\u001b[1;32m     89\u001b[0m proc\u001b[39m.\u001b[39mstart()\n\u001b[0;32m---> 90\u001b[0m proc\u001b[39m.\u001b[39;49mjoin()\n\u001b[1;32m     91\u001b[0m offset \u001b[39m=\u001b[39m offset \u001b[39m+\u001b[39m db_chunk\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_pid \u001b[39m==\u001b[39m os\u001b[39m.\u001b[39mgetpid(), \u001b[39m'\u001b[39m\u001b[39mcan only join a child process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mcan only join a started process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_popen\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[39m.\u001b[39mdiscard(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/multiprocessing/popen_fork.py:47\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[39m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpoll(os\u001b[39m.\u001b[39;49mWNOHANG \u001b[39mif\u001b[39;49;00m timeout \u001b[39m==\u001b[39;49m \u001b[39m0.0\u001b[39;49m \u001b[39melse\u001b[39;49;00m \u001b[39m0\u001b[39;49m)\n\u001b[1;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/multiprocessing/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         pid, sts \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mwaitpid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpid, flag)\n\u001b[1;32m     28\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     29\u001b[0m         \u001b[39m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[39m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f280fd7371c43cfaa018a8a8a458c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import train_imagen_batch\n",
    "\n",
    "\n",
    "\n",
    "range_of_epochs = range(1,epochs + 1)\n",
    "\n",
    "if(continue_from_epoch_and_offset_flag != 0):\n",
    "    \n",
    "    range_of_epochs = range(continue_from_epoch, epochs + 1)\n",
    "\n",
    "offset_zero_flag = 0\n",
    "\n",
    "for epoch in range_of_epochs:\n",
    "\n",
    "    print(\"Training in epoch \" + str(epoch))\n",
    "\n",
    "    offset = 0\n",
    "\n",
    "    if(continue_from_epoch_and_offset_flag != 0 and offset_zero_flag == 0):\n",
    "        offset = continue_from_offset\n",
    "        offset_zero_flag = 1\n",
    "\n",
    "    while(True):\n",
    "\n",
    "        print(\"Training in offset \" + str(offset))\n",
    "\n",
    "        data = con.execute(\"SELECT V.ID, V.VIDEO_PATH, V.AGE CAPTION_A, \" + \n",
    "                            \"V.ETHNICITY CAPTION_E, \" +\n",
    "                            \"lower(V.GENDER) CAPTION_G, \" +\n",
    "                            \"A.SPEAKER_EMB, \"+ (\"A.WAV_TO_VEC, \" if(audio_embs == 'wav2vec') else \"A.AUDIO_EMB2, \" if(audio_embs == 'openl3')  else \"A.PYANNOTE_TITANET, \" if(audio_embs == 'pyannoteTitaNet') else ', ') +\n",
    "                            \"A.LANG CAPTION_L, \"+\n",
    "                            \"F.FACE_PATH \"+\n",
    "                            \"FROM VIDEO V \"+\n",
    "                            \"INNER JOIN AUDIO A ON V.ID = A.VIDEO_ID INNER JOIN FACE F ON V.ID = F.VIDEO_ID WHERE AUDIO_PRE = 2 AND FACES_PRE = 1 AND AUDIO_LENGTH = \" + audio_length_used + ' ' +\n",
    "                            \"LIMIT \"+ str(db_chunk) +\" OFFSET \" + str(offset))\n",
    "        dataGotten = data.fetchall()\n",
    "\n",
    "        if(len(dataGotten) == 0):\n",
    "            break\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(dataGotten,columns = ['ID','VIDEO_PATH','caption_a','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','caption_l','image_path'])\n",
    "\n",
    "        df3 = df[[\"image_path\",\"caption_a\",\"caption_e\",\"caption_g\",\"caption_l\"]]\n",
    "\n",
    "        data_frame = df3\n",
    "        data_frame['SPEAKER_EMB'] = df['SPEAKER_EMB']\n",
    "        data_frame['AUDIO_EMB'] = df['AUDIO_EMB']\n",
    "\n",
    "        \n",
    "        data_frame['SPEAKER_EMB'] = data_frame['SPEAKER_EMB'].apply(lambda x: speaker_emb_preprocess(x))\n",
    "        data_frame['AUDIO_EMB'] = data_frame['AUDIO_EMB'].apply(lambda x: audio_emb_preprocess2(x))\n",
    "        data_frame['caption_a'] = data_frame['caption_a'].apply(lambda x: process_age(x))\n",
    "        data_frame['caption_g'] = data_frame['caption_g'].apply(lambda x: process_gender(x))\n",
    "        data_frame['caption_l'] = data_frame['caption_l'].apply(lambda x: process_language(x))\n",
    "        data_frame['caption_e'] = data_frame['caption_e'].apply(lambda x: process_ethnicity(x))\n",
    "\n",
    "        data_frame['INPUT'] = data_frame['SPEAKER_EMB']\n",
    "\n",
    "        for index, row in data_frame.iterrows():\n",
    "            x = process_input(data_frame.loc[index,\"caption_a\"],data_frame.loc[index,\"caption_e\"]\n",
    "                              ,data_frame.loc[index,\"caption_g\"],\n",
    "                            data_frame.loc[index,\"caption_l\"],data_frame.loc[index,\"SPEAKER_EMB\"],\n",
    "                            data_frame.loc[index,\"AUDIO_EMB\"])\n",
    "            x = [x]\n",
    "            #AADFS = AADFS\n",
    "            data_frame.loc[index,\"INPUT\"] = x\n",
    "\n",
    "        data_frame = data_frame.drop(['caption_e', 'caption_g','caption_l','SPEAKER_EMB','AUDIO_EMB'], axis=1)\n",
    "\n",
    "\n",
    "        data_frame['image_path'] = data_frame['image_path'].apply(lambda x: process_image_path(x))\n",
    "\n",
    "        input = data_frame['INPUT'].to_numpy()\n",
    "        input = np.array([np.array(xi) for xi in input])\n",
    "        \n",
    "        output = data_frame['image_path'].to_numpy()\n",
    "        output = np.array([np.array(xi) for xi in output])\n",
    "        output.squeeze().shape\n",
    "\n",
    "\n",
    "\n",
    "        proc = multiprocessing.Process(target=train_imagen_batch.train_batch_unet1, args=(input,output,model_filename,sub_epochs,batch_size,sample_every,save_model_every,image_size,unet_dim,))\n",
    "        proc.start()\n",
    "        proc.join()\n",
    "\n",
    "        proc = multiprocessing.Process(target=train_imagen_batch.train_batch_unet2, args=(input,output,model_filename,sub_epochs,batch_size,sample_every,save_model_every,image_size,unet_dim))\n",
    "        proc.start()\n",
    "        proc.join()\n",
    "        offset = offset + db_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78dccc615e96ab04385280185a87a524fe0822daf5dbad2f4bf2e7d7b28366a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
