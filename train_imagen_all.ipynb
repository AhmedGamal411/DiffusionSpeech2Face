{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
      "env: ROCM_PATH=/opt/rocm\n"
     ]
    }
   ],
   "source": [
    "%env HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
    "%env ROCM_PATH=/opt/rocm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#TODO document jupyter\n",
    "import pickle\n",
    "import configparser\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "datasetPathDatabase =  configParser.get('COMMON', 'datasetPathDatabase') + '/dataset.db'\n",
    "image_size =  int(configParser.get('COMMON', 'resizeImageTo'))\n",
    "audio_length_used =  configParser.get('train_imagen', 'audio_length_used') \n",
    "model_filename =  configParser.get('train_imagen', 'model_filename') + '_'  + audio_length_used +  's.pt'\n",
    "sub_epochs=  int(configParser.get('train_imagen', 'sub_epochs') )\n",
    "batch_size=  int(configParser.get('train_imagen', 'batch_size') )\n",
    "sample_every=  int(configParser.get('train_imagen', 'sample_every') ) - 1\n",
    "save_model_every=  int(configParser.get('train_imagen', 'save_model_every') ) - 1\n",
    "epochs=  int(configParser.get('train_imagen', 'epochs') )\n",
    "\n",
    "continue_from_epoch = int(configParser.get('train_imagen', 'continue_from_epoch'))\n",
    "continue_from_offset = int(configParser.get('train_imagen', 'continue_from_offset'))\n",
    "continue_from_epoch_and_offset_flag = int(configParser.get('train_imagen', 'continue_from_epoch_and_offset_flag'))\n",
    "db_chunk = int(configParser.get('train_imagen', 'db_chunk'))\n",
    "\n",
    "con = sl.connect(datasetPathDatabase)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speaker_emb_preprocess(speaker_emb2):\n",
    "\n",
    "    speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "    speaker_emb2 = speaker_emb2.squeeze()\n",
    "    speaker_emb2 = np.pad(speaker_emb2, (288), 'constant', constant_values=(0))\n",
    "    speaker_emb2 = np.tile(speaker_emb2, (1, 1))\n",
    "    speaker_emb2 = speaker_emb2 / 200.0\n",
    "    speaker_emb2 = np.array(speaker_emb2).tolist()\n",
    "    return speaker_emb2\n",
    "\n",
    "\n",
    "def audio_emb_preprocess2(speaker_emb2):\n",
    "    speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "    a = speaker_emb2 #np.zeros(shape=(24, 512))\n",
    "    b = np.zeros(shape=(a.shape[0], 768-a.shape[1]))\n",
    "    c = np.zeros(shape=(250-a.shape[0], 768))\n",
    "    arr = np.concatenate((a, b), axis=1)\n",
    "    arr = np.concatenate((arr, c), axis=0)\n",
    "    arr = arr / 10.0\n",
    "    speaker_emb2 = np.array(arr).tolist()\n",
    "    return speaker_emb2\n",
    "\n",
    "from PIL import Image\n",
    "def getImage(face_path):\n",
    "    im = Image.open(face_path)\n",
    "    im.load() # required for png.split()\n",
    "\n",
    "    im2 = Image.new(\"RGB\", im.size, (255, 255, 255))\n",
    "    im2.paste(im, mask=im.split()[3]) # 3 is the alpha channel\n",
    "    im3 = np.array(im2)\n",
    "    im4 = np.rollaxis(im3,2)\n",
    "    return im4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def process_gender(gender):\n",
    "    if(random.random() < 0.2):\n",
    "        return np.zeros(768)\n",
    "    elif(gender == 'man'):\n",
    "        return np.ones(768)\n",
    "    elif(gender == \"woman\"):\n",
    "        return np.ones(768) * -1\n",
    "    else:\n",
    "        return np.zeros(768)\n",
    "    \n",
    "# TODO\n",
    "def process_ethnicity(eth):\n",
    "    x = np.zeros(768)\n",
    "    if(random.random() < 0.2):\n",
    "        x = x\n",
    "    elif(eth == \"indian\"):\n",
    "        x[0] = 1\n",
    "    elif(eth == \"asian\"):\n",
    "        x[16]=1\n",
    "    elif(eth == \"latino hispanic\"):\n",
    "        x[2]=1\n",
    "    elif(eth == \"black\"):\n",
    "        x[3]=1\n",
    "    elif(eth == \"middle eastern\"):\n",
    "        x[4]=1\n",
    "    elif(eth == \"white\"):\n",
    "        x[5]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x     \n",
    "\n",
    "def process_language(lan):\n",
    "    x = np.zeros(768)\n",
    "    if(random.random() < 0.2):\n",
    "        x = x\n",
    "    elif(lan == \"Arabic\"):\n",
    "        x[0] = 1\n",
    "    elif(lan == \"Portuguese\"):\n",
    "        x[16]=1\n",
    "    elif(lan == \"Romansh_Sursilvan\"):\n",
    "        x[2]=1\n",
    "    elif(lan == \"Japanese\"):\n",
    "        x[3]=1\n",
    "    elif(lan == \"Ukranian\"):\n",
    "        x[4]=1\n",
    "    elif(lan == \"German\"):\n",
    "        x[5]=1   \n",
    "    elif(lan == \"Chinese_China\"):\n",
    "        x[6]=1   \n",
    "    elif(lan == \"Welsh\"):\n",
    "        x[7]=1  \n",
    "    elif(lan == \"English\"):\n",
    "        x[8]=1\n",
    "    elif(lan == \"Kabyle\"):\n",
    "        x[9]=1 \n",
    "    elif(lan == \"Kyrgyz\"):\n",
    "        x[10]=1\n",
    "    elif(lan == \"Georgian\"):\n",
    "        x[11]=1\n",
    "    elif(lan == \"Persian\"):\n",
    "        x[12]=1 \n",
    "    elif(lan == \"French\"):\n",
    "        x[13]=1\n",
    "    elif(lan == \"Interlingua\"):\n",
    "        x[14]=1\n",
    "    elif(lan == \"Swedish\"):\n",
    "        x[15]=1\n",
    "    elif(lan == \"Spanish\"):\n",
    "        x[16]=1 \n",
    "    elif(lan == \"Dhivehi\"):\n",
    "        x[17]=1\n",
    "    elif(lan == \"Kinyarwanda\"):\n",
    "        x[18]=1 \n",
    "    elif(lan == \"Tatar\"):\n",
    "        x[19]=1\n",
    "    elif(lan == \"Hakha_Chin\"):\n",
    "        x[20]=1 \n",
    "    elif(lan == \"Tamil\"):\n",
    "        x[21]=1 \n",
    "    elif(lan == \"Greek\"):\n",
    "        x[22]=1\n",
    "    elif(lan == \"Latvian\"):\n",
    "        x[23]=1 \n",
    "    elif(lan == \"Russian\"):\n",
    "        x[24]=1\n",
    "    elif(lan == \"Breton\"):\n",
    "        x[25]=1\n",
    "    elif(lan == \"Catalan\"):\n",
    "        x[26]=1    \n",
    "    elif(lan == \"Maltese\"):\n",
    "        x[27]=1 \n",
    "    elif(lan == \"Slovenian\"):\n",
    "        x[28]=1    \n",
    "    elif(lan == \"Indonesian\"):\n",
    "        x[29]=1    \n",
    "    elif(lan == \"Dutch\"):\n",
    "        x[30]=1\n",
    "    elif(lan == \"Chinese_Taiwan\"):\n",
    "        x[31]=1 \n",
    "    elif(lan == \"Sakha\"):\n",
    "        x[32]=1 \n",
    "    elif(lan == \"Polish\"):\n",
    "        x[33]=1 \n",
    "    elif(lan == \"Czech\"):\n",
    "        x[34]=1 \n",
    "    elif(lan == \"Romanian\"):\n",
    "        x[35]=1 \n",
    "    elif(lan == \"Mangolian\"):\n",
    "        x[36]=1 \n",
    "    elif(lan == \"Italian\"):\n",
    "        x[37]=1 \n",
    "    elif(lan == \"Chinese_Hongkong\"):\n",
    "        x[38]=1 \n",
    "    elif(lan == \"Estonian\"):\n",
    "        x[39]=1 \n",
    "    elif(lan == \"Basque\"):\n",
    "        x[40]=1 \n",
    "    elif(lan == \"Esperanto\"):\n",
    "        x[41]=1 \n",
    "    elif(lan == \"Frisian\"):\n",
    "        x[42]=1 \n",
    "    elif(lan == \"Turkish\"):\n",
    "        x[43]=1 \n",
    "    elif(lan == \"Chuvash\"):\n",
    "        x[44]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO add age\n",
    "def process_input(ethnicity,gender,language,speaker_emb,audio_emb):\n",
    "    speaker_emb = np.asarray(speaker_emb, dtype=np.float32)\n",
    "    speaker_emb = speaker_emb.squeeze()\n",
    "    audio_emb = np.asarray(audio_emb, dtype=np.float32)\n",
    "    audio_emb = audio_emb.squeeze()\n",
    "    #print(speaker_emb.shape)\n",
    "    h = np.vstack((ethnicity, gender))\n",
    "    h = np.vstack((h, language))\n",
    "    h = np.vstack((h, speaker_emb))\n",
    "    h = np.vstack((h, audio_emb))\n",
    "\n",
    "    j = np.zeros(768)\n",
    "    j = np.tile(j,(256-h.shape[0],1))\n",
    "    h = np.vstack((h, j))\n",
    "    #print(h.shape)\n",
    "    return h.tolist()\n",
    "\n",
    "from PIL import Image\n",
    "def process_image_path(path):\n",
    "    \n",
    "    image = Image.open(path).convert('RGB')\n",
    "    \n",
    "    pix = np.array(image,np.float32)\n",
    "    pix = np.moveaxis(pix, -1, 0)\n",
    "    \n",
    "    pix = pix / 255\n",
    "    return pix.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in epoch 1\n",
      "Training in offset 0\n",
      "Training Unet No. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 10:02:44.412954: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-29 10:02:44.654442: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n",
      "training with dataset of 975 samples and validating with randomly splitted 25 samples\n",
      "valid loss: 0.3891371190547943\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:48<00:00, 20.83it/s]\n",
      "0it [00:48, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to imagen_two_unets_24s.pt\n",
      "valid loss: 0.2543244957923889\n",
      "valid loss: 0.06447862833738327\n",
      "valid loss: 0.15998338162899017\n",
      "valid loss: 0.036453913897275925\n",
      "valid loss: 0.054477132856845856\n",
      "valid loss: 0.03573376312851906\n",
      "valid loss: 0.05886620283126831\n",
      "valid loss: 0.05402786657214165\n",
      "valid loss: 0.040450554341077805\n",
      "valid loss: 0.03463262319564819\n",
      "valid loss: 0.02409813180565834\n",
      "valid loss: 0.019666103646159172\n",
      "valid loss: 0.02213306538760662\n",
      "valid loss: 0.023678231984376907\n",
      "valid loss: 0.02285417541861534\n",
      "valid loss: 0.019883031025528908\n",
      "valid loss: 0.020458677783608437\n",
      "valid loss: 0.026252586394548416\n",
      "valid loss: 0.02275071106851101\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:49<00:00, 20.09it/s]\n",
      "0it [00:49, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.020509880036115646\n",
      "valid loss: 0.02117871306836605\n",
      "valid loss: 0.014328350313007832\n",
      "valid loss: 0.023072345182299614\n",
      "valid loss: 0.016899079084396362\n",
      "valid loss: 0.018882114440202713\n",
      "valid loss: 0.022858453914523125\n",
      "valid loss: 0.01632832922041416\n",
      "valid loss: 0.02163705602288246\n",
      "valid loss: 0.018583331257104874\n",
      "valid loss: 0.014947894029319286\n",
      "valid loss: 0.018120355904102325\n",
      "valid loss: 0.0145344203338027\n",
      "valid loss: 0.020027536898851395\n",
      "valid loss: 0.013471423648297787\n",
      "valid loss: 0.012844473123550415\n",
      "valid loss: 0.01229296624660492\n",
      "valid loss: 0.010675760917365551\n",
      "valid loss: 0.020008375868201256\n",
      "valid loss: 0.013393387198448181\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:39<00:00, 25.18it/s]\n",
      "0it [00:39, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.012072809971868992\n",
      "valid loss: 0.016208970919251442\n",
      "valid loss: 0.00856966432183981\n",
      "valid loss: 0.017524152994155884\n",
      "valid loss: 0.011353912763297558\n",
      "valid loss: 0.009523238055408001\n",
      "valid loss: 0.013955935835838318\n",
      "valid loss: 0.010137384757399559\n",
      "valid loss: 0.015466727316379547\n",
      "valid loss: 0.008461196906864643\n",
      "valid loss: 0.010242508724331856\n",
      "valid loss: 0.008072711527347565\n",
      "valid loss: 0.009068849496543407\n",
      "valid loss: 0.009601830504834652\n",
      "valid loss: 0.009030766785144806\n",
      "valid loss: 0.00911314133554697\n",
      "valid loss: 0.01313907839357853\n",
      "valid loss: 0.007007093634456396\n",
      "valid loss: 0.009306075051426888\n",
      "valid loss: 0.010007055476307869\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:45<00:00, 21.92it/s]\n",
      "0it [00:45, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.009721084497869015\n",
      "valid loss: 0.00600834283977747\n",
      "valid loss: 0.006445742677897215\n",
      "valid loss: 0.01359113771468401\n",
      "valid loss: 0.00811461266130209\n",
      "valid loss: 0.00855631846934557\n",
      "valid loss: 0.012333860620856285\n",
      "valid loss: 0.008832008577883244\n",
      "valid loss: 0.006313313730061054\n",
      "valid loss: 0.007794629782438278\n",
      "valid loss: 0.005387544631958008\n",
      "valid loss: 0.008430096320807934\n",
      "valid loss: 0.002790815196931362\n",
      "valid loss: 0.01158077921718359\n",
      "valid loss: 0.01081536989659071\n",
      "valid loss: 0.0079592764377594\n",
      "valid loss: 0.00910022109746933\n",
      "valid loss: 0.007288520224392414\n",
      "valid loss: 0.00862449686974287\n",
      "valid loss: 0.00745846051722765\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:48<00:00, 20.76it/s]\n",
      "0it [00:48, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.006202549673616886\n",
      "valid loss: 0.010790250264108181\n",
      "valid loss: 0.008483140729367733\n",
      "valid loss: 0.012363786809146404\n",
      "valid loss: 0.006213061511516571\n",
      "valid loss: 0.005668238736689091\n",
      "valid loss: 0.009470432065427303\n",
      "valid loss: 0.005093584768474102\n",
      "valid loss: 0.011751674115657806\n",
      "valid loss: 0.010045419447124004\n",
      "valid loss: 0.004608652088791132\n",
      "valid loss: 0.00975386705249548\n",
      "valid loss: 0.004335814621299505\n",
      "valid loss: 0.00993011798709631\n",
      "valid loss: 0.008410741575062275\n",
      "valid loss: 0.005480055697262287\n",
      "valid loss: 0.010018798522651196\n",
      "valid loss: 0.006615058984607458\n",
      "valid loss: 0.009657257236540318\n",
      "valid loss: 0.00675449101254344\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:44<00:00, 22.22it/s]\n",
      "0it [00:45, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.00642593577504158\n",
      "valid loss: 0.007435027044266462\n",
      "valid loss: 0.006391589995473623\n",
      "valid loss: 0.008599533699452877\n",
      "valid loss: 0.005195121746510267\n",
      "valid loss: 0.0056032720021903515\n",
      "valid loss: 0.009496055543422699\n",
      "valid loss: 0.006817270070314407\n",
      "valid loss: 0.010386916808784008\n",
      "valid loss: 0.007645599544048309\n",
      "valid loss: 0.0038466956466436386\n",
      "valid loss: 0.009328744374215603\n",
      "valid loss: 0.007110488601028919\n",
      "valid loss: 0.01191024947911501\n",
      "valid loss: 0.0083148879930377\n",
      "valid loss: 0.00669734925031662\n",
      "valid loss: 0.007195673882961273\n",
      "valid loss: 0.00469167297706008\n",
      "valid loss: 0.007374861743301153\n",
      "valid loss: 0.008500120602548122\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:39<00:00, 25.47it/s]\n",
      "0it [00:39, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.005373069085180759\n",
      "valid loss: 0.007639619987457991\n",
      "valid loss: 0.0058811018243432045\n",
      "valid loss: 0.00901543628424406\n",
      "valid loss: 0.00682677561417222\n",
      "valid loss: 0.004656483419239521\n",
      "valid loss: 0.007257204037159681\n",
      "valid loss: 0.005464732646942139\n",
      "valid loss: 0.00977238267660141\n",
      "valid loss: 0.005594031419605017\n",
      "valid loss: 0.003983765374869108\n",
      "valid loss: 0.007964836433529854\n",
      "valid loss: 0.003510687267407775\n",
      "valid loss: 0.008823463693261147\n",
      "valid loss: 0.009196844883263111\n",
      "valid loss: 0.006358177866786718\n",
      "valid loss: 0.005899365525692701\n",
      "valid loss: 0.0062963091768324375\n",
      "valid loss: 0.006845247931778431\n",
      "valid loss: 0.006446675397455692\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:39<00:00, 25.64it/s]\n",
      "0it [00:39, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.00772020174190402\n",
      "valid loss: 0.00791101437062025\n",
      "valid loss: 0.004157820716500282\n",
      "valid loss: 0.00801832415163517\n",
      "valid loss: 0.006585156079381704\n",
      "valid loss: 0.003751336829736829\n",
      "valid loss: 0.009115665219724178\n",
      "valid loss: 0.004114388022571802\n",
      "valid loss: 0.007633686065673828\n",
      "valid loss: 0.007197733968496323\n",
      "valid loss: 0.004269130527973175\n",
      "valid loss: 0.0066002532839775085\n",
      "valid loss: 0.004592688288539648\n",
      "valid loss: 0.009788990020751953\n",
      "valid loss: 0.0037871592212468386\n",
      "valid loss: 0.005172679666429758\n",
      "valid loss: 0.005960770882666111\n",
      "valid loss: 0.002866522641852498\n",
      "valid loss: 0.008792994543910027\n",
      "valid loss: 0.0052202907390892506\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:39<00:00, 25.55it/s]\n",
      "0it [00:39, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.00546037033200264\n",
      "valid loss: 0.007659864611923695\n",
      "valid loss: 0.00511659262701869\n",
      "valid loss: 0.00621434673666954\n",
      "valid loss: 0.007904489524662495\n",
      "valid loss: 0.005208795890212059\n",
      "valid loss: 0.007503500673919916\n",
      "valid loss: 0.004236615262925625\n",
      "valid loss: 0.00838144589215517\n",
      "valid loss: 0.005477188620716333\n",
      "valid loss: 0.0066963559947907925\n",
      "valid loss: 0.006108559668064117\n",
      "valid loss: 0.004544887226074934\n",
      "valid loss: 0.005518184043467045\n",
      "valid loss: 0.008452284149825573\n",
      "valid loss: 0.0039927843026816845\n",
      "valid loss: 0.0026031166780740023\n",
      "valid loss: 0.004706013482064009\n",
      "valid loss: 0.007000885903835297\n",
      "valid loss: 0.008126816712319851\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:39<00:00, 25.50it/s]\n",
      "0it [00:39, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.005127099808305502\n",
      "valid loss: 0.005583592690527439\n",
      "valid loss: 0.0020399894565343857\n",
      "valid loss: 0.006974907126277685\n",
      "valid loss: 0.008809690363705158\n",
      "valid loss: 0.0038068839348852634\n",
      "valid loss: 0.0042213802225887775\n",
      "valid loss: 0.0027861776761710644\n",
      "valid loss: 0.010470437817275524\n",
      "valid loss: 0.006864666938781738\n",
      "valid loss: 0.0034580689389258623\n",
      "valid loss: 0.0065537854097783566\n",
      "valid loss: 0.0035679207649081945\n",
      "valid loss: 0.00739654153585434\n",
      "valid loss: 0.006513823755085468\n",
      "valid loss: 0.004586127586662769\n",
      "valid loss: 0.008333833888173103\n",
      "valid loss: 0.004175384528934956\n",
      "valid loss: 0.0066811563447117805\n",
      "valid loss: 0.006009380333125591\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:40<00:00, 24.44it/s]\n",
      "0it [00:41, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to imagen_two_unets_24s.pt\n",
      "valid loss: 0.003190948162227869\n",
      "valid loss: 0.004620830994099379\n",
      "valid loss: 0.0038663402665406466\n",
      "valid loss: 0.00857212208211422\n",
      "valid loss: 0.007263661362230778\n",
      "valid loss: 0.00459773326292634\n",
      "valid loss: 0.0058495341800153255\n",
      "valid loss: 0.004806817509233952\n",
      "valid loss: 0.006487388163805008\n",
      "valid loss: 0.0047182985581457615\n",
      "valid loss: 0.0067859129048883915\n",
      "valid loss: 0.005218944046646357\n",
      "valid loss: 0.002261386252939701\n",
      "valid loss: 0.005329435225576162\n",
      "valid loss: 0.006759905721992254\n",
      "valid loss: 0.005063430871814489\n",
      "valid loss: 0.005827638320624828\n",
      "valid loss: 0.004408302716910839\n",
      "valid loss: 0.006252015475183725\n",
      "valid loss: 0.004411484580487013\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:49<00:00, 20.30it/s]\n",
      "0it [00:49, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss: 0.006041982676833868\n",
      "valid loss: 0.008196547627449036\n",
      "valid loss: 0.0032660861033946276\n",
      "valid loss: 0.0061682346276938915\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import train_imagen_batch\n",
    "\n",
    "\n",
    "\n",
    "range_of_epochs = range(1,epochs + 1)\n",
    "\n",
    "if(continue_from_epoch_and_offset_flag != 0):\n",
    "    \n",
    "    range_of_epochs = range(continue_from_epoch, epochs + 1)\n",
    "\n",
    "offset_zero_flag = 0\n",
    "\n",
    "for epoch in range_of_epochs:\n",
    "\n",
    "    print(\"Training in epoch \" + str(epoch))\n",
    "\n",
    "    offset = 0\n",
    "\n",
    "    if(continue_from_epoch_and_offset_flag != 0 and offset_zero_flag == 0):\n",
    "        offset = continue_from_offset\n",
    "        offset_zero_flag = 1\n",
    "\n",
    "    while(True):\n",
    "\n",
    "        print(\"Training in offset \" + str(offset))\n",
    "\n",
    "        data = con.execute(\"SELECT V.ID, V.VIDEO_PATH, V.AGE, \" + \n",
    "                            \"V.ETHNICITY CAPTION_E, \" +\n",
    "                            \"lower(V.GENDER) CAPTION_G, \" +\n",
    "                            \"A.SPEAKER_EMB, \"+ \"A.AUDIO_EMB2, \" +\n",
    "                            \"A.LANG CAPTION_L, \"+\n",
    "                            \"F.FACE_PATH \"+\n",
    "                            \"FROM VIDEO V \"+\n",
    "                            \"INNER JOIN AUDIO A ON V.ID = A.VIDEO_ID INNER JOIN FACE F ON V.ID = F.VIDEO_ID WHERE AUDIO_PRE = 2 AND FACES_PRE = 1 AND AUDIO_LENGTH = \" + audio_length_used + ' ' +\n",
    "                            \"LIMIT \"+ str(db_chunk) +\" OFFSET \" + str(offset))\n",
    "        dataGotten = data.fetchall()\n",
    "\n",
    "        if(len(dataGotten) == 0):\n",
    "            break\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(dataGotten,columns = ['ID','VIDEO_PATH','AGE','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','caption_l','image_path'])\n",
    "\n",
    "        df3 = df[[\"image_path\",\"caption_e\",\"caption_g\",\"caption_l\"]]\n",
    "\n",
    "        data_frame = df3\n",
    "        data_frame['SPEAKER_EMB'] = df['SPEAKER_EMB']\n",
    "        data_frame['AUDIO_EMB'] = df['AUDIO_EMB']\n",
    "\n",
    "\n",
    "        for index, row in data_frame.iterrows():\n",
    "            x = speaker_emb_preprocess(data_frame.loc[index,\"SPEAKER_EMB\"])\n",
    "            x = [x]\n",
    "            data_frame.loc[index,\"SPEAKER_EMB\"] = x\n",
    "\n",
    "        #CHECK\n",
    "        for index, row in data_frame.iterrows():\n",
    "            x = audio_emb_preprocess2(data_frame.loc[index,\"AUDIO_EMB\"])\n",
    "            x = [x]\n",
    "            #AADFS = AADFS\n",
    "            data_frame.loc[index,\"AUDIO_EMB\"] = x\n",
    "\n",
    "        for index, row in data_frame.iterrows():\n",
    "            x = process_gender(data_frame.loc[index,\"caption_g\"])\n",
    "            x = [x]\n",
    "            #AADFS = AADFS\n",
    "            data_frame.loc[index,\"caption_g\"] = x\n",
    "\n",
    "        for index, row in data_frame.iterrows():\n",
    "            x = process_language(data_frame.loc[index,\"caption_l\"])\n",
    "            x = [x]\n",
    "            #AADFS = AADFS\n",
    "            data_frame.loc[index,\"caption_l\"] = x\n",
    "            \n",
    "        for index, row in data_frame.iterrows():\n",
    "            x = process_ethnicity(data_frame.loc[index,\"caption_e\"])\n",
    "            x = [x]\n",
    "            #AADFS = AADFS\n",
    "            data_frame.loc[index,\"caption_e\"] = x\n",
    "\n",
    "        data_frame['INPUT'] = data_frame['SPEAKER_EMB']\n",
    "\n",
    "        for index, row in data_frame.iterrows():\n",
    "            x = process_input(data_frame.loc[index,\"caption_e\"],data_frame.loc[index,\"caption_g\"],\n",
    "                            data_frame.loc[index,\"caption_l\"],data_frame.loc[index,\"SPEAKER_EMB\"],\n",
    "                            data_frame.loc[index,\"AUDIO_EMB\"])\n",
    "            x = [x]\n",
    "            #AADFS = AADFS\n",
    "            data_frame.loc[index,\"INPUT\"] = x\n",
    "\n",
    "        data_frame = data_frame.drop(['caption_e', 'caption_g','caption_l','SPEAKER_EMB','AUDIO_EMB'], axis=1)\n",
    "\n",
    "        for index, row in data_frame.iterrows():\n",
    "            x = process_image_path(data_frame.loc[index,\"image_path\"])\n",
    "            x = [x]\n",
    "            #AADFS = AADFS\n",
    "            data_frame.loc[index,\"image_path\"] = x\n",
    "\n",
    "        input = data_frame['INPUT'].to_numpy()\n",
    "        input = np.array([np.array(xi) for xi in input])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        output = data_frame['image_path'].to_numpy()\n",
    "        output = np.array([np.array(xi) for xi in output])\n",
    "        output.squeeze().shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        proc = multiprocessing.Process(target=train_imagen_batch.train_batch_unet1, args=(input,output,model_filename,sub_epochs,batch_size,sample_every,save_model_every,image_size,))\n",
    "        proc.start()\n",
    "        proc.join()\n",
    "\n",
    "        proc = multiprocessing.Process(target=train_imagen_batch.train_batch_unet2, args=(input,output,model_filename,sub_epochs,batch_size,sample_every,save_model_every,image_size,))\n",
    "        proc.start()\n",
    "        proc.join()\n",
    "        offset = offset + db_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78dccc615e96ab04385280185a87a524fe0822daf5dbad2f4bf2e7d7b28366a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
