{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "# Loading configurations\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "\n",
    "insert_amd_env_vars =  int(configParser.get('COMMON', 'insert_amd_env_vars'))\n",
    "HSA_OVERRIDE_GFX_VERSION =  configParser.get('COMMON', 'HSA_OVERRIDE_GFX_VERSION')\n",
    "ROCM_PATH =  configParser.get('COMMON', 'ROCM_PATH')\n",
    "\n",
    "if(insert_amd_env_vars != 0):\n",
    "    os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = HSA_OVERRIDE_GFX_VERSION\n",
    "    os.environ[\"ROCM_PATH\"] = ROCM_PATH\n",
    "    \n",
    "#import os\n",
    "#os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "#os.environ[\"AUTOGRAPH_VERBOSITY\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import configparser\n",
    "\n",
    "# Loading configurations\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "\n",
    "\n",
    "datasetPathVideo =  configParser.get('COMMON', 'test_datasetPathVideo')\n",
    "\n",
    "datasetPathFeatures =  configParser.get('evaluate_imagen', 'test_datasetPathFeatures')\n",
    "datasetPathGeneratedFaces =  configParser.get('evaluate_imagen', 'test_datasetPathGeneratedFaces')\n",
    "\n",
    "datasetPathDatabase =  configParser.get('COMMON', 'test_datasetPathDatabase') + '/dataset.db'\n",
    "\n",
    "ttwbdf =  int(configParser.get('evaluate_imagen', 'time_to_wait_before_deleting_files'))\n",
    "\n",
    "\n",
    "cuda =  int(configParser.get('COMMON', 'cuda'))\n",
    "cpus =  int(configParser.get('COMMON', 'cpus'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO document jupyter\n",
    "import pickle\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from textwrap import wrap\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size =  int(configParser.get('COMMON', 'resizeImageTo'))\n",
    "begin_with_image_size = int(configParser.get('COMMON', 'begin_with_image_size'))\n",
    "timesteps= int(configParser.get('COMMON', 'timesteps') )\n",
    "unet1_image_size =  int(configParser.get('COMMON', 'unet1_image_size'))\n",
    "audio_length_used =  configParser.get('evaluate_imagen', 'audio_length_used') \n",
    "model_filename =  configParser.get('evaluate_imagen', 'model_filename') + '_'  + audio_length_used +  's.pt'\n",
    "openl3_mode =  configParser.get('evaluate_imagen', 'openl3_mode')\n",
    "folder =  configParser.get('evaluate_imagen', 'folder')\n",
    "audio_embs =  configParser.get('COMMON', 'audio_embs') \n",
    "unet_dim =  int(configParser.get('COMMON', 'unet_dim'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_speaker_embedding = bool(int(configParser.get('train_imagen', 'ignore_speaker_embedding') ))\n",
    "ignore_speech_brain = bool(int(configParser.get('train_imagen', 'ignore_speech_brain') ))\n",
    "ignore_pyannote_titanet_speakernet = bool(int(configParser.get('train_imagen', 'ignore_pyannote_titanet_speakernet') ))\n",
    "ignore_audio_features = bool(int(configParser.get('train_imagen', 'ignore_audio_features') ))\n",
    "ignore_pyAudioAnalysis = bool(int(configParser.get('train_imagen', 'ignore_pyAudioAnalysis') ))\n",
    "ignore_librosa = bool(int(configParser.get('train_imagen', 'ignore_librosa') ))\n",
    "ignore_image_guide = bool(int(configParser.get('train_imagen', 'ignore_image_guide') ))\n",
    "ignore_additional_attributes = bool(int(configParser.get('train_imagen', 'ignore_additional_attributes') ))\n",
    "ignore_age = bool(int(configParser.get('train_imagen', 'ignore_age') ))\n",
    "ignore_gender = bool(int(configParser.get('train_imagen', 'ignore_gender') ))\n",
    "ignore_ethnicity = bool(int(configParser.get('train_imagen', 'ignore_ethnicity') ))\n",
    "ignore_language_spoken = bool(int(configParser.get('train_imagen', 'ignore_language_spoken') ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_speaker_embedding = bool(int(configParser.get('evaluate_imagen', 'skip_speaker_embedding') ))\n",
    "skip_speech_brain = bool(int(configParser.get('evaluate_imagen', 'skip_speech_brain') ))\n",
    "skip_pyannote_titanet_speakernet = bool(int(configParser.get('evaluate_imagen', 'skip_pyannote_titanet_speakernet') ))\n",
    "skip_audio_features = bool(int(configParser.get('evaluate_imagen', 'skip_audio_features') ))\n",
    "skip_pyAudioAnalysis = bool(int(configParser.get('evaluate_imagen', 'skip_pyAudioAnalysis') ))\n",
    "skip_librosa = bool(int(configParser.get('evaluate_imagen', 'skip_librosa') ))\n",
    "skip_image_guide = bool(int(configParser.get('evaluate_imagen', 'skip_image_guide') ))\n",
    "skip_additional_attributes = bool(int(configParser.get('evaluate_imagen', 'skip_additional_attributes') ))\n",
    "skip_age = bool(int(configParser.get('evaluate_imagen', 'skip_age') ))\n",
    "skip_gender = bool(int(configParser.get('evaluate_imagen', 'skip_gender') ))\n",
    "skip_ethnicity = bool(int(configParser.get('evaluate_imagen', 'skip_ethnicity') ))\n",
    "skip_language_spoken = bool(int(configParser.get('evaluate_imagen', 'skip_language_spoken') ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speaker_emb_preprocess(speaker_emb2):\n",
    "\n",
    "    if(ignore_speaker_embedding or ignore_speech_brain \n",
    "       or skip_speaker_embedding or skip_speech_brain):\n",
    "        #print('ignore speech brain')\n",
    "        return np.zeros((1,768)).tolist()\n",
    "    else:\n",
    "        speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "        #print(speaker_emb2.print)\n",
    "        speaker_emb2 = speaker_emb2.squeeze()\n",
    "        speaker_emb2 = np.pad(speaker_emb2, (288), 'constant', constant_values=(0))\n",
    "        speaker_emb2 = np.tile(speaker_emb2, (1, 1))\n",
    "        speaker_emb2 = speaker_emb2 / 200.0\n",
    "        #print(speaker_emb2.shape)\n",
    "        speaker_emb2 = np.array(speaker_emb2).tolist()\n",
    "        return speaker_emb2\n",
    "\n",
    "\n",
    "def audio_emb_preprocess2(speaker_emb2):\n",
    "    \n",
    "    if(ignore_pyannote_titanet_speakernet or ignore_speaker_embedding\n",
    "       or skip_pyannote_titanet_speakernet or skip_speaker_embedding):\n",
    "        #print('ignore speech brain')\n",
    "        return np.zeros((28,768)).tolist()\n",
    "    else:\n",
    "        speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "        a = speaker_emb2 #np.zeros(shape=(24, 512))\n",
    "        a = a[-28:,:] # Actually only last 39 are the relevant ones\n",
    "        #print('au')\n",
    "        #print(a.shape)\n",
    "        b = np.zeros(shape=(a.shape[0], 768-a.shape[1]))\n",
    "        #c = np.zeros(shape=(28-a.shape[0], 768))\n",
    "        arr = np.concatenate((a, b), axis=1)\n",
    "        #arr = np.concatenate((arr, c), axis=0)\n",
    "        arr = arr / (1 if(audio_embs == 'wav2vec') else 10 if(audio_embs == 'openl3')  else 1 if(audio_embs == 'pyannoteTitaNet') else 1)\n",
    "        #print(str(arr.max()))\n",
    "        #print(arr.shape)\n",
    "        speaker_emb2 = np.array(arr).tolist()\n",
    "        return speaker_emb2\n",
    "\n",
    "\n",
    "\n",
    "def audio_features_preprocess(audio_features):\n",
    "    #  79 belong to pyaudioanalysis\n",
    "    # 111 belogn to liborsa\n",
    "    if(ignore_audio_features or skip_audio_features):\n",
    "        #print('ignore speech brain')\n",
    "        return np.zeros((190,768)).tolist()\n",
    "    else:\n",
    "        #print(video_id)\n",
    "        audio_features = pickle.loads(audio_features)\n",
    "        #print(audio_features.shape) # 190 x 128\n",
    "        audio_features = audio_features[0:190]\n",
    "        \n",
    "        #import sys\n",
    "        #np.set_printoptions(threshold=sys.maxsize)\n",
    "        #print(np.argwhere(audio_features == 0))\n",
    "\n",
    "        if((ignore_pyAudioAnalysis and not ignore_librosa)\n",
    "           or (skip_pyAudioAnalysis and not skip_librosa)):\n",
    "            zpa = np.zeros((190-110,128))\n",
    "            audio_features = audio_features[80:190]\n",
    "            audio_features = np.vstack((audio_features,zpa))\n",
    "            #print('ignore pyAudioAnalysis')\n",
    "            #print(audio_features.shape)\n",
    "        elif((ignore_librosa and not ignore_pyAudioAnalysis)\n",
    "             or(skip_librosa and not skip_pyAudioAnalysis)):\n",
    "            zpa = np.zeros((190-80,128))\n",
    "            audio_features = audio_features[0:80]\n",
    "            audio_features = np.vstack((audio_features,zpa))\n",
    "            #print('ignore librosa')\n",
    "            #print(audio_features.shape)\n",
    "        else:\n",
    "            audio_features = np.zeros((190,128))\n",
    "            #print('ignore pyaudioanalysis and librosa')\n",
    "\n",
    "\n",
    "        z1 = np.zeros((190,768-128))\n",
    "        audio_features = np.hstack((audio_features,z1))\n",
    "        audio_features = audio_features / 100.0\n",
    "        #print(audio_features.shape)\n",
    "    return audio_features\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def make_square(im, min_size=image_size, fill_color=(0, 0, 0, 0)):\n",
    "    x, y = im.size\n",
    "    size = max(min_size, x, y)\n",
    "    new_im = Image.new('RGB', (size, size), fill_color)\n",
    "    new_im.paste(im, (int((size - x) / 2), int((size - y) / 2)))\n",
    "    im.close()\n",
    "    return new_im\n",
    "\n",
    "def image_guide_preprocess_low_res_dummy(path):\n",
    "    #print(face_id)\n",
    "\n",
    "    image_guide = np.zeros((1, 768))\n",
    "    return image_guide\n",
    "\n",
    "def image_guide_preprocess_low_res(path):\n",
    "    #print(face_id)\n",
    "\n",
    "\n",
    "    if(random.random() > 2): #never\n",
    "        image_guide = np.zeros((49, 768))\n",
    "        #print(image_guide.shape)\n",
    "    else:\n",
    "\n",
    "        image = Image.open(path).convert('RGB')\n",
    "\n",
    "        w_s = image_size / (1+2 * 0.5)\n",
    "        h_s = image_size / (1+2 * 0.4)\n",
    "\n",
    "        image = image.crop((0.4*w_s, 0.0*h_s, 1.6*w_s, 1.4*h_s))\n",
    "\n",
    "        image = make_square(image)\n",
    "\n",
    "        image = image.resize((begin_with_image_size,begin_with_image_size))\n",
    "        im = image\n",
    "        \n",
    "\n",
    "        #print('saving')\n",
    "        #image.save('opop.png')\n",
    "\n",
    "        #print(np.array(image,np.float32).shape)\n",
    "\n",
    "        pix = np.array(image, np.float32)\n",
    "        pix = np.moveaxis(pix, -1, 0)\n",
    "\n",
    "        pix = pix / 255\n",
    "        image.close()\n",
    "        im.close()\n",
    "        return pix.tolist()\n",
    "    return image_guide\n",
    "\n",
    "\n",
    "def image_guide_preprocess(image_guide):\n",
    "\n",
    "    image_guide = pickle.loads(image_guide)\n",
    "    print(image_guide.shape)\n",
    "    image_guide = image_guide.squeeze() #197 x 768\n",
    "    image_guide = image_guide[1::4] #49 x 768\n",
    "    return image_guide\n",
    "\n",
    "from PIL import Image,ImageFilter\n",
    "import random\n",
    "#import cv2\n",
    "import numpy as np\n",
    "#from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def process_age(age):\n",
    "    if(ignore_age or skip_age):\n",
    "        x = np.zeros(768)\n",
    "        x[767] = 1\n",
    "        return x\n",
    "    elif(age > 100 or age < 0):\n",
    "        x = np.zeros(768)\n",
    "        x[767] = 1\n",
    "        return x\n",
    "    else:\n",
    "        x = np.ones(768) * (age / 100.0)\n",
    "        x[767] = 0\n",
    "        return x\n",
    "\n",
    "def process_gender(gender):\n",
    "    if(ignore_gender or skip_gender):\n",
    "        return np.zeros(768) \n",
    "    elif(gender == 'man'):\n",
    "        return np.ones(768)\n",
    "    elif(gender == \"woman\"):\n",
    "        return np.ones(768) * -1\n",
    "    else:\n",
    "        return np.zeros(768)\n",
    "    \n",
    "# TODO\n",
    "def process_ethnicity(eth):\n",
    "    x = np.zeros(768)\n",
    "    if(ignore_ethnicity or skip_ethnicity):\n",
    "        x = x\n",
    "    elif(eth == \"indian\"):\n",
    "        x[0] = 1\n",
    "    elif(eth == \"asian\"):\n",
    "        x[16]=1\n",
    "    elif(eth == \"latino hispanic\"):\n",
    "        x[2]=1\n",
    "    elif(eth == \"black\"):\n",
    "        x[3]=1\n",
    "    elif(eth == \"middle eastern\"):\n",
    "        x[4]=1\n",
    "    elif(eth == \"white\"):\n",
    "        x[5]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x     \n",
    "\n",
    "def process_language(lan):\n",
    "    x = np.zeros(768)\n",
    "    if(ignore_language_spoken or skip_language_spoken):\n",
    "        x = x\n",
    "    elif(lan == \"Arabic\"):\n",
    "        x[0] = 1\n",
    "    elif(lan == \"Portuguese\"):\n",
    "        x[16]=1\n",
    "    elif(lan == \"Romansh_Sursilvan\"):\n",
    "        x[2]=1\n",
    "    elif(lan == \"Japanese\"):\n",
    "        x[3]=1\n",
    "    elif(lan == \"Ukranian\"):\n",
    "        x[4]=1\n",
    "    elif(lan == \"German\"):\n",
    "        x[5]=1   \n",
    "    elif(lan == \"Chinese_China\"):\n",
    "        x[6]=1   \n",
    "    elif(lan == \"Welsh\"):\n",
    "        x[7]=1  \n",
    "    elif(lan == \"English\"):\n",
    "        x[8]=1\n",
    "    elif(lan == \"Kabyle\"):\n",
    "        x[9]=1 \n",
    "    elif(lan == \"Kyrgyz\"):\n",
    "        x[10]=1\n",
    "    elif(lan == \"Georgian\"):\n",
    "        x[11]=1\n",
    "    elif(lan == \"Persian\"):\n",
    "        x[12]=1 \n",
    "    elif(lan == \"French\"):\n",
    "        x[13]=1\n",
    "    elif(lan == \"Interlingua\"):\n",
    "        x[14]=1\n",
    "    elif(lan == \"Swedish\"):\n",
    "        x[15]=1\n",
    "    elif(lan == \"Spanish\"):\n",
    "        x[16]=1 \n",
    "    elif(lan == \"Dhivehi\"):\n",
    "        x[17]=1\n",
    "    elif(lan == \"Kinyarwanda\"):\n",
    "        x[18]=1 \n",
    "    elif(lan == \"Tatar\"):\n",
    "        x[19]=1\n",
    "    elif(lan == \"Hakha_Chin\"):\n",
    "        x[20]=1 \n",
    "    elif(lan == \"Tamil\"):\n",
    "        x[21]=1 \n",
    "    elif(lan == \"Greek\"):\n",
    "        x[22]=1\n",
    "    elif(lan == \"Latvian\"):\n",
    "        x[23]=1 \n",
    "    elif(lan == \"Russian\"):\n",
    "        x[24]=1\n",
    "    elif(lan == \"Breton\"):\n",
    "        x[25]=1\n",
    "    elif(lan == \"Catalan\"):\n",
    "        x[26]=1    \n",
    "    elif(lan == \"Maltese\"):\n",
    "        x[27]=1 \n",
    "    elif(lan == \"Slovenian\"):\n",
    "        x[28]=1    \n",
    "    elif(lan == \"Indonesian\"):\n",
    "        x[29]=1    \n",
    "    elif(lan == \"Dutch\"):\n",
    "        x[30]=1\n",
    "    elif(lan == \"Chinese_Taiwan\"):\n",
    "        x[31]=1 \n",
    "    elif(lan == \"Sakha\"):\n",
    "        x[32]=1 \n",
    "    elif(lan == \"Polish\"):\n",
    "        x[33]=1 \n",
    "    elif(lan == \"Czech\"):\n",
    "        x[34]=1 \n",
    "    elif(lan == \"Romanian\"):\n",
    "        x[35]=1 \n",
    "    elif(lan == \"Mangolian\"):\n",
    "        x[36]=1 \n",
    "    elif(lan == \"Italian\"):\n",
    "        x[37]=1 \n",
    "    elif(lan == \"Chinese_Hongkong\"):\n",
    "        x[38]=1 \n",
    "    elif(lan == \"Estonian\"):\n",
    "        x[39]=1 \n",
    "    elif(lan == \"Basque\"):\n",
    "        x[40]=1 \n",
    "    elif(lan == \"Esperanto\"):\n",
    "        x[41]=1 \n",
    "    elif(lan == \"Frisian\"):\n",
    "        x[42]=1 \n",
    "    elif(lan == \"Turkish\"):\n",
    "        x[43]=1 \n",
    "    elif(lan == \"Chuvash\"):\n",
    "        x[44]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(age,ethnicity,gender,language,speaker_emb,audio_emb,audio_features,image_guide):\n",
    "    speaker_emb = np.asarray(speaker_emb, dtype=np.float32)\n",
    "    speaker_emb = speaker_emb.squeeze()\n",
    "    audio_emb = np.asarray(audio_emb, dtype=np.float32)\n",
    "    audio_emb = audio_emb.squeeze()\n",
    "    #print(speaker_emb.shape)\n",
    "    h = np.vstack((age, ethnicity))\n",
    "    h = np.vstack((h, gender))\n",
    "    h = np.vstack((h, language))\n",
    "    h = np.vstack((h, speaker_emb))\n",
    "    h = np.vstack((h, audio_emb))\n",
    "    h = np.vstack((h,audio_features))\n",
    "    h = np.vstack((h,image_guide))\n",
    "    #print('aaaaaaaaaaa')\n",
    "    #print(h.shape)\n",
    "    j = np.zeros(768)\n",
    "    j = np.tile(j,(256-h.shape[0],1))\n",
    "    h = np.vstack((h, j))\n",
    "    #print(h.shape)\n",
    "    return h.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "uri = datasetPathDatabase\n",
    "db = lancedb.connect(uri)\n",
    "\n",
    "table = db.open_table(\"video_stage1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from imagen_pytorch import Unet, Imagen, ImagenTrainer,NullUnet\n",
    "from imagen_pytorch.data import Dataset\n",
    "\n",
    "unet0 = NullUnet()  # add a placeholder \"null\" unet for the base unet\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = unet_dim,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = 3,\n",
    "    layer_attns = (False, True, True, True),\n",
    "    layer_cross_attns = (False, True, True, True)\n",
    ")\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = unet_dim,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = (2, 4, 8, 8),\n",
    "    layer_attns = (False, False, False, True),\n",
    "    layer_cross_attns = (False, False, False, True)\n",
    ")\n",
    "\n",
    "#unet = Unet(\n",
    "#    dim = 32,\n",
    "#    dim_mults = (1, 2, 4, 8),\n",
    "#    num_resnet_blocks = 1,\n",
    "#    layer_attns = (False, False, False, True),\n",
    "#    layer_cross_attns = False\n",
    "#)\n",
    "\n",
    "# imagen, which contains the unet above\n",
    "\n",
    "#imagen = Imagen(\n",
    "#    unets = unet,\n",
    "#    image_sizes = 32,\n",
    "#    timesteps = 1000\n",
    "#)\n",
    "\n",
    "imagen = Imagen(\n",
    "    unets = (unet0,unet1, unet2),\n",
    "    image_sizes = (begin_with_image_size,unet1_image_size, image_size),\n",
    "    timesteps = timesteps,\n",
    "    cond_drop_prob = 0.1\n",
    ").cuda()\n",
    "\n",
    "trainer = ImagenTrainer(\n",
    "    imagen = imagen,\n",
    "    split_valid_from_train = True # whether to split the validation dataset from the training\n",
    ").cuda()\n",
    "\n",
    "print('Using model file ' + model_filename)\n",
    "trainer.load(model_filename)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pathlib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for index, row in table.to_pandas().iterrows():\n",
    "\n",
    "    absPathVideo = row['video_path']\n",
    "    absPathFace = row['face_path']\n",
    "\n",
    "    data_frame = None\n",
    "    \n",
    "\n",
    "    with open(row['features_path'], 'rb') as pickle_file:\n",
    "        data_frame = pickle.load(pickle_file)\n",
    "\n",
    "    data_frame['low_res_image'] = absPathFace\n",
    "\n",
    "    data_frame['SPEAKER_EMB'] = data_frame['SPEAKER_EMB'].apply(lambda x: speaker_emb_preprocess(x))\n",
    "    data_frame['AUDIO_EMB'] = data_frame['AUDIO_EMB'].apply(lambda x: audio_emb_preprocess2(x))\n",
    "    data_frame['AUDIO_FEATURES'] = data_frame['AUDIO_FEATURES'].apply(lambda x: audio_features_preprocess(x))\n",
    "    data_frame['image_guide'] = data_frame['image_guide'].apply(lambda x: image_guide_preprocess_low_res_dummy(x))\n",
    "    data_frame['low_res_image'] = data_frame['low_res_image'].apply(lambda x: image_guide_preprocess_low_res(x))\n",
    "    data_frame['caption_a'] = data_frame['caption_a'].apply(lambda x: process_age(x))\n",
    "    data_frame['caption_g'] = data_frame['caption_g'].apply(lambda x: process_gender(x))\n",
    "    data_frame['caption_l'] = data_frame['caption_l'].apply(lambda x: process_language(x))\n",
    "    data_frame['caption_e'] = data_frame['caption_e'].apply(lambda x: process_ethnicity(x))\n",
    "\n",
    "    data_frame['INPUT'] = data_frame['SPEAKER_EMB']\n",
    "\n",
    "    for index, row in data_frame.iterrows():\n",
    "        x = process_input(data_frame.loc[index,\"caption_a\"],data_frame.loc[index,\"caption_e\"]\n",
    "                            ,data_frame.loc[index,\"caption_g\"],\n",
    "                        data_frame.loc[index,\"caption_l\"],data_frame.loc[index,\"SPEAKER_EMB\"],\n",
    "                        data_frame.loc[index,\"AUDIO_EMB\"],data_frame.loc[index,\"AUDIO_FEATURES\"],\n",
    "                        data_frame.loc[index,\"image_guide\"])\n",
    "        x = [x]\n",
    "        #AADFS = AADFS\n",
    "        data_frame.loc[index,\"INPUT\"] = x\n",
    "\n",
    "    data_frame = data_frame.drop(['caption_e', 'caption_g','caption_l','SPEAKER_EMB','AUDIO_EMB'], axis=1)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    input = data_frame['INPUT'].to_numpy()\n",
    "    input = np.array([np.array(xi) for xi in input])\n",
    "    input[np.isnan(input)] = 0\n",
    "    input[input > 10] = 10\n",
    "    input[input < -10] = -10\n",
    "\n",
    "    input2 = data_frame['low_res_image'].to_numpy()\n",
    "    input2 = np.array([np.array(xi) for xi in input2])\n",
    "\n",
    "    print(\"Data Preprocessed...\")\n",
    "\n",
    "    input = input.squeeze().squeeze()\n",
    "    input2 = input2.squeeze().squeeze()\n",
    "\n",
    "    input = torch.from_numpy(input)\n",
    "    input = input.to(torch.float)\n",
    "\n",
    "    input2 = torch.from_numpy(input2)\n",
    "    input2 = input2.to(torch.float)\n",
    "\n",
    "    input = input.squeeze()\n",
    "    input = input[None,:,:]\n",
    "\n",
    "    input2 = input2.squeeze()\n",
    "    input2 = input2[None,:,:]\n",
    "\n",
    "\n",
    "    absPathGeneratedFace = absPathVideo.replace(datasetPathVideo,datasetPathGeneratedFaces)\n",
    "    absPathGeneratedFace = os.path.splitext(absPathGeneratedFace)[0]\n",
    "    absPathGeneratedFace = absPathGeneratedFace + \"_generated_face_\"+ \".png\"\n",
    "    #print(absPathFace)\n",
    "    #Create Directory\n",
    "    pathlib.Path(os.path.dirname(absPathGeneratedFace)).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    images = trainer.sample(text_embeds=input,start_image_or_video = input2,\n",
    "                        stop_at_unet_number=3,start_at_unet_number = 2\n",
    "                        ,batch_size = 1, return_pil_images = True) # returns List[Image]\n",
    "    \n",
    "    images[0].save(absPathGeneratedFace)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os._exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78dccc615e96ab04385280185a87a524fe0822daf5dbad2f4bf2e7d7b28366a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
