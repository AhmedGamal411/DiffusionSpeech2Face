{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "# Loading configurations\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "\n",
    "insert_amd_env_vars =  int(configParser.get('COMMON', 'insert_amd_env_vars'))\n",
    "HSA_OVERRIDE_GFX_VERSION =  configParser.get('COMMON', 'HSA_OVERRIDE_GFX_VERSION')\n",
    "ROCM_PATH =  configParser.get('COMMON', 'ROCM_PATH')\n",
    "\n",
    "if(insert_amd_env_vars != 0):\n",
    "    os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = HSA_OVERRIDE_GFX_VERSION\n",
    "    os.environ[\"ROCM_PATH\"] = ROCM_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#TODO document jupyter\n",
    "import pickle\n",
    "import configparser\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "datasetPathDatabase =  configParser.get('COMMON', 'datasetPathDatabase') + '/dataset.db'\n",
    "datasetPathDatabaseAdditional =  configParser.get('COMMON', 'datasetPathDatabase') + '/datasetAdditional.db'\n",
    "datasetPathDatabaseVgg =  configParser.get('COMMON', 'datasetPathDatabase') + '/datasetFacesBlurred.db'\n",
    "image_size =  int(configParser.get('COMMON', 'resizeImageTo'))\n",
    "begin_with_image_size = int(configParser.get('COMMON', 'begin_with_image_size'))\n",
    "unet1_dim =  int(configParser.get('COMMON', 'unet1_dim'))\n",
    "unet2_dim =  int(configParser.get('COMMON', 'unet2_dim'))\n",
    "unet1_image_size =  int(configParser.get('COMMON', 'unet1_image_size'))\n",
    "audio_embs =  str(configParser.get('COMMON', 'audio_embs'))\n",
    "audio_length_used =  configParser.get('evaluate_imagen', 'audio_length_used') \n",
    "model_filename =  configParser.get('evaluate_imagen', 'model_filename') + '_'  + audio_length_used +  's.pt'\n",
    "cond_scale =  int(configParser.get('evaluate_imagen', 'cond_scale'))\n",
    "datasetPathVideo =  configParser.get('COMMON', 'test_datasetPathVideo')\n",
    "datasetPathGeneratedFaces =  configParser.get('evaluate_imagen', 'test_datasetPathGeneratedFaces')\n",
    "generated_face_table_name =  configParser.get('evaluate_imagen', 'generated_face_table_name')\n",
    "\n",
    "\n",
    "timesteps= int(configParser.get('COMMON', 'timesteps') )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#db_sampling_chunk = int(configParser.get('train_imagen', 'db_sampling_chunk'))\n",
    "sampling_chunk = int(configParser.get('evaluate_imagen', 'sampling_chunk'))\n",
    "preprocessing_chunk = int(configParser.get('evaluate_imagen', 'preprocessing_chunk'))\n",
    "\n",
    "\n",
    "\n",
    "ignore_speaker_embedding = bool(int(configParser.get('evaluate_imagen', 'ignore_speaker_embedding') ))\n",
    "ignore_speech_brain = bool(int(configParser.get('evaluate_imagen', 'ignore_speech_brain') ))\n",
    "ignore_pyannote_titanet_speakernet = bool(int(configParser.get('evaluate_imagen', 'ignore_pyannote_titanet_speakernet') ))\n",
    "ignore_audio_features = bool(int(configParser.get('evaluate_imagen', 'ignore_audio_features') ))\n",
    "ignore_pyAudioAnalysis = bool(int(configParser.get('evaluate_imagen', 'ignore_pyAudioAnalysis') ))\n",
    "ignore_librosa = bool(int(configParser.get('evaluate_imagen', 'ignore_librosa') ))\n",
    "ignore_image_guide = bool(int(configParser.get('evaluate_imagen', 'ignore_image_guide') ))\n",
    "ignore_additional_attributes = bool(int(configParser.get('evaluate_imagen', 'ignore_additional_attributes') ))\n",
    "ignore_age = bool(int(configParser.get('evaluate_imagen', 'ignore_age') ))\n",
    "ignore_gender = bool(int(configParser.get('evaluate_imagen', 'ignore_gender') ))\n",
    "ignore_ethnicity = bool(int(configParser.get('evaluate_imagen', 'ignore_ethnicity') ))\n",
    "ignore_language_spoken = bool(int(configParser.get('evaluate_imagen', 'ignore_language_spoken') ))\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetPathDatabase =  configParser.get('COMMON', 'test_datasetPathDatabase') + '/dataset.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lancedb\n",
    "#uri = datasetPathDatabase\n",
    "#db = lancedb.connect(uri)\n",
    "\n",
    "#table = db.open_table(\"video_stage1\")\n",
    "#videos = table.to_pandas()\n",
    "#videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speaker_emb_preprocess(speaker_emb2):\n",
    "\n",
    "    if(ignore_speaker_embedding or ignore_speech_brain):\n",
    "        #print('ignore speech brain')\n",
    "        return np.zeros((1,768))\n",
    "    else:\n",
    "        speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "        #print(speaker_emb2.print)\n",
    "        speaker_emb2 = speaker_emb2.squeeze()\n",
    "        speaker_emb2 = np.pad(speaker_emb2, (288), 'constant', constant_values=(0))\n",
    "        speaker_emb2 = np.tile(speaker_emb2, (1, 1))\n",
    "        speaker_emb2 = speaker_emb2 / 200.0\n",
    "        #print(speaker_emb2.shape)\n",
    "        #speaker_emb2 = np.array(speaker_emb2).tolist()\n",
    "        #print(type(speaker_emb2))\n",
    "        return speaker_emb2\n",
    "\n",
    "\n",
    "def audio_emb_preprocess2(speaker_emb2):\n",
    "    \n",
    "    if(ignore_pyannote_titanet_speakernet or ignore_speaker_embedding):\n",
    "        #print('ignore speech brain')\n",
    "        return np.zeros((28,768))\n",
    "    else:\n",
    "        #print(speaker_emb2)\n",
    "        speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "        a = speaker_emb2 #np.zeros(shape=(24, 512))\n",
    "        a = a[-28:,:] # Actually only last 39 are the relevant ones\n",
    "        #print('au')\n",
    "        #print(a.shape)\n",
    "        b = np.zeros(shape=(a.shape[0], 768-a.shape[1]))\n",
    "        #c = np.zeros(shape=(28-a.shape[0], 768))\n",
    "        arr = np.concatenate((a, b), axis=1)\n",
    "        #arr = np.concatenate((arr, c), axis=0)\n",
    "        arr = arr / (1 if(audio_embs == 'wav2vec') else 10 if(audio_embs == 'openl3')  else 1 if(audio_embs == 'pyannoteTitaNet') else 1)\n",
    "        #print(str(arr.max()))\n",
    "        #print(arr.shape)\n",
    "        speaker_emb2 = np.array(arr)\n",
    "        return speaker_emb2\n",
    "\n",
    "def audio_features_preprocess(audio_features):\n",
    "    #  79 belong to pyaudioanalysis\n",
    "    # 111 belogn to liborsa\n",
    "    if(ignore_audio_features):\n",
    "        #print('ignore speech brain')\n",
    "        return np.zeros((190,768))\n",
    "    else:\n",
    "        audio_features = pickle.loads(audio_features)\n",
    "        #print(audio_features.shape) # 190 x 128\n",
    "        audio_features = audio_features[0:190]\n",
    "        \n",
    "        #import sys\n",
    "        #np.set_printoptions(threshold=sys.maxsize)\n",
    "        #print(np.argwhere(audio_features == 0))\n",
    "\n",
    "        if(ignore_pyAudioAnalysis and not ignore_librosa):\n",
    "            zpa = np.zeros((190-110,128))\n",
    "            audio_features = audio_features[80:190]\n",
    "            audio_features = np.vstack((audio_features,zpa))\n",
    "            #print('ignore pyAudioAnalysis')\n",
    "            #print(audio_features.shape)\n",
    "        elif(ignore_librosa and not ignore_pyAudioAnalysis):\n",
    "            zpa = np.zeros((190-80,128))\n",
    "            audio_features = audio_features[0:80]\n",
    "            audio_features = np.vstack((audio_features,zpa))\n",
    "            #print('ignore librosa')\n",
    "            #print(audio_features.shape)\n",
    "        else:\n",
    "            audio_features = np.zeros((190,128))\n",
    "            #print('ignore pyaudioanalysis and librosa')\n",
    "\n",
    "\n",
    "        z1 = np.zeros((190,768-128))\n",
    "        audio_features = np.hstack((audio_features,z1))\n",
    "        audio_features = audio_features / 100.0\n",
    "        #print(audio_features.shape)\n",
    "    return audio_features\n",
    "\n",
    "def audio_transformer_features_preprocess(video_id):\n",
    "    #print(video_id)\n",
    "    conAdditional = sl.connect(datasetPathDatabaseAdditional)\n",
    "    cur = conAdditional.cursor()\n",
    "    sql = '''SELECT AUDIO_FEATURES FROM AUDIO_TRANSFORMER WHERE VIDEO_ID = ?'''\n",
    "    cur.execute(sql, [video_id])\n",
    "    audio_features = cur.fetchall()\n",
    "    #print(audio_features[0])\n",
    "    audio_features = pickle.loads(audio_features[0][0])\n",
    "    #print(audio_features.shape) # 514 x 768\n",
    "    audio_features = audio_features.squeeze()\n",
    "    audio_features = audio_features[0::3]#172*768\n",
    "    #z1 = np.zeros((161,768-128))\n",
    "    #audio_features = np.hstack((audio_features,z1))\n",
    "    #audio_features = audio_features / 100.0\n",
    "    return audio_features\n",
    "\n",
    "import random\n",
    "def image_guide_preprocess(face_id):\n",
    "    #print(face_id)\n",
    "\n",
    "    if(random.random() > 2):\n",
    "        image_guide = np.zeros((49,768))\n",
    "        #print(image_guide.shape)\n",
    "    else:\n",
    "        conAdditional = sl.connect(datasetPathDatabaseVgg)\n",
    "        cur = conAdditional.cursor()\n",
    "        sql = '''SELECT BLURRED_FACE_EMB FROM FACES_BLURRED WHERE FACE_ID = ?'''\n",
    "        cur.execute(sql, [face_id])\n",
    "        image_guide = cur.fetchall()\n",
    "        #print(image_guide)\n",
    "        image_guide = pickle.loads(image_guide[0][0])\n",
    "        image_guide = image_guide.squeeze() #197 x 768\n",
    "        image_guide = image_guide[1::4] #49 x 768\n",
    "    return image_guide\n",
    "\n",
    "\n",
    "#boxBlurMin =  int(configParser.get('extractVggBlurred', 'boxBlurMin'))\n",
    "#boxBlurMax =  int(configParser.get('extractVggBlurred', 'boxBlurMax'))\n",
    "\n",
    "#gaussianBlurMin =  int(configParser.get('extractVggBlurred', 'gaussianBlurMin'))\n",
    "#gaussianBlurMax =  int(configParser.get('extractVggBlurred', 'gaussianBlurMax'))\n",
    "\n",
    "\n",
    "from PIL import Image,ImageFilter\n",
    "import random\n",
    "#import cv2\n",
    "import numpy as np\n",
    "#from matplotlib import pyplot as plt\n",
    "  \n",
    "\n",
    "\n",
    "def image_guide_preprocess_low_res_dummy(path):\n",
    "    #print(face_id)\n",
    "\n",
    "    image_guide = np.zeros((1, 768))\n",
    "    return image_guide\n",
    "\n",
    "def image_guide_preprocess_low_res(path):\n",
    "    #print(face_id)\n",
    "\n",
    "\n",
    "    if(random.random() > 2): #never\n",
    "        image_guide = np.zeros((49, 768))\n",
    "        #print(image_guide.shape)\n",
    "    else:\n",
    "\n",
    "        image = Image.open(path).convert('RGB')\n",
    "\n",
    "        #print(image.size)\n",
    "\n",
    "        w_s = image_size / (1+2 * 0.4)\n",
    "        h_s = image_size / (1+2 * 0.4)\n",
    "\n",
    "        image = image.crop((0.2*w_s, 0.0*h_s, 1.6*w_s, 1.4*h_s))\n",
    "\n",
    "        image = image.resize((image_size,image_size))\n",
    "\n",
    "        image = image.resize((begin_with_image_size,begin_with_image_size))\n",
    "        im = image\n",
    "        \n",
    "\n",
    "        #print('saving')\n",
    "        #image.save('opop.png')\n",
    "\n",
    "        #print(np.array(image,np.float32).shape)\n",
    "\n",
    "        pix = np.array(image, np.float32)\n",
    "        pix = np.moveaxis(pix, -1, 0)\n",
    "\n",
    "        pix = pix / 255\n",
    "        image.close()\n",
    "        im.close()\n",
    "        return pix\n",
    "    return image_guide\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def process_age(age):\n",
    "    if(ignore_age or ignore_additional_attributes):\n",
    "        x = np.zeros(768)\n",
    "        x[767] = 1\n",
    "        return x\n",
    "    else:\n",
    "        x = np.ones(768) * (age / 100.0)\n",
    "        x[767] = 0\n",
    "        return x\n",
    "\n",
    "def process_gender(gender):\n",
    "    if(ignore_gender or ignore_additional_attributes):\n",
    "        return np.zeros(768)\n",
    "    elif(gender == 'man'):\n",
    "        return np.ones(768)\n",
    "    elif(gender == \"woman\"):\n",
    "        return np.ones(768) * -1\n",
    "    else:\n",
    "        return np.zeros(768)\n",
    "    \n",
    "# TODO\n",
    "def process_ethnicity(eth):\n",
    "    x = np.zeros(768)\n",
    "    if(ignore_ethnicity or ignore_additional_attributes):\n",
    "        x = x\n",
    "    elif(eth == \"indian\"):\n",
    "        x[0] = 1\n",
    "    elif(eth == \"asian\"):\n",
    "        x[16]=1\n",
    "    elif(eth == \"latino hispanic\"):\n",
    "        x[2]=1\n",
    "    elif(eth == \"black\"):\n",
    "        x[3]=1\n",
    "    elif(eth == \"middle eastern\"):\n",
    "        x[4]=1\n",
    "    elif(eth == \"white\"):\n",
    "        x[5]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x     \n",
    "\n",
    "def process_language(lan):\n",
    "    x = np.zeros(768)\n",
    "    if(ignore_language_spoken or ignore_additional_attributes):\n",
    "        x = x\n",
    "    elif(lan == \"Arabic\"):\n",
    "        x[0] = 1\n",
    "    elif(lan == \"Portuguese\"):\n",
    "        x[16]=1\n",
    "    elif(lan == \"Romansh_Sursilvan\"):\n",
    "        x[2]=1\n",
    "    elif(lan == \"Japanese\"):\n",
    "        x[3]=1\n",
    "    elif(lan == \"Ukranian\"):\n",
    "        x[4]=1\n",
    "    elif(lan == \"German\"):\n",
    "        x[5]=1   \n",
    "    elif(lan == \"Chinese_China\"):\n",
    "        x[6]=1   \n",
    "    elif(lan == \"Welsh\"):\n",
    "        x[7]=1  \n",
    "    elif(lan == \"English\"):\n",
    "        x[8]=1\n",
    "    elif(lan == \"Kabyle\"):\n",
    "        x[9]=1 \n",
    "    elif(lan == \"Kyrgyz\"):\n",
    "        x[10]=1\n",
    "    elif(lan == \"Georgian\"):\n",
    "        x[11]=1\n",
    "    elif(lan == \"Persian\"):\n",
    "        x[12]=1 \n",
    "    elif(lan == \"French\"):\n",
    "        x[13]=1\n",
    "    elif(lan == \"Interlingua\"):\n",
    "        x[14]=1\n",
    "    elif(lan == \"Swedish\"):\n",
    "        x[15]=1\n",
    "    elif(lan == \"Spanish\"):\n",
    "        x[16]=1 \n",
    "    elif(lan == \"Dhivehi\"):\n",
    "        x[17]=1\n",
    "    elif(lan == \"Kinyarwanda\"):\n",
    "        x[18]=1 \n",
    "    elif(lan == \"Tatar\"):\n",
    "        x[19]=1\n",
    "    elif(lan == \"Hakha_Chin\"):\n",
    "        x[20]=1 \n",
    "    elif(lan == \"Tamil\"):\n",
    "        x[21]=1 \n",
    "    elif(lan == \"Greek\"):\n",
    "        x[22]=1\n",
    "    elif(lan == \"Latvian\"):\n",
    "        x[23]=1 \n",
    "    elif(lan == \"Russian\"):\n",
    "        x[24]=1\n",
    "    elif(lan == \"Breton\"):\n",
    "        x[25]=1\n",
    "    elif(lan == \"Catalan\"):\n",
    "        x[26]=1    \n",
    "    elif(lan == \"Maltese\"):\n",
    "        x[27]=1 \n",
    "    elif(lan == \"Slovenian\"):\n",
    "        x[28]=1    \n",
    "    elif(lan == \"Indonesian\"):\n",
    "        x[29]=1    \n",
    "    elif(lan == \"Dutch\"):\n",
    "        x[30]=1\n",
    "    elif(lan == \"Chinese_Taiwan\"):\n",
    "        x[31]=1 \n",
    "    elif(lan == \"Sakha\"):\n",
    "        x[32]=1 \n",
    "    elif(lan == \"Polish\"):\n",
    "        x[33]=1 \n",
    "    elif(lan == \"Czech\"):\n",
    "        x[34]=1 \n",
    "    elif(lan == \"Romanian\"):\n",
    "        x[35]=1 \n",
    "    elif(lan == \"Mangolian\"):\n",
    "        x[36]=1 \n",
    "    elif(lan == \"Italian\"):\n",
    "        x[37]=1 \n",
    "    elif(lan == \"Chinese_Hongkong\"):\n",
    "        x[38]=1 \n",
    "    elif(lan == \"Estonian\"):\n",
    "        x[39]=1 \n",
    "    elif(lan == \"Basque\"):\n",
    "        x[40]=1 \n",
    "    elif(lan == \"Esperanto\"):\n",
    "        x[41]=1 \n",
    "    elif(lan == \"Frisian\"):\n",
    "        x[42]=1 \n",
    "    elif(lan == \"Turkish\"):\n",
    "        x[43]=1 \n",
    "    elif(lan == \"Chuvash\"):\n",
    "        x[44]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_input(row):\n",
    "    #print(row)\n",
    "    age = row[\"caption_a\"]\n",
    "    ethnicity=row[\"caption_e\"]\n",
    "    gender=row[\"caption_g\"]\n",
    "    language=row[\"caption_l\"]\n",
    "    speaker_emb=row[\"SPEAKER_EMB\"]\n",
    "    audio_emb=row[\"AUDIO_EMB\"]\n",
    "    audio_features=row[\"AUDIO_FEATURES\"]\n",
    "    image_guide=row[\"image_guide\"]\n",
    "    #print(image_guide)\n",
    "    #print(speaker_emb)\n",
    "    speaker_emb = np.asarray(speaker_emb, dtype=np.float32)\n",
    "    speaker_emb = speaker_emb.squeeze()\n",
    "    audio_emb = np.asarray(audio_emb, dtype=np.float32)\n",
    "    audio_emb = audio_emb.squeeze()\n",
    "    #print(speaker_emb.shape)\n",
    "    h = np.vstack((age, ethnicity))\n",
    "    h = np.vstack((h, gender))\n",
    "    h = np.vstack((h, language))\n",
    "    h = np.vstack((h, speaker_emb))\n",
    "    h = np.vstack((h, audio_emb))\n",
    "    h = np.vstack((h,audio_features))\n",
    "    h = np.vstack((h,image_guide))\n",
    "    #print('aaaaaaaaaaa')\n",
    "    #print(h.shape)\n",
    "    j = np.zeros(768)\n",
    "    j = np.tile(j,(256-h.shape[0],1))\n",
    "    h = np.vstack((h, j))\n",
    "    #print(h.shape)\n",
    "    \n",
    "    return h\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_image_path(path):\n",
    "    \n",
    "    image = Image.open(path).convert('RGB')\n",
    "\n",
    "    w_s = image_size / (1+2 * 0.4)\n",
    "    h_s = image_size / (1+2 * 0.4)\n",
    "\n",
    "    #print(image.size)\n",
    "    image = image.crop((0.2*w_s,0.0*h_s,1.6*w_s,1.4*h_s))\n",
    "    image = image.resize((image_size,image_size))\n",
    "\n",
    "    #print('saving')\n",
    "    #image.save(str(random.random()) + '.png')\n",
    "\n",
    "    \n",
    "\n",
    "    #print(np.array(image,np.float32).shape)\n",
    "    pix = np.array(image,np.float32)\n",
    "    pix = np.moveaxis(pix, -1, 0)\n",
    "    \n",
    "    pix = pix / 255\n",
    "    image.close()\n",
    "    return pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pathlib\n",
    "import lancedb\n",
    "uri = datasetPathDatabase\n",
    "db = lancedb.connect(uri)\n",
    "\n",
    "try:\n",
    "    db.drop_table(generated_face_table_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def save_generated_image(row):\n",
    "    id = row['videos_id']\n",
    "    row_videos = videos_df.loc[videos_df['id'] == id]\n",
    "    generated_image = row['generated']\n",
    "    \n",
    "    absPathVideo = row_videos['video_path'].iloc[0]\n",
    "\n",
    "\n",
    "    absPathGeneratedFace = absPathVideo.replace(datasetPathVideo,datasetPathGeneratedFaces)\n",
    "    absPathGeneratedFace = os.path.splitext(absPathGeneratedFace)[0]\n",
    "    absPathGeneratedFace = absPathGeneratedFace + \"_generated_face_\"+ \".png\"\n",
    "    pathlib.Path(os.path.dirname(absPathGeneratedFace)).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    #print(row)\n",
    "    #try:\n",
    "    #    print(generated_image)\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "    #try:\n",
    "    #    print(generated_image.shape)\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "    #print(generated_image)\n",
    "    generated_image.save(absPathGeneratedFace)\n",
    "\n",
    "    p_small = pathlib.Path(os.path.dirname(absPathVideo))\n",
    "    p_big = p_small.parent.absolute()\n",
    "    df_table2 = pd.DataFrame()\n",
    "    df_table2 = df_table2.append({'id':int(id),'id_true': int(id),\n",
    "                    'generated_face_path': absPathGeneratedFace,'user':p_big.name}, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        table_stage2 = db.open_table(generated_face_table_name)\n",
    "        table_stage2.add(df_table2)\n",
    "    except:\n",
    "        db.create_table(generated_face_table_name, df_table2)\n",
    "\n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>face_path</th>\n",
       "      <th>user</th>\n",
       "      <th>blurred_face_path</th>\n",
       "      <th>features_path</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>vector</th>\n",
       "      <th>stage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int64</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from_pandas, 1 graph layer</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                  id video_path face_path    user blurred_face_path features_path      age  gender ethnicity  vector  stage\n",
       "npartitions=3                                                                                                              \n",
       "0              int64     object    object  object            object        object  float64  object    object  object  int64\n",
       "4                ...        ...       ...     ...               ...           ...      ...     ...       ...     ...    ...\n",
       "8                ...        ...       ...     ...               ...           ...      ...     ...       ...     ...    ...\n",
       "10               ...        ...       ...     ...               ...           ...      ...     ...       ...     ...    ...\n",
       "Dask Name: from_pandas, 1 graph layer"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "db = lancedb.connect(uri)\n",
    "\n",
    "table = db.open_table(\"video_stage1\")\n",
    "videos = table.to_pandas()\n",
    "videos_df = videos\n",
    "videos = dd.from_pandas(videos,chunksize=preprocessing_chunk)\n",
    "videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of preprocessing chunks: 3\n",
      "4\n",
      "Getting Data\n",
      "Data Preprocessed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180262/2406898082.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(feature)\n",
      "/tmp/ipykernel_180262/2406898082.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(feature)\n",
      "/tmp/ipykernel_180262/2406898082.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(feature)\n",
      "/tmp/ipykernel_180262/2406898082.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(feature)\n",
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-31 22:01:16.390971: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n",
      "Using model file imagen_features_low_res_asis_24s.pt\n",
      "checkpoint loaded from imagen_features_low_res_asis_24s.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 256/256 [00:17<00:00, 14.64it/s]\n",
      "sampling loop time step: 100%|██████████| 256/256 [00:22<00:00, 11.36it/s]\n",
      "2it [00:41, 20.61s/it]\n",
      "sampling loop time step: 100%|██████████| 256/256 [00:17<00:00, 14.97it/s]\n",
      "sampling loop time step: 100%|██████████| 256/256 [00:22<00:00, 11.48it/s]\n",
      "2it [00:40, 20.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images\n",
      "[<PIL.Image.Image image mode=RGB size=32x32 at 0x7FE08DCD0DC0>, <PIL.Image.Image image mode=RGB size=32x32 at 0x7FE08DCD0F70>, <PIL.Image.Image image mode=RGB size=32x32 at 0x7FE08DCD0D00>, <PIL.Image.Image image mode=RGB size=32x32 at 0x7FE08DCD0E20>]\n",
      "no of chunks:3\n",
      "1\n",
      "4\n",
      "Getting Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180262/3921310896.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_table2 = df_table2.append({'id':int(id),'id_true': int(id),\n",
      "/tmp/ipykernel_180262/3921310896.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_table2 = df_table2.append({'id':int(id),'id_true': int(id),\n",
      "/tmp/ipykernel_180262/3921310896.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_table2 = df_table2.append({'id':int(id),'id_true': int(id),\n",
      "/tmp/ipykernel_180262/3921310896.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_table2 = df_table2.append({'id':int(id),'id_true': int(id),\n",
      "/tmp/ipykernel_180262/2406898082.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(feature)\n",
      "/tmp/ipykernel_180262/2406898082.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(feature)\n",
      "/tmp/ipykernel_180262/2406898082.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(feature)\n",
      "/tmp/ipykernel_180262/2406898082.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(feature)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-31 22:02:59.260519: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n",
      "Using model file imagen_features_low_res_asis_24s.pt\n",
      "checkpoint loaded from imagen_features_low_res_asis_24s.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 256/256 [00:18<00:00, 13.81it/s]\n",
      "sampling loop time step: 100%|██████████| 256/256 [00:22<00:00, 11.30it/s]\n",
      "2it [00:42, 21.22s/it]\n",
      "sampling loop time step: 100%|██████████| 256/256 [00:17<00:00, 14.72it/s]\n",
      "sampling loop time step: 100%|██████████| 256/256 [00:22<00:00, 11.45it/s]\n",
      "2it [00:40, 20.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images\n",
      "[<PIL.Image.Image image mode=RGB size=32x32 at 0x7FE08DCD0D60>, <PIL.Image.Image image mode=RGB size=32x32 at 0x7FE08F48A1C0>, <PIL.Image.Image image mode=RGB size=32x32 at 0x7FE08EDB6040>, <PIL.Image.Image image mode=RGB size=32x32 at 0x7FE08EDB6250>]\n",
      "no of chunks:3\n",
      "2\n",
      "3\n",
      "Getting Data\n",
      "Data Preprocessed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180262/3921310896.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_table2 = df_table2.append({'id':int(id),'id_true': int(id),\n",
      "/tmp/ipykernel_180262/3921310896.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_table2 = df_table2.append({'id':int(id),'id_true': int(id),\n",
      "/tmp/ipykernel_180262/3921310896.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_table2 = df_table2.append({'id':int(id),'id_true': int(id),\n",
      "/tmp/ipykernel_180262/3921310896.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_table2 = df_table2.append({'id':int(id),'id_true': int(id),\n",
      "/tmp/ipykernel_180262/2406898082.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(feature)\n",
      "/tmp/ipykernel_180262/2406898082.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(feature)\n",
      "/tmp/ipykernel_180262/2406898082.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features = features.append(feature)\n",
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-31 22:04:45.938879: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n",
      "Using model file imagen_features_low_res_asis_24s.pt\n",
      "checkpoint loaded from imagen_features_low_res_asis_24s.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 256/256 [00:17<00:00, 14.27it/s]\n",
      "sampling loop time step: 100%|██████████| 256/256 [00:22<00:00, 11.31it/s]\n",
      "2it [00:41, 20.91s/it]\n",
      "sampling loop time step: 100%|██████████| 256/256 [00:17<00:00, 14.36it/s]\n",
      "sampling loop time step: 100%|██████████| 256/256 [00:20<00:00, 12.53it/s]\n",
      "2it [00:39, 19.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images\n",
      "[<PIL.Image.Image image mode=RGB size=32x32 at 0x7FE08DCDCD90>, <PIL.Image.Image image mode=RGB size=32x32 at 0x7FE08DCDCE50>, <PIL.Image.Image image mode=RGB size=32x32 at 0x7FE08DCDC9A0>]\n",
      "no of chunks:3\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180262/3921310896.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_table2 = df_table2.append({'id':int(id),'id_true': int(id),\n",
      "/tmp/ipykernel_180262/3921310896.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_table2 = df_table2.append({'id':int(id),'id_true': int(id),\n",
      "/tmp/ipykernel_180262/3921310896.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_table2 = df_table2.append({'id':int(id),'id_true': int(id),\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import train_imagen_batch\n",
    "import dask.dataframe as dd\n",
    "import math\n",
    "\n",
    "j = 0\n",
    "\n",
    "no_of_video_chunks = math.ceil(len(videos)/preprocessing_chunk)\n",
    "print(\"No. of preprocessing chunks: \" + str(no_of_video_chunks))\n",
    "\n",
    "while(True):\n",
    "\n",
    "    videos_chunk = videos.partitions[j:j+1]\n",
    "    print(len(videos_chunk))\n",
    "\n",
    "    features = pd.DataFrame()\n",
    "    print(\"Getting Data\")\n",
    "    for index, row in videos_chunk.iterrows():\n",
    "\n",
    "        with open(row['features_path'], 'rb') as handle:\n",
    "            feature = pickle.load(handle)\n",
    "            feature['videos_id'] = row['id']\n",
    "            #print(feature)\n",
    "            features = features.append(feature)\n",
    "\n",
    "    #print(features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #print(features)\n",
    "    data_frame = features[[\"ID\",\"FACE_ID\",\"image_path\",\"caption_a\",\"caption_e\",\"caption_g\",\"caption_l\",\"videos_id\"]]\n",
    "    data_frame['SPEAKER_EMB'] = features['SPEAKER_EMB']\n",
    "    data_frame['AUDIO_EMB'] = features['AUDIO_EMB']\n",
    "    data_frame['image_guide'] = features['image_guide']\n",
    "    data_frame['AUDIO_FEATURES'] = features['AUDIO_FEATURES']\n",
    "\n",
    "\n",
    "    data_frame['SPEAKER_EMB'] = data_frame['SPEAKER_EMB'].apply(lambda x: speaker_emb_preprocess(x))\n",
    "    data_frame['AUDIO_EMB'] = data_frame['AUDIO_EMB'].apply(lambda x: audio_emb_preprocess2(x))\n",
    "    data_frame['AUDIO_FEATURES'] = data_frame['AUDIO_FEATURES'].apply(lambda x: audio_features_preprocess(x))\n",
    "    data_frame = data_frame.drop(['ID'], axis=1)\n",
    "    data_frame['image_guide'] = data_frame['image_path'].apply(lambda x: image_guide_preprocess_low_res_dummy(x))\n",
    "    data_frame = data_frame.drop(['FACE_ID'], axis=1)\n",
    "    data_frame['caption_a'] = data_frame['caption_a'].apply(lambda x: process_age(x))\n",
    "    data_frame['caption_g'] = data_frame['caption_g'].apply(lambda x: process_gender(x))\n",
    "    data_frame['caption_l'] = data_frame['caption_l'].apply(lambda x: process_language(x))\n",
    "    data_frame['caption_e'] = data_frame['caption_e'].apply(lambda x: process_ethnicity(x))\n",
    "    data_frame['low_res_image'] = data_frame['image_path'].apply(lambda x: image_guide_preprocess_low_res(x))\n",
    "\n",
    "    data_frame['INPUT'] = data_frame['SPEAKER_EMB']\n",
    "\n",
    "    data_frame['INPUT'] = data_frame.apply(process_input,args=(),axis=1)\n",
    "\n",
    "\n",
    "    data_frame = data_frame.drop(['caption_e', 'caption_g','caption_l','SPEAKER_EMB','AUDIO_EMB'], axis=1)\n",
    "\n",
    "\n",
    "    data_frame['image_path'] = data_frame['image_path'].apply(lambda x: process_image_path(x))\n",
    "\n",
    "    print(\"Data Preprocessed...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    part = data_frame[['INPUT','low_res_image']]\n",
    "\n",
    "\n",
    "\n",
    "    input0 = np.array(part['INPUT'])\n",
    "    input0 = np.array([np.array(xi) for xi in input0])\n",
    "    input0[np.isnan(input0)] = 0\n",
    "    input0[input0 > 10] = 10\n",
    "    input0[input0 < -10] = -10\n",
    "    #print(sys.getsizeof(input))\n",
    "\n",
    "\n",
    "    input2 = np.array(part['low_res_image'])\n",
    "    input2 = np.array([np.array(xi) for xi in input2])\n",
    "    input2.squeeze()\n",
    "    #print(sys.getsizeof(input2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    proc = multiprocessing.Process(target=train_imagen_batch.sample_batch_unet1, args=(input0,input2,model_filename,image_size,unet1_dim,unet2_dim,timesteps,begin_with_image_size,unet1_image_size,cond_scale,sampling_chunk,))\n",
    "    proc.start()\n",
    "    proc.join()\n",
    "\n",
    "    with open('evaluate_imagen2.tmp', 'rb') as handle:\n",
    "        images = pickle.load(handle)\n",
    "\n",
    "    print(\"images\")\n",
    "    print(images)\n",
    "\n",
    "\n",
    "    data_frame.reset_index(drop=True, inplace=True)\n",
    "    data_frame['generated'] = pd.DataFrame(data=images)\n",
    "    data_frame.apply(save_generated_image,args=(),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    del images\n",
    "    del features\n",
    "    del data_frame\n",
    "    del input0\n",
    "    del input2\n",
    "    gc.collect()\n",
    "\n",
    "    j = j+1\n",
    "    print(\"no of chunks:\" + str(no_of_video_chunks))\n",
    "    print(j)\n",
    "    if(j == no_of_video_chunks):\n",
    "        break\n",
    "        \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78dccc615e96ab04385280185a87a524fe0822daf5dbad2f4bf2e7d7b28366a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
