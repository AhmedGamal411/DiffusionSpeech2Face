{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n",
      "env: ROCM_PATH=/opt/rocm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 11:37:47.576798: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-21 11:37:47.616725: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-21 11:37:48.475012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-21 11:37:48.490396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-21 11:37:48.490501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "%env HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true\n",
    "%env ROCM_PATH=/opt/rocm\n",
    "import tensorflow as tf\n",
    "tf.test.is_built_with_rocm()\n",
    "# document export HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution,enable_eager_execution\n",
    "#disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got chunk of DATA from database. Training diffusion network\n"
     ]
    }
   ],
   "source": [
    "#TODO document jupyter\n",
    "import pickle\n",
    "import configparser\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "datasetPathDatabase =  configParser.get('COMMON', 'datasetPathDatabase') + '/dataset.db'\n",
    "p =  configParser.get('extractFacesLatents', 'dbChunk')\n",
    "\n",
    "# TODO Better display of progress and handling of exceptions\n",
    "contLoop = True # Flag to continue to get chunks of videos from database\n",
    "con = sl.connect(datasetPathDatabase)\n",
    "offset = 0\n",
    "while(contLoop):\n",
    "    data = con.execute('''SELECT F.LATENT_REP,A.SPEAKER_EMB FROM VIDEO V \n",
    "                        INNER JOIN FACE F ON V.ID = F.VIDEO_ID \n",
    "                       INNER JOIN AUDIO A ON V.ID = A.VIDEO_ID \n",
    "                       WHERE F.LATENT_REP IS NOT NULL AND AUDIO_PRE = 1 AND FACES_PRE = 1\n",
    "                        ''')\n",
    "    contLoop = False\n",
    "    offset = offset + int(p)\n",
    "    print(\"Got chunk of DATA from database. Training diffusion network\")\n",
    "    dataGotten = data.fetchall()\n",
    "    for latent_rep_pickled, speaker_emb_pickled in dataGotten:\n",
    "        latent_rep = pickle.loads(latent_rep_pickled)\n",
    "        speaker_emb = pickle.loads(speaker_emb_pickled)\n",
    "\n",
    "\n",
    "con.close()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/stable-diffusion-using-hugging-face-501d8dbdd8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, logging\n",
    "## disable warnings\n",
    "logging.disable(logging.WARNING)  \n",
    "## Import the CLIP artifacts \n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "## Initiating tokenizer and encoder.\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"a dog wearing hat\"]\n",
    "tok =tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\") \n",
    "print(tok.input_ids.shape)\n",
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in list(tok.input_ids[0,:7]): \n",
    "    print(f\"{token}:{tokenizer.convert_ids_to_tokens(int(token))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = text_encoder(tok.input_ids.to(\"cuda\"))[0].half()\n",
    "print(f\"Shape of embedding : {emb.shape}\")\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To import an image from a URL \n",
    "from fastdownload import FastDownload  \n",
    "## Imaging  library \n",
    "from PIL import Image \n",
    "from torchvision import transforms as tfms  \n",
    "## Basic libraries \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline  \n",
    "## Loading a VAE model \n",
    "from diffusers import AutoencoderKL \n",
    "import torch\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "def load_image(p):\n",
    "   '''     \n",
    "   Function to load images from a defined path     \n",
    "   '''    \n",
    "   return Image.open(p).convert('RGB').resize((512,512))\n",
    "def pil_to_latents(image):\n",
    "    '''     \n",
    "    Function to convert image to latents     \n",
    "    '''     \n",
    "    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0   \n",
    "    init_image = init_image.to(device=\"cuda\", dtype=torch.float16)\n",
    "    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215     \n",
    "    return init_latent_dist  \n",
    "def latents_to_pil(latents):     \n",
    "    '''     \n",
    "    Function to convert latents to images     \n",
    "    '''     \n",
    "    latents = (1 / 0.18215) * latents     \n",
    "    with torch.no_grad():         \n",
    "        image = vae.decode(latents).sample     \n",
    "    \n",
    "    image = (image / 2 + 0.5).clamp(0, 1)     \n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()      \n",
    "    images = (image * 255).round().astype(\"uint8\")     \n",
    "    pil_images = [Image.fromarray(image) for image in images]        \n",
    "    return pil_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = FastDownload().download('https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg')\n",
    "img = load_image(p)\n",
    "print(f\"Dimension of this image: {np.array(img).shape}\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_img = pil_to_latents(img)\n",
    "print(f\"Dimension of this latent representation: {latent_img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for c in range(4):\n",
    "    axs[c].imshow(latent_img[0][c].detach().cpu(), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_img = latents_to_pil(latent_img)\n",
    "decoded_img[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DConditionModel, LMSDiscreteScheduler\n",
    "## Initializing a scheduler\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "## Setting number of sampling steps\n",
    "scheduler.set_timesteps(51)\n",
    "## Initializing the U-Net model\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn_like(latent_img) # Random noise\n",
    "fig, axs = plt.subplots(2, 3, figsize=(16, 12))\n",
    "for c, sampling_step in enumerate(range(0,51,10)):\n",
    "    encoded_and_noised = scheduler.add_noise(latent_img, noise, timesteps=torch.tensor([scheduler.timesteps[sampling_step]]))\n",
    "    axs[c//3][c%3].imshow(latents_to_pil(encoded_and_noised)[0])\n",
    "    axs[c//3][c%3].set_title(f\"Step - {sampling_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_and_noised = scheduler.add_noise(latent_img, noise, timesteps=torch.tensor([scheduler.timesteps[40]])) \n",
    "latents_to_pil(encoded_and_noised)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unconditional textual prompt\n",
    "prompt = [\"\"]\n",
    "## Using clip model to get embeddings\n",
    "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad(): \n",
    "    text_embeddings = text_encoder(\n",
    "        text_input.input_ids.to(\"cuda\")\n",
    "    )[0]\n",
    "    \n",
    "## Using U-Net to predict noise    \n",
    "latent_model_input = torch.cat([encoded_and_noised.to(\"cuda\").float()]).half()\n",
    "with torch.no_grad():\n",
    "    noise_pred = unet(\n",
    "        latent_model_input,40,encoder_hidden_states=text_embeddings\n",
    "    )[\"sample\"]\n",
    "## Visualize after subtracting noise \n",
    "latents_to_pil(encoded_and_noised- noise_pred)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, logging\n",
    "## disable warnings\n",
    "logging.disable(logging.WARNING)  \n",
    "## Imaging  library\n",
    "from PIL import Image\n",
    "from torchvision import transforms as tfms\n",
    "## Basic libraries\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import shutil\n",
    "import os\n",
    "## For video display\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "## Import the CLIP artifacts \n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
    "## Initiating tokenizer and encoder.\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "## Initiating the VAE\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "## Initializing a scheduler and Setting number of sampling steps\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "scheduler.set_timesteps(50)\n",
    "## Initializing the U-Net model\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "## Helper functions\n",
    "def load_image(p):\n",
    "    '''\n",
    "    Function to load images from a defined path\n",
    "    '''\n",
    "    return Image.open(p).convert('RGB').resize((512,512))\n",
    "def pil_to_latents(image):\n",
    "    '''\n",
    "    Function to convert image to latents\n",
    "    '''\n",
    "    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n",
    "    init_image = init_image.to(device=\"cuda\", dtype=torch.float16) \n",
    "    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n",
    "    return init_latent_dist\n",
    "def latents_to_pil(latents):\n",
    "    '''\n",
    "    Function to convert latents to images\n",
    "    '''\n",
    "    latents = (1 / 0.18215) * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images\n",
    "def text_enc(prompts, maxlen=None):\n",
    "    '''\n",
    "    A function to take a texual promt and convert it into embeddings\n",
    "    '''\n",
    "    if maxlen is None: maxlen = tokenizer.model_max_length\n",
    "    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\") \n",
    "    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
