{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 22:12:18.930407: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You do not have pycocotools installed, so KerasCV pycoco metrics are not available. Please run `pip install pycocotools`.\n",
      "You do not have pyococotools installed, so the `PyCOCOCallback` API is not available.\n",
      "You do not have Waymo Open Dataset installed, so KerasCV Waymo metrics are not available.\n"
     ]
    }
   ],
   "source": [
    "from textwrap import wrap\n",
    "import os\n",
    "\n",
    "import keras_cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "from keras_cv.models.stable_diffusion.clip_tokenizer import SimpleTokenizer\n",
    "from keras_cv.models.stable_diffusion.diffusion_model import DiffusionModel\n",
    "from keras_cv.models.stable_diffusion.image_encoder import ImageEncoder\n",
    "from keras_cv.models.stable_diffusion.noise_scheduler import NoiseScheduler\n",
    "from keras_cv.models.stable_diffusion.text_encoder import TextEncoder\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 22:12:21.918672: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 22:12:21.918966: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 22:12:22.225315: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 22:12:22.225379: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 22:12:22.225416: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 22:12:22.225674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1990] Ignoring visible gpu device (device: 0, name: AMD Radeon RX 6700S, pci bus id: 0000:03:00.0) with AMDGPU version : gfx1032. The supported AMDGPU versions are gfx1030, gfx900, gfx906, gfx908, gfx90a.\n",
      "2023-03-22 22:12:22.225711: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-22 22:12:22.225721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1990] Ignoring visible gpu device (device: 1, name: , pci bus id: 0000:07:00.0) with AMDGPU version : gfx1035. The supported AMDGPU versions are gfx1030, gfx900, gfx906, gfx908, gfx90a.\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO document jupyter\n",
    "import pickle\n",
    "import configparser\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "datasetPathDatabase =  configParser.get('COMMON', 'datasetPathDatabase') + '/dataset.db'\n",
    "\n",
    "con = sl.connect(datasetPathDatabase)\n",
    "data = con.execute(\"SELECT V.ID, V.VIDEO_PATH, V.AGE, V.ETHNICITY, V.GENDER, A.SPEAKER_EMB, A.LANG, F.FACE_PATH, 'The face of a person whose speaker embedding is ' CAPTION  FROM VIDEO V INNER JOIN AUDIO A ON V.ID = A.VIDEO_ID INNER JOIN FACE F ON V.ID = F.VIDEO_ID\")\n",
    "dataGotten = data.fetchall()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.DataFrame(dataGotten,columns = ['ID','VIDEO_PATH','AGE','ETHNICITY','GENDER','SPEAKER_EMB','LANG','image_path','caption'])\n",
    "\n",
    "def speaker_emb_preprocess(speaker_emb2):\n",
    "    '''\n",
    "    A function to take a texual promt and convert it into embeddings\n",
    "    '''\n",
    "    #if maxlen is None: maxlen = tokenizer.model_max_length\n",
    "    #inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\") \n",
    "    #return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n",
    "\n",
    "    #speaker_emb2 = speaker_emb2.squeeze()\n",
    "    speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "    #print(speaker_emb2.shape)\n",
    "    speaker_emb2 = speaker_emb2.squeeze()\n",
    "    speaker_emb2 = np.pad(speaker_emb2, (288), 'constant', constant_values=(0))\n",
    "    #print(speaker_emb2.shape)\n",
    "    speaker_emb2 = np.tile(speaker_emb2, (256, 1))\n",
    "\n",
    "    speaker_emb2 = np.array(speaker_emb2).tolist()\n",
    "    \n",
    "    \n",
    "    #speaker_emb2 = torch.from_numpy(speaker_emb2).type(torch.FloatTensor)\n",
    "    \n",
    "    #return speaker_emb2.to(\"cuda\").half()\n",
    "    return speaker_emb2\n",
    "\n",
    "from PIL import Image\n",
    "def getImage(face_path):\n",
    "    im = Image.open(face_path)\n",
    "    im.load() # required for png.split()\n",
    "\n",
    "    im2 = Image.new(\"RGB\", im.size, (255, 255, 255))\n",
    "    im2.paste(im, mask=im.split()[3]) # 3 is the alpha channel\n",
    "    im3 = np.array(im2)\n",
    "    im4 = np.rollaxis(im3,2)\n",
    "    #im4 = torch.from_numpy(im4).type(torch.FloatTensor)\n",
    "    \n",
    "    #return im4.to(\"cuda\").half()\n",
    "    return im4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for index, row in df.iterrows():\n",
    "#    df.loc[index,\"caption\"] = speaker_emb_preprocess(df.loc[index,'SPEAKER_EMB'])\n",
    "#    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df[[\"image_path\",\"caption\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZ...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZ...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>480 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            image_path  \\\n",
       "0    /home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...   \n",
       "1    /home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...   \n",
       "2    /home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...   \n",
       "3    /home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...   \n",
       "4    /home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...   \n",
       "..                                                 ...   \n",
       "475  /home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZ...   \n",
       "476  /home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZ...   \n",
       "477  /home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...   \n",
       "478  /home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...   \n",
       "479  /home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...   \n",
       "\n",
       "                                              caption  \n",
       "0    The face of a person whose speaker embedding is   \n",
       "1    The face of a person whose speaker embedding is   \n",
       "2    The face of a person whose speaker embedding is   \n",
       "3    The face of a person whose speaker embedding is   \n",
       "4    The face of a person whose speaker embedding is   \n",
       "..                                                ...  \n",
       "475  The face of a person whose speaker embedding is   \n",
       "476  The face of a person whose speaker embedding is   \n",
       "477  The face of a person whose speaker embedding is   \n",
       "478  The face of a person whose speaker embedding is   \n",
       "479  The face of a person whose speaker embedding is   \n",
       "\n",
       "[480 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path  \\\n",
       "0  /home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...   \n",
       "1  /home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...   \n",
       "2  /home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...   \n",
       "3  /home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...   \n",
       "4  /home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...   \n",
       "\n",
       "                                            caption  \n",
       "0  The face of a person whose speaker embedding is   \n",
       "1  The face of a person whose speaker embedding is   \n",
       "2  The face of a person whose speaker embedding is   \n",
       "3  The face of a person whose speaker embedding is   \n",
       "4  The face of a person whose speaker embedding is   "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = df3\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The padding token and maximum prompt length are specific to the text encoder.\n",
    "# If you're using a different text encoder be sure to change them accordingly.\n",
    "PADDING_TOKEN = 49407\n",
    "MAX_PROMPT_LENGTH = 77\n",
    "\n",
    "# Load the tokenizer.\n",
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "#  Method to tokenize and pad the tokens.\n",
    "def process_text(caption):\n",
    "    tokens = tokenizer.encode(caption)\n",
    "    tokens = tokens + [PADDING_TOKEN] * (MAX_PROMPT_LENGTH - len(tokens))\n",
    "    return np.array(tokens)\n",
    "\n",
    "\n",
    "# Collate the tokenized captions into an array.\n",
    "tokenized_texts = np.empty((len(data_frame), MAX_PROMPT_LENGTH))\n",
    "\n",
    "all_captions = list(data_frame[\"caption\"].values)\n",
    "for i, caption in enumerate(all_captions):\n",
    "    tokenized_texts[i] = process_text(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49406.,   518.,  1710., ..., 49407., 49407., 49407.],\n",
       "       [49406.,   518.,  1710., ..., 49407., 49407., 49407.],\n",
       "       [49406.,   518.,  1710., ..., 49407., 49407., 49407.],\n",
       "       ...,\n",
       "       [49406.,   518.,  1710., ..., 49407., 49407., 49407.],\n",
       "       [49406.,   518.,  1710., ..., 49407., 49407., 49407.],\n",
       "       [49406.,   518.,  1710., ..., 49407., 49407., 49407.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 77)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZ...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZ...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>480 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            image_path  \\\n",
       "0    /home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...   \n",
       "1    /home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...   \n",
       "2    /home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...   \n",
       "3    /home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...   \n",
       "4    /home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...   \n",
       "..                                                 ...   \n",
       "475  /home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZ...   \n",
       "476  /home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZ...   \n",
       "477  /home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...   \n",
       "478  /home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...   \n",
       "479  /home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...   \n",
       "\n",
       "                                              caption  \n",
       "0    The face of a person whose speaker embedding is   \n",
       "1    The face of a person whose speaker embedding is   \n",
       "2    The face of a person whose speaker embedding is   \n",
       "3    The face of a person whose speaker embedding is   \n",
       "4    The face of a person whose speaker embedding is   \n",
       "..                                                ...  \n",
       "475  The face of a person whose speaker embedding is   \n",
       "476  The face of a person whose speaker embedding is   \n",
       "477  The face of a person whose speaker embedding is   \n",
       "478  The face of a person whose speaker embedding is   \n",
       "479  The face of a person whose speaker embedding is   \n",
       "\n",
       "[480 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14815/433991658.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_frame['SPEAKER_EMB'] = df['SPEAKER_EMB']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>caption</th>\n",
       "      <th>SPEAKER_EMB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZ...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZ...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>480 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            image_path  \\\n",
       "0    /home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...   \n",
       "1    /home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...   \n",
       "2    /home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...   \n",
       "3    /home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...   \n",
       "4    /home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...   \n",
       "..                                                 ...   \n",
       "475  /home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZ...   \n",
       "476  /home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZ...   \n",
       "477  /home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...   \n",
       "478  /home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...   \n",
       "479  /home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...   \n",
       "\n",
       "                                              caption  \\\n",
       "0    The face of a person whose speaker embedding is    \n",
       "1    The face of a person whose speaker embedding is    \n",
       "2    The face of a person whose speaker embedding is    \n",
       "3    The face of a person whose speaker embedding is    \n",
       "4    The face of a person whose speaker embedding is    \n",
       "..                                                ...   \n",
       "475  The face of a person whose speaker embedding is    \n",
       "476  The face of a person whose speaker embedding is    \n",
       "477  The face of a person whose speaker embedding is    \n",
       "478  The face of a person whose speaker embedding is    \n",
       "479  The face of a person whose speaker embedding is    \n",
       "\n",
       "                                           SPEAKER_EMB  \n",
       "0    b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...  \n",
       "1    b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...  \n",
       "2    b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...  \n",
       "3    b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...  \n",
       "4    b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...  \n",
       "..                                                 ...  \n",
       "475  b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...  \n",
       "476  b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...  \n",
       "477  b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...  \n",
       "478  b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...  \n",
       "479  b'\\x80\\x04\\x95\\x8f\\x03\\x00\\x00\\x00\\x00\\x00\\x00...  \n",
       "\n",
       "[480 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame['SPEAKER_EMB'] = df['SPEAKER_EMB']\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "for index, row in data_frame.iterrows():\n",
    "    x = speaker_emb_preprocess(data_frame.loc[index,\"SPEAKER_EMB\"])\n",
    "    x = [x]\n",
    "    data_frame.loc[index,\"SPEAKER_EMB\"] = x\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>caption</th>\n",
       "      <th>SPEAKER_EMB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZ...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZ...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>/home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...</td>\n",
       "      <td>The face of a person whose speaker embedding is</td>\n",
       "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>480 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            image_path  \\\n",
       "0    /home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...   \n",
       "1    /home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...   \n",
       "2    /home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe...   \n",
       "3    /home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...   \n",
       "4    /home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvb...   \n",
       "..                                                 ...   \n",
       "475  /home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZ...   \n",
       "476  /home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZ...   \n",
       "477  /home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...   \n",
       "478  /home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...   \n",
       "479  /home/gamal/Datasets/Dataset1/Faces/q1552sXvKN...   \n",
       "\n",
       "                                              caption  \\\n",
       "0    The face of a person whose speaker embedding is    \n",
       "1    The face of a person whose speaker embedding is    \n",
       "2    The face of a person whose speaker embedding is    \n",
       "3    The face of a person whose speaker embedding is    \n",
       "4    The face of a person whose speaker embedding is    \n",
       "..                                                ...   \n",
       "475  The face of a person whose speaker embedding is    \n",
       "476  The face of a person whose speaker embedding is    \n",
       "477  The face of a person whose speaker embedding is    \n",
       "478  The face of a person whose speaker embedding is    \n",
       "479  The face of a person whose speaker embedding is    \n",
       "\n",
       "                                           SPEAKER_EMB  \n",
       "0    [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  \n",
       "1    [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  \n",
       "2    [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  \n",
       "3    [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  \n",
       "4    [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  \n",
       "..                                                 ...  \n",
       "475  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  \n",
       "476  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  \n",
       "477  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  \n",
       "478  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  \n",
       "479  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  \n",
       "\n",
       "[480 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(x):\n",
    "    print(x)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = 128\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "POS_IDS = tf.convert_to_tensor([list(range(MAX_PROMPT_LENGTH))], dtype=tf.int32)\n",
    "\n",
    "augmenter = keras_cv.layers.Augmenter(\n",
    "    layers=[\n",
    "        keras_cv.layers.CenterCrop(RESOLUTION, RESOLUTION),\n",
    "        keras_cv.layers.RandomFlip(),\n",
    "        tf.keras.layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "    ]\n",
    ")\n",
    "text_encoder = TextEncoder(MAX_PROMPT_LENGTH)\n",
    "\n",
    "\n",
    "def process_image(image_path, tokenized_text,speaker_emb):\n",
    "    #y = tf.py_function(func=show, inp=[speaker_emb], Tout=tf.float32)\n",
    "\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_png(image, 3)\n",
    "    image = tf.image.resize(image, (RESOLUTION, RESOLUTION))\n",
    "    return image, tokenized_text,speaker_emb\n",
    "\n",
    "\n",
    "def apply_augmentation(image_batch, token_batch,speaker_emb):\n",
    "    return augmenter(image_batch), token_batch,speaker_emb\n",
    "\n",
    "\n",
    "def run_text_encoder(image_batch, token_batch,speaker_emb):\n",
    "    print(speaker_emb.get_static_value())\n",
    "\n",
    "    textEncoderOp = text_encoder([token_batch, POS_IDS], training=False)\n",
    "    \n",
    "    return (\n",
    "        image_batch,\n",
    "        token_batch,\n",
    "        speaker_emb,\n",
    "        textEncoderOp,\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_dict(image_batch, token_batch, speaker_emb,encoded_text_batch):\n",
    "    return {\n",
    "        \"images\": image_batch,\n",
    "        \"tokens\": token_batch,\n",
    "        \"index\":speaker_emb,\n",
    "        \"encoded_text\": encoded_text_batch,\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_dataset(image_paths, tokenized_texts,speaker_emb , batch_size=1):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, tokenized_texts, np.asarray(speaker_emb)))\n",
    "    dataset = dataset.shuffle(batch_size * 10)\n",
    "    dataset = dataset.map(process_image, num_parallel_calls=AUTO).batch(batch_size)\n",
    "    dataset = dataset.map(apply_augmentation, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.map(run_text_encoder, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.map(prepare_dict, num_parallel_calls=AUTO)\n",
    "    return dataset.prefetch(AUTO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in data_frame.iterrows():\n",
    "    print(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Prepare the dataset.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m training_dataset \u001b[39m=\u001b[39m prepare_dataset(\n\u001b[1;32m      3\u001b[0m     np\u001b[39m.\u001b[39;49marray(data_frame[\u001b[39m\"\u001b[39;49m\u001b[39mimage_path\u001b[39;49m\u001b[39m\"\u001b[39;49m]), tokenized_texts, data_frame[\u001b[39m\"\u001b[39;49m\u001b[39mSPEAKER_EMB\u001b[39;49m\u001b[39m\"\u001b[39;49m],batch_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m\n\u001b[1;32m      4\u001b[0m )\n",
      "Cell \u001b[0;32mIn[16], line 52\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[0;34m(image_paths, tokenized_texts, speaker_emb, batch_size)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_dataset\u001b[39m(image_paths, tokenized_texts,speaker_emb , batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> 52\u001b[0m     dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices((image_paths, tokenized_texts, np\u001b[39m.\u001b[39;49masarray(speaker_emb, dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat32)))\n\u001b[1;32m     53\u001b[0m     dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mshuffle(batch_size \u001b[39m*\u001b[39m \u001b[39m10\u001b[39m)\n\u001b[1;32m     54\u001b[0m     dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(process_image, num_parallel_calls\u001b[39m=\u001b[39mAUTO)\u001b[39m.\u001b[39mbatch(batch_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/pandas/core/series.py:893\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    847\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \u001b[39m    Return the values as a NumPy array.\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[39m          dtype='datetime64[ns]')\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Prepare the dataset.\n",
    "training_dataset = prepare_dataset(\n",
    "    np.array(data_frame[\"image_path\"]), tokenized_texts, data_frame[\"SPEAKER_EMB\"],batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Take a sample batch and investigate.\n",
    "sample_batch = next(iter(training_dataset))\n",
    "\n",
    "for k in sample_batch:\n",
    "    print(k, sample_batch[k].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i in range(3):\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.imshow((sample_batch[\"images\"][i] + 1) / 2)\n",
    "\n",
    "    text = tokenizer.decode(sample_batch[\"tokens\"][i].numpy().squeeze())\n",
    "    text = text.replace(\"<|startoftext|>\", \"\")\n",
    "    text = text.replace(\"<|endoftext|>\", \"\")\n",
    "    text = \"\\n\".join(wrap(text, 12))\n",
    "    plt.title(text, fontsize=15)\n",
    "\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(tf.keras.Model):\n",
    "    # Reference:\n",
    "    # https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusion_model,\n",
    "        vae,\n",
    "        noise_scheduler,\n",
    "        use_mixed_precision=False,\n",
    "        max_grad_norm=1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.diffusion_model = diffusion_model\n",
    "        self.vae = vae\n",
    "        self.noise_scheduler = noise_scheduler\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.use_mixed_precision = use_mixed_precision\n",
    "        self.vae.trainable = False\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images = inputs[\"images\"]\n",
    "        encoded_text = inputs[\"encoded_text\"]\n",
    "        batch_size = tf.shape(images)[0]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Project image into the latent space and sample from it.\n",
    "            latents = self.sample_from_encoder_outputs(self.vae(images, training=False))\n",
    "            # Know more about the magic number here:\n",
    "            # https://keras.io/examples/generative/fine_tune_via_textual_inversion/\n",
    "            latents = latents * 0.18215\n",
    "\n",
    "            # Sample noise that we'll add to the latents.\n",
    "            noise = tf.random.normal(tf.shape(latents))\n",
    "\n",
    "            # Sample a random timestep for each image.\n",
    "            timesteps = tnp.random.randint(\n",
    "                0, self.noise_scheduler.train_timesteps, (batch_size,)\n",
    "            )\n",
    "\n",
    "            # Add noise to the latents according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process).\n",
    "            noisy_latents = self.noise_scheduler.add_noise(\n",
    "                tf.cast(latents, noise.dtype), noise, timesteps\n",
    "            )\n",
    "\n",
    "            # Get the target for loss depending on the prediction type\n",
    "            # just the sampled noise for now.\n",
    "            target = noise  # noise_schedule.predict_epsilon == True\n",
    "\n",
    "            # Predict the noise residual and compute loss.\n",
    "            timestep_embedding = tf.map_fn(\n",
    "                lambda t: self.get_timestep_embedding(t), timesteps, dtype=tf.float32\n",
    "            )\n",
    "            timestep_embedding = tf.squeeze(timestep_embedding, 1)\n",
    "            model_pred = self.diffusion_model(\n",
    "                [noisy_latents, timestep_embedding, encoded_text], training=True\n",
    "            )\n",
    "            loss = self.compiled_loss(target, model_pred)\n",
    "            if self.use_mixed_precision:\n",
    "                loss = self.optimizer.get_scaled_loss(loss)\n",
    "\n",
    "        # Update parameters of the diffusion model.\n",
    "        trainable_vars = self.diffusion_model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        if self.use_mixed_precision:\n",
    "            gradients = self.optimizer.get_unscaled_gradients(gradients)\n",
    "        gradients = [tf.clip_by_norm(g, self.max_grad_norm) for g in gradients]\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def get_timestep_embedding(self, timestep, dim=320, max_period=10000):\n",
    "        half = dim // 2\n",
    "        log_max_preiod = tf.math.log(tf.cast(max_period, tf.float32))\n",
    "        freqs = tf.math.exp(\n",
    "            -log_max_preiod * tf.range(0, half, dtype=tf.float32) / half\n",
    "        )\n",
    "        args = tf.convert_to_tensor([timestep], dtype=tf.float32) * freqs\n",
    "        embedding = tf.concat([tf.math.cos(args), tf.math.sin(args)], 0)\n",
    "        embedding = tf.reshape(embedding, [1, -1])\n",
    "        return embedding\n",
    "\n",
    "    def sample_from_encoder_outputs(self, outputs):\n",
    "        mean, logvar = tf.split(outputs, 2, axis=-1)\n",
    "        logvar = tf.clip_by_value(logvar, -30.0, 20.0)\n",
    "        std = tf.exp(0.5 * logvar)\n",
    "        sample = tf.random.normal(tf.shape(mean), dtype=mean.dtype)\n",
    "        return mean + std * sample\n",
    "\n",
    "    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n",
    "        # Overriding this method will allow us to use the `ModelCheckpoint`\n",
    "        # callback directly with this trainer class. In this case, it will\n",
    "        # only checkpoint the `diffusion_model` since that's what we're training\n",
    "        # during fine-tuning.\n",
    "        self.diffusion_model.save_weights(\n",
    "            filepath=filepath,\n",
    "            overwrite=overwrite,\n",
    "            save_format=save_format,\n",
    "            options=options,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable mixed-precision training if the underlying GPU has tensor cores.\n",
    "USE_MP = False\n",
    "if USE_MP:\n",
    "    keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "image_encoder = ImageEncoder(RESOLUTION, RESOLUTION)\n",
    "diffusion_ft_trainer = Trainer(\n",
    "    diffusion_model=DiffusionModel(RESOLUTION, RESOLUTION, MAX_PROMPT_LENGTH),\n",
    "    # Remove the top layer from the encoder, which cuts off the variance and only\n",
    "    # returns the mean.\n",
    "    vae=tf.keras.Model(\n",
    "        image_encoder.input,\n",
    "        image_encoder.layers[-2].output,\n",
    "    ),\n",
    "    noise_scheduler=NoiseScheduler(),\n",
    "    use_mixed_precision=USE_MP,\n",
    ")\n",
    "\n",
    "# These hyperparameters come from this tutorial by Hugging Face:\n",
    "# https://huggingface.co/docs/diffusers/training/text2image\n",
    "lr = 1e-5\n",
    "beta_1, beta_2 = 0.9, 0.999\n",
    "weight_decay = (1e-2,)\n",
    "epsilon = 1e-08\n",
    "\n",
    "optimizer = tf.keras.optimizers.experimental.AdamW(\n",
    "    learning_rate=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    beta_1=beta_1,\n",
    "    beta_2=beta_2,\n",
    "    epsilon=epsilon,\n",
    ")\n",
    "diffusion_ft_trainer.compile(optimizer=optimizer, loss=\"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "ckpt_path = \"finetuned_stable_diffusion.h5\"\n",
    "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    ckpt_path,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "diffusion_ft_trainer.fit(training_dataset, epochs=epochs, callbacks=[ckpt_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights_path = \"finetuned_stable_diffusion.h5\"\n",
    "img_height = img_width = 128\n",
    "pokemon_model = keras_cv.models.StableDiffusion(\n",
    "    img_width=img_width, img_height=img_height\n",
    ")\n",
    "# We just reload the weights of the fine-tuned diffusion model.\n",
    "pokemon_model.diffusion_model.load_weights(weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "weights_path = tf.keras.utils.get_file(\n",
    "    origin=\"https://huggingface.co/sayakpaul/kerascv_sd_pokemon_finetuned/resolve/main/ckpt_epochs_72_res_512_mp_True.h5\"\n",
    ")\n",
    "\n",
    "img_height = img_width = 512\n",
    "pokemon_model = keras_cv.models.StableDiffusion(\n",
    "    img_width=img_width, img_height=img_height\n",
    ")\n",
    "# We just reload the weights of the fine-tuned diffusion model.\n",
    "pokemon_model.diffusion_model.load_weights(weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompts = [\"Hello Kitty\"] #\"Hello Kitty\", \"A pokemon with red eyes\"]\n",
    "#prompts = [\"Yoda\", \"Hello Kitty\", \"A pokemon with red eyes\"]\n",
    "images_to_generate = 3\n",
    "outputs = {}\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated_images = pokemon_model.text_to_image(\n",
    "        prompt, batch_size=images_to_generate, unconditional_guidance_scale=40\n",
    "    )\n",
    "    outputs.update({prompt: generated_images})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, title):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(images)):\n",
    "        ax = plt.subplot(1, len(images), i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(title, fontsize=12)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "\n",
    "for prompt in outputs:\n",
    "    plot_images(outputs[prompt], prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
