{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
      "env: ROCM_PATH=/opt/rocm\n"
     ]
    }
   ],
   "source": [
    "%env HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
    "%env ROCM_PATH=/opt/rocm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"AUTOGRAPH_VERBOSITY\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You do not have pycocotools installed, so KerasCV pycoco metrics are not available. Please run `pip install pycocotools`.\n",
      "You do not have pyococotools installed, so the `PyCOCOCallback` API is not available.\n",
      "You do not have Waymo Open Dataset installed, so KerasCV Waymo metrics are not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#TODO document jupyter\n",
    "import pickle\n",
    "import configparser\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import logging\n",
    "\n",
    "\n",
    "from textwrap import wrap\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "import keras_cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "from keras_cv.models.stable_diffusion.clip_tokenizer import SimpleTokenizer\n",
    "from keras_cv.models.stable_diffusion.diffusion_model import DiffusionModel\n",
    "from keras_cv.models.stable_diffusion.image_encoder import ImageEncoder\n",
    "from keras_cv.models.stable_diffusion.noise_scheduler import NoiseScheduler\n",
    "from keras_cv.models.stable_diffusion.text_encoder import TextEncoder\n",
    "from tensorflow import keras\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "datasetPathDatabase =  configParser.get('COMMON', 'datasetPathDatabase') + '/dataset.db'\n",
    "db_chunk = int(configParser.get('fineTuneStableDiffusionTraining', 'db_chunk'))\n",
    "dev_mode = int(configParser.get('fineTuneStableDiffusionTraining', 'dev_mode'))\n",
    "\n",
    "continue_from_epoch = int(configParser.get('fineTuneStableDiffusionTraining', 'continue_from_epoch'))\n",
    "continue_from_offset = int(configParser.get('fineTuneStableDiffusionTraining', 'continue_from_offset'))\n",
    "continue_from_epoch_and_offset = int(configParser.get('fineTuneStableDiffusionTraining', 'continue_from_epoch_and_offset'))\n",
    "\n",
    "unconditional_guidance_scale = int(configParser.get('fineTuneStableDiffusionTraining', 'unconditional_guidance_scale'))\n",
    "\n",
    "con = sl.connect(datasetPathDatabase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speaker_emb_preprocess(speaker_emb2):\n",
    "    '''\n",
    "    A function to take a texual promt and convert it into embeddings\n",
    "    '''\n",
    "    #if maxlen is None: maxlen = tokenizer.model_max_length\n",
    "    #inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\") \n",
    "    #return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n",
    "\n",
    "    #speaker_emb2 = speaker_emb2.squeeze()\n",
    "    speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "    #print(speaker_emb2.shape)\n",
    "    speaker_emb2 = speaker_emb2.squeeze()\n",
    "    speaker_emb2 = np.pad(speaker_emb2, (288), 'constant', constant_values=(0))\n",
    "    #print(speaker_emb2.shape)\n",
    "    speaker_emb2 = np.tile(speaker_emb2, (1, 1))\n",
    "\n",
    "    speaker_emb2 = np.array(speaker_emb2).tolist()\n",
    "    \n",
    "    \n",
    "    #speaker_emb2 = torch.from_numpy(speaker_emb2).type(torch.FloatTensor)\n",
    "    \n",
    "    #return speaker_emb2.to(\"cuda\").half()\n",
    "    return speaker_emb2\n",
    "\n",
    "\n",
    "def getImage(face_path):\n",
    "    im = Image.open(face_path)\n",
    "    im.load() # required for png.split()\n",
    "\n",
    "    im2 = Image.new(\"RGB\", im.size, (255, 255, 255))\n",
    "    im2.paste(im, mask=im.split()[3]) # 3 is the alpha channel\n",
    "    im3 = np.array(im2)\n",
    "    im4 = np.rollaxis(im3,2)\n",
    "    #im4 = torch.from_numpy(im4).type(torch.FloatTensor)\n",
    "    \n",
    "    #return im4.to(\"cuda\").half()\n",
    "    return im4\n",
    "\n",
    "# The padding token and maximum prompt length are specific to the text encoder.\n",
    "# If you're using a different text encoder be sure to change them accordingly.\n",
    "PADDING_TOKEN = 49407\n",
    "MAX_PROMPT_LENGTH = 77\n",
    "\n",
    "# Load the tokenizer.\n",
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "#  Method to tokenize and pad the tokens.\n",
    "def process_text(caption):\n",
    "    tokens = tokenizer.encode(caption)\n",
    "    tokens = tokens + [PADDING_TOKEN] * (MAX_PROMPT_LENGTH - len(tokens))\n",
    "    return np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = 128\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "POS_IDS = tf.convert_to_tensor([list(range(MAX_PROMPT_LENGTH))], dtype=tf.int32)\n",
    "\n",
    "augmenter = keras_cv.layers.Augmenter(\n",
    "    layers=[\n",
    "        keras_cv.layers.CenterCrop(RESOLUTION, RESOLUTION),\n",
    "        keras_cv.layers.RandomFlip(),\n",
    "        tf.keras.layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "    ]\n",
    ")\n",
    "text_encoder = TextEncoder(MAX_PROMPT_LENGTH)\n",
    "\n",
    "\n",
    "def process_image(image_path, tokenized_text,speaker_emb):\n",
    "    #y = tf.py_function(func=show, inp=[speaker_emb], Tout=tf.float32)\n",
    "\n",
    "\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_png(image, 3)\n",
    "    image = tf.image.resize(image, (RESOLUTION, RESOLUTION))\n",
    "    return image, tokenized_text,speaker_emb\n",
    "\n",
    "\n",
    "def apply_augmentation(image_batch, token_batch,speaker_emb):\n",
    "    return augmenter(image_batch), token_batch,speaker_emb\n",
    "\n",
    "\n",
    "def run_text_encoder(image_batch, token_batch,speaker_emb):\n",
    "\n",
    "    speaker_emb = tf.cast(speaker_emb, tf.float32)\n",
    "    \n",
    "    textEncoderOp = text_encoder([token_batch, POS_IDS], training=False)\n",
    "\n",
    "    #print(textEncoderOp.shape)\n",
    "    textEncoderOp = textEncoderOp[:,:-1,:]\n",
    "\n",
    "    #print(speaker_emb.shape)\n",
    "    #print(textEncoderOp.shape)\n",
    "\n",
    "    textEncoderOp = tf.concat([textEncoderOp, speaker_emb], 1)\n",
    "\n",
    "    #print(textEncoderOp.shape)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return (\n",
    "        image_batch,\n",
    "        token_batch,\n",
    "        speaker_emb,\n",
    "        textEncoderOp,\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_dict(image_batch, token_batch, speaker_emb,encoded_text_batch):\n",
    "    return {\n",
    "        \"images\": image_batch,\n",
    "        \"tokens\": token_batch,\n",
    "        \"index\":speaker_emb,\n",
    "        \"encoded_text\": encoded_text_batch,\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_dataset(image_paths, tokenized_texts,speaker_emb , batch_size=1):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, tokenized_texts, speaker_emb))\n",
    "    dataset = dataset.shuffle(batch_size * 10)\n",
    "    dataset = dataset.map(process_image, num_parallel_calls=AUTO).batch(batch_size)\n",
    "    dataset = dataset.map(apply_augmentation, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.map(run_text_encoder, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.map(prepare_dict, num_parallel_calls=AUTO)\n",
    "    return dataset.prefetch(AUTO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54319/688326817.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(tf.keras.Model):\n",
    "    # Reference:\n",
    "    # https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusion_model,\n",
    "        vae,\n",
    "        noise_scheduler,\n",
    "        use_mixed_precision=False,\n",
    "        max_grad_norm=1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.diffusion_model = diffusion_model\n",
    "        self.vae = vae\n",
    "        self.noise_scheduler = noise_scheduler\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.use_mixed_precision = use_mixed_precision\n",
    "        self.vae.trainable = False\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images = inputs[\"images\"]\n",
    "        encoded_text = inputs[\"encoded_text\"]\n",
    "        batch_size = tf.shape(images)[0]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Project image into the latent space and sample from it.\n",
    "            latents = self.sample_from_encoder_outputs(self.vae(images, training=False))\n",
    "            # Know more about the magic number here:\n",
    "            # https://keras.io/examples/generative/fine_tune_via_textual_inversion/\n",
    "            latents = latents * 0.18215\n",
    "\n",
    "            # Sample noise that we'll add to the latents.\n",
    "            noise = tf.random.normal(tf.shape(latents))\n",
    "\n",
    "            # Sample a random timestep for each image.\n",
    "            timesteps = tnp.random.randint(\n",
    "                0, self.noise_scheduler.train_timesteps, (batch_size,)\n",
    "            )\n",
    "\n",
    "            # Add noise to the latents according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process).\n",
    "            noisy_latents = self.noise_scheduler.add_noise(\n",
    "                tf.cast(latents, noise.dtype), noise, timesteps\n",
    "            )\n",
    "\n",
    "            # Get the target for loss depending on the prediction type\n",
    "            # just the sampled noise for now.\n",
    "            target = noise  # noise_schedule.predict_epsilon == True\n",
    "\n",
    "            # Predict the noise residual and compute loss.\n",
    "            timestep_embedding = tf.map_fn(\n",
    "                lambda t: self.get_timestep_embedding(t), timesteps, dtype=tf.float32\n",
    "            )\n",
    "            timestep_embedding = tf.squeeze(timestep_embedding, 1)\n",
    "            model_pred = self.diffusion_model(\n",
    "                [noisy_latents, timestep_embedding, encoded_text], training=True\n",
    "            )\n",
    "            loss = self.compiled_loss(target, model_pred)\n",
    "            if self.use_mixed_precision:\n",
    "                loss = self.optimizer.get_scaled_loss(loss)\n",
    "\n",
    "        # Update parameters of the diffusion model.\n",
    "        trainable_vars = self.diffusion_model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        if self.use_mixed_precision:\n",
    "            gradients = self.optimizer.get_unscaled_gradients(gradients)\n",
    "        gradients = [tf.clip_by_norm(g, self.max_grad_norm) for g in gradients]\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def get_timestep_embedding(self, timestep, dim=320, max_period=10000):\n",
    "        half = dim // 2\n",
    "        log_max_preiod = tf.math.log(tf.cast(max_period, tf.float32))\n",
    "        freqs = tf.math.exp(\n",
    "            -log_max_preiod * tf.range(0, half, dtype=tf.float32) / half\n",
    "        )\n",
    "        args = tf.convert_to_tensor([timestep], dtype=tf.float32) * freqs\n",
    "        embedding = tf.concat([tf.math.cos(args), tf.math.sin(args)], 0)\n",
    "        embedding = tf.reshape(embedding, [1, -1])\n",
    "        return embedding\n",
    "\n",
    "    def sample_from_encoder_outputs(self, outputs):\n",
    "        mean, logvar = tf.split(outputs, 2, axis=-1)\n",
    "        logvar = tf.clip_by_value(logvar, -30.0, 20.0)\n",
    "        std = tf.exp(0.5 * logvar)\n",
    "        sample = tf.random.normal(tf.shape(mean), dtype=mean.dtype)\n",
    "        return mean + std * sample\n",
    "\n",
    "    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n",
    "        # Overriding this method will allow us to use the `ModelCheckpoint`\n",
    "        # callback directly with this trainer class. In this case, it will\n",
    "        # only checkpoint the `diffusion_model` since that's what we're training\n",
    "        # during fine-tuning.\n",
    "        self.diffusion_model.save_weights(\n",
    "            filepath=filepath,\n",
    "            overwrite=overwrite,\n",
    "            save_format=save_format,\n",
    "            options=options,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable mixed-precision training if the underlying GPU has tensor cores.\n",
    "USE_MP = False\n",
    "if USE_MP:\n",
    "    keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "image_encoder = ImageEncoder(RESOLUTION, RESOLUTION)\n",
    "diffusion_ft_trainer = Trainer(\n",
    "    diffusion_model=DiffusionModel(RESOLUTION, RESOLUTION, MAX_PROMPT_LENGTH),\n",
    "    # Remove the top layer from the encoder, which cuts off the variance and only\n",
    "    # returns the mean.\n",
    "    vae=tf.keras.Model(\n",
    "        image_encoder.input,\n",
    "        image_encoder.layers[-2].output,\n",
    "    ),\n",
    "    noise_scheduler=NoiseScheduler(),\n",
    "    use_mixed_precision=USE_MP,\n",
    ")\n",
    "\n",
    "# These hyperparameters come from this tutorial by Hugging Face:\n",
    "# https://huggingface.co/docs/diffusers/training/text2image\n",
    "lr = 1e-5\n",
    "beta_1, beta_2 = 0.9, 0.999\n",
    "weight_decay = (1e-2,)\n",
    "epsilon = 1e-08\n",
    "\n",
    "optimizer = tf.keras.optimizers.experimental.AdamW(\n",
    "    learning_rate=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    beta_1=beta_1,\n",
    "    beta_2=beta_2,\n",
    "    epsilon=epsilon,\n",
    ")\n",
    "diffusion_ft_trainer.compile(optimizer=optimizer, loss=\"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"finetuned_stable_diffusion.h5\"\n",
    "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    ckpt_path,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"loss\",\n",
    "    mode=\"min\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,offset,return_var):\n",
    "    con = sl.connect(datasetPathDatabase)\n",
    "    data = con.execute(\"SELECT V.ID, V.VIDEO_PATH, V.AGE, \" + \n",
    "                    \"'This person is '|| V.ETHNICITY || '. ' CAPTION_E, \" +\n",
    "\t\t\t\t    \"'This person is a '|| lower(V.GENDER) || '. ' CAPTION_G, \" +\n",
    "\t\t\t\t\t\"A.SPEAKER_EMB, \"+\n",
    "\t\t\t\t\t\"'This person speaks ' || A.LANG || '. ' CAPTION_L, \"+\n",
    "\t\t\t\t\t\"F.FACE_PATH, \"+\n",
    "                    \"'The face of a person. ' CAPTION  FROM VIDEO V \"+\n",
    "                    \"INNER JOIN AUDIO A ON V.ID = A.VIDEO_ID INNER JOIN FACE F ON V.ID = F.VIDEO_ID \" +\n",
    "                    \"LIMIT \"+ str(db_chunk) +\" OFFSET \" + str(offset))\n",
    "    dataGotten = data.fetchall()\n",
    "    \n",
    "    #print('gotten ' + str(db_chunk))\n",
    "\n",
    "    if(len(dataGotten) == 0):\n",
    "        con.close()\n",
    "        return_var = 1\n",
    "        return\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(dataGotten,columns = ['ID','VIDEO_PATH','AGE','caption_e','caption_g','SPEAKER_EMB','caption_l','image_path','caption'])\n",
    "    df3 = df[[\"image_path\",\"caption\",\"caption_e\",\"caption_g\",\"caption_l\"]]\n",
    "    df3 = df3.fillna('')\n",
    "\n",
    "    df3['caption_e'] = df3['caption_e'].apply(lambda x: x if random.random() < 0.2 else '')\n",
    "    df3['caption_g'] = df3['caption_g'].apply(lambda x: x if random.random() < 0.2 else '')\n",
    "    df3['caption_l'] = df3['caption_l'].apply(lambda x: x if random.random() < 0.2 else '')\n",
    "\n",
    "    df3['caption'] = df3['caption'] + df3['caption_e'] + df3['caption_g'] + df3['caption_l']\n",
    "\n",
    "    df3 = df3[[\"image_path\",\"caption\"]]\n",
    "\n",
    "\n",
    "    data_frame = df3\n",
    "    #print(df3)\n",
    "\n",
    "    # Collate the tokenized captions into an array.\n",
    "    tokenized_texts = np.empty((len(data_frame), MAX_PROMPT_LENGTH))\n",
    "\n",
    "    all_captions = list(data_frame[\"caption\"].values)\n",
    "    for i, caption in enumerate(all_captions):\n",
    "        tokenized_texts[i] = process_text(caption)\n",
    "\n",
    "    data_frame['SPEAKER_EMB'] = df['SPEAKER_EMB']\n",
    "\n",
    "    \n",
    "\n",
    "    for index, row in data_frame.iterrows():\n",
    "        x = speaker_emb_preprocess(data_frame.loc[index,\"SPEAKER_EMB\"])\n",
    "        x = [x]\n",
    "        data_frame.loc[index,\"SPEAKER_EMB\"] = x\n",
    "    \n",
    "    data_frame_length = len(data_frame)\n",
    "    a = np.zeros(shape=(data_frame_length, 768))\n",
    "    for index,row in data_frame.iterrows():\n",
    "        a[index] = ( np.asarray(row[2], dtype=np.float32).squeeze())\n",
    "        #print(row['image_path'])\n",
    "    a.squeeze()\n",
    "    a = np.expand_dims(a, axis=1)\n",
    "\n",
    "    emb = np.expand_dims(a[0], axis=1)\n",
    "    ##encoded_text = np.concatenate((encoded_text, emb), axis=1)\n",
    "\n",
    "\n",
    "    # Prepare the dataset.\n",
    "    training_dataset = prepare_dataset(\n",
    "        np.array(data_frame[\"image_path\"]), tokenized_texts, a,batch_size=1\n",
    "    )\n",
    "\n",
    "    # Take a sample batch and investigate.\n",
    "    #sample_batch = next(iter(training_dataset))\n",
    "\n",
    "    #for k in sample_batch:\n",
    "    #    print(k, sample_batch[k].shape)\n",
    "\n",
    "    #plt.figure(figsize=(20, 10))\n",
    "\n",
    "\n",
    "    #ax = plt.subplot(1, 4, 1)\n",
    "    #plt.imshow((sample_batch[\"images\"][0] + 1) / 2)\n",
    "\n",
    "    #text = tokenizer.decode(sample_batch[\"tokens\"][0].numpy().squeeze())\n",
    "    #text = text.replace(\"<|startoftext|>\", \"\")\n",
    "    #text = text.replace(\"<|endoftext|>\", \"\")\n",
    "    #text = \"\\n\".join(wrap(text, 12))\n",
    "    #plt.title(text, fontsize=15)\n",
    "\n",
    "    #plt.axis(\"off\")\n",
    "\n",
    "    if(dev_mode == 0):\n",
    "\n",
    "        print(\"Live Mode: Training: epoch:\" + str(epoch) + \" offset:\" + str(offset))\n",
    "\n",
    "        diffusion_ft_trainer.fit(training_dataset, epochs=1, callbacks=[ckpt_callback])\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        weights_path = \"finetuned_stable_diffusion.h5\"\n",
    "        img_height = img_width = 128\n",
    "        s2f_model = keras_cv.models.StableDiffusion(\n",
    "            img_width=img_width, img_height=img_height\n",
    "        )\n",
    "        # We just reload the weights of the fine-tuned diffusion model.\n",
    "        s2f_model.diffusion_model.load_weights(weights_path)\n",
    "\n",
    "        testsFolder = 'tests'\n",
    "        if not os.path.exists(testsFolder):\n",
    "            os.makedirs(testsFolder)\n",
    "        prompt = \"The face of a person. \"\n",
    "        images_to_generate = 1\n",
    "\n",
    "        outputs = {}\n",
    "        encoded_text = s2f_model.encode_text(prompt)\n",
    "        encoded_text = encoded_text[:,:-1,:]\n",
    "\n",
    "        emb = np.expand_dims(a[0], axis=1)\n",
    "        encoded_text = np.concatenate((encoded_text, emb), axis=1)\n",
    "\n",
    "        generated_images = s2f_model.generate_image(\n",
    "            encoded_text = encoded_text, batch_size=images_to_generate, \n",
    "            unconditional_guidance_scale=unconditional_guidance_scale\n",
    "        )\n",
    "        outputs.update({prompt: generated_images})\n",
    "\n",
    "        images = outputs[prompt]\n",
    "        for prompt in outputs:\n",
    "            for i in range(len(images)):\n",
    "                image_to_be_saved = Image.fromarray(images[i])\n",
    "                image_to_be_saved.save(testsFolder + '/' + 'epoch' + str(epoch) + '-offset' + str(offset) + \n",
    "                                    '-image' + str(i) + '.jpg')\n",
    "                \n",
    "        del training_dataset\n",
    "        del s2f_model\n",
    "        tf.keras.clear_session()\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Dev Mode: Training: epoch:\" + str(epoch) + \" offset:\" + str(offset))\n",
    "    con.close()\n",
    "    return_var = 0\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Live Mode: Training: epoch:1 offset:0\n",
      "10/10 [==============================] - 100s 1s/step - loss: 0.0950\n",
      "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n",
      "50/50 [==============================] - 25s 267ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras' has no attribute 'clear_session'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 29\u001b[0m\n\u001b[1;32m     23\u001b[0m return_var \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[39m#multiprocessing.set_start_method('spawn')\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m#proc = Process(target=train, args=(epoch,offset,return_var,))\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m#proc.start()\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m#proc.join()\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m return_var \u001b[39m=\u001b[39m train(epoch,offset,return_var)\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m(return_var \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     31\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 137\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, offset, return_var)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[39mdel\u001b[39;00m training_dataset\n\u001b[1;32m    136\u001b[0m     \u001b[39mdel\u001b[39;00m s2f_model\n\u001b[0;32m--> 137\u001b[0m     tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mclear_session()\n\u001b[1;32m    138\u001b[0m     tf\u001b[39m.\u001b[39mreset_default_graph()\n\u001b[1;32m    141\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras' has no attribute 'clear_session'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "con.close()\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "range_of_epochs = range(1,epochs + 1)\n",
    "\n",
    "if(continue_from_epoch_and_offset != 0):\n",
    "    \n",
    "    range_of_epochs = range(continue_from_epoch, epochs + 1)\n",
    "\n",
    "offset_zero_flag = 0\n",
    "\n",
    "for epoch in range_of_epochs:\n",
    "\n",
    "    offset = 0\n",
    "\n",
    "    if(continue_from_epoch_and_offset != 0 and offset_zero_flag == 0):\n",
    "        offset = continue_from_offset\n",
    "        offset_zero_flag = 1\n",
    "\n",
    "    while(True):\n",
    "        return_var = 0\n",
    "        #multiprocessing.set_start_method('spawn')\n",
    "        #proc = Process(target=train, args=(epoch,offset,return_var,))\n",
    "        #proc.start()\n",
    "        #proc.join()\n",
    "\n",
    "        return_var = train(epoch,offset,return_var)\n",
    "        if(return_var == 1):\n",
    "            break\n",
    "\n",
    "        offset = offset + db_chunk\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
