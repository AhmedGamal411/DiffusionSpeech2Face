{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNET = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "# Loading configurations\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "\n",
    "insert_amd_env_vars =  int(configParser.get('COMMON', 'insert_amd_env_vars'))\n",
    "HSA_OVERRIDE_GFX_VERSION =  configParser.get('COMMON', 'HSA_OVERRIDE_GFX_VERSION')\n",
    "ROCM_PATH =  configParser.get('COMMON', 'ROCM_PATH')\n",
    "\n",
    "if(insert_amd_env_vars != 0):\n",
    "    os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = HSA_OVERRIDE_GFX_VERSION\n",
    "    os.environ[\"ROCM_PATH\"] = ROCM_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#TODO document jupyter\n",
    "import pickle\n",
    "import configparser\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "datasetPathDatabase =  configParser.get('COMMON', 'datasetPathDatabase') + '/dataset.db'\n",
    "datasetPathDatabaseAdditional =  configParser.get('COMMON', 'datasetPathDatabase') + '/datasetAdditional.db'\n",
    "datasetPathDatabaseVgg =  configParser.get('COMMON', 'datasetPathDatabase') + '/datasetFacesBlurred.db'\n",
    "image_size =  int(configParser.get('COMMON', 'resizeImageTo'))\n",
    "begin_with_image_size = int(configParser.get('COMMON', 'begin_with_image_size'))\n",
    "unet1_dim =  int(configParser.get('COMMON', 'unet1_dim'))\n",
    "unet2_dim =  int(configParser.get('COMMON', 'unet2_dim'))\n",
    "unet1_image_size =  int(configParser.get('COMMON', 'unet1_image_size'))\n",
    "audio_embs =  str(configParser.get('COMMON', 'audio_embs'))\n",
    "audio_length_used =  configParser.get('train_imagen', 'audio_length_used') \n",
    "model_filename =  configParser.get('train_imagen', 'model_filename')\n",
    "sub_epochs=  int(configParser.get('train_imagen', 'sub_epochs') )\n",
    "inner_epochs=  int(configParser.get('train_imagen', 'inner_epochs') )\n",
    "batch_size=  int(configParser.get('train_imagen', 'batch_size') )\n",
    "timesteps= int(configParser.get('COMMON', 'timesteps') )\n",
    "sample_every=  int(configParser.get('train_imagen', 'sample_every') ) - 1\n",
    "sample_probability = int(configParser.get('train_imagen', 'sample_probability') )\n",
    "save_model_every=  int(configParser.get('train_imagen', 'save_model_every') )\n",
    "sample_every_offset=  int(configParser.get('train_imagen', 'sample_every_offset') ) - 1\n",
    "save_every_offset=  int(configParser.get('train_imagen', 'save_every_offset') ) - 1\n",
    "imagen_samples_folder = configParser.get('train_imagen', 'imagen_samples_folder') \n",
    "db_chunk = int(configParser.get('train_imagen', 'db_chunk'))\n",
    "dask_chunk = int(configParser.get('train_imagen', 'dask_chunk'))\n",
    "stop_at_no_of_samples = int(configParser.get('train_imagen', 'stop_at_no_of_samples'))\n",
    "epochs = int(configParser.get('train_imagen', 'epochs'))\n",
    "dataloader_cache_size = int(configParser.get('train_imagen', 'dataloader_cache_size'))\n",
    "ignore_speaker_embedding = bool(int(configParser.get('train_imagen', 'ignore_speaker_embedding') ))\n",
    "ignore_speech_brain = bool(int(configParser.get('train_imagen', 'ignore_speech_brain') ))\n",
    "ignore_pyannote_titanet_speakernet = bool(int(configParser.get('train_imagen', 'ignore_pyannote_titanet_speakernet') ))\n",
    "ignore_audio_features = bool(int(configParser.get('train_imagen', 'ignore_audio_features') ))\n",
    "ignore_pyAudioAnalysis = bool(int(configParser.get('train_imagen', 'ignore_pyAudioAnalysis') ))\n",
    "ignore_librosa = bool(int(configParser.get('train_imagen', 'ignore_librosa') ))\n",
    "ignore_image_guide = bool(int(configParser.get('train_imagen', 'ignore_image_guide') ))\n",
    "ignore_additional_attributes = bool(int(configParser.get('train_imagen', 'ignore_additional_attributes') ))\n",
    "ignore_age = bool(int(configParser.get('train_imagen', 'ignore_age') ))\n",
    "ignore_gender = bool(int(configParser.get('train_imagen', 'ignore_gender') ))\n",
    "ignore_ethnicity = bool(int(configParser.get('train_imagen', 'ignore_ethnicity') ))\n",
    "ignore_language_spoken = bool(int(configParser.get('train_imagen', 'ignore_language_spoken') ))\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speaker_emb_preprocess(speaker_emb2):\n",
    "\n",
    "    if(ignore_speaker_embedding or ignore_speech_brain):\n",
    "        #print('ignore speech brain')\n",
    "        return np.zeros((1,768))\n",
    "    else:\n",
    "        speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "        #print(speaker_emb2.print)\n",
    "        speaker_emb2 = speaker_emb2.squeeze()\n",
    "        speaker_emb2 = np.pad(speaker_emb2, (288), 'constant', constant_values=(0))\n",
    "        speaker_emb2 = np.tile(speaker_emb2, (1, 1))\n",
    "        speaker_emb2 = speaker_emb2 / 200.0\n",
    "        #print(speaker_emb2.shape)\n",
    "        #speaker_emb2 = np.array(speaker_emb2).tolist()\n",
    "        #print(type(speaker_emb2))\n",
    "        return speaker_emb2\n",
    "\n",
    "\n",
    "def audio_emb_preprocess2(speaker_emb2):\n",
    "    \n",
    "    if(ignore_pyannote_titanet_speakernet or ignore_speaker_embedding):\n",
    "        #print('ignore speech brain')\n",
    "        return np.zeros((28,768))\n",
    "    else:\n",
    "        #print(speaker_emb2)\n",
    "        speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "        a = speaker_emb2 #np.zeros(shape=(24, 512))\n",
    "        a = a[-28:,:] # Actually only last 39 are the relevant ones\n",
    "        #print('au')\n",
    "        #print(a.shape)\n",
    "        b = np.zeros(shape=(a.shape[0], 768-a.shape[1]))\n",
    "        #c = np.zeros(shape=(28-a.shape[0], 768))\n",
    "        arr = np.concatenate((a, b), axis=1)\n",
    "        #arr = np.concatenate((arr, c), axis=0)\n",
    "        arr = arr / (1 if(audio_embs == 'wav2vec') else 10 if(audio_embs == 'openl3')  else 1 if(audio_embs == 'pyannoteTitaNet') else 1)\n",
    "        #print(str(arr.max()))\n",
    "        #print(arr.shape)\n",
    "        speaker_emb2 = np.array(arr)\n",
    "        return speaker_emb2\n",
    "\n",
    "def audio_features_preprocess(video_id):\n",
    "    #  79 belong to pyaudioanalysis\n",
    "    # 111 belogn to liborsa\n",
    "    if(ignore_audio_features):\n",
    "        #print('ignore speech brain')\n",
    "        return np.zeros((190,768))\n",
    "    else:\n",
    "        #print(video_id)\n",
    "        conAdditional = sl.connect(datasetPathDatabaseAdditional)\n",
    "        cur = conAdditional.cursor()\n",
    "        sql = '''SELECT AUDIO_FEATURES FROM AUDIO WHERE VIDEO_ID = ?'''\n",
    "        cur.execute(sql, [video_id])\n",
    "        audio_features = cur.fetchall()\n",
    "        #print(audio_features[0])\n",
    "        audio_features = pickle.loads(audio_features[0][0])\n",
    "        #print(audio_features.shape) # 190 x 128\n",
    "        audio_features = audio_features[0:190]\n",
    "        \n",
    "        #import sys\n",
    "        #np.set_printoptions(threshold=sys.maxsize)\n",
    "        #print(np.argwhere(audio_features == 0))\n",
    "\n",
    "        if(ignore_pyAudioAnalysis and not ignore_librosa):\n",
    "            zpa = np.zeros((190-110,128))\n",
    "            audio_features = audio_features[80:190]\n",
    "            audio_features = np.vstack((audio_features,zpa))\n",
    "            #print('ignore pyAudioAnalysis')\n",
    "            #print(audio_features.shape)\n",
    "        elif(ignore_librosa and not ignore_pyAudioAnalysis):\n",
    "            zpa = np.zeros((190-80,128))\n",
    "            audio_features = audio_features[0:80]\n",
    "            audio_features = np.vstack((audio_features,zpa))\n",
    "            #print('ignore librosa')\n",
    "            #print(audio_features.shape)\n",
    "        else:\n",
    "            audio_features = np.zeros((190,128))\n",
    "            #print('ignore pyaudioanalysis and librosa')\n",
    "\n",
    "\n",
    "        z1 = np.zeros((190,768-128))\n",
    "        audio_features = np.hstack((audio_features,z1))\n",
    "        audio_features = audio_features / 100.0\n",
    "        #print(audio_features.shape)\n",
    "    return audio_features\n",
    "\n",
    "def audio_transformer_features_preprocess(video_id):\n",
    "    #print(video_id)\n",
    "    conAdditional = sl.connect(datasetPathDatabaseAdditional)\n",
    "    cur = conAdditional.cursor()\n",
    "    sql = '''SELECT AUDIO_FEATURES FROM AUDIO_TRANSFORMER WHERE VIDEO_ID = ?'''\n",
    "    cur.execute(sql, [video_id])\n",
    "    audio_features = cur.fetchall()\n",
    "    #print(audio_features[0])\n",
    "    audio_features = pickle.loads(audio_features[0][0])\n",
    "    #print(audio_features.shape) # 514 x 768\n",
    "    audio_features = audio_features.squeeze()\n",
    "    audio_features = audio_features[0::3]#172*768\n",
    "    #z1 = np.zeros((161,768-128))\n",
    "    #audio_features = np.hstack((audio_features,z1))\n",
    "    #audio_features = audio_features / 100.0\n",
    "    return audio_features\n",
    "\n",
    "import random\n",
    "def image_guide_preprocess(face_id):\n",
    "    #print(face_id)\n",
    "\n",
    "    if(random.random() > 2):\n",
    "        image_guide = np.zeros((49,768))\n",
    "        #print(image_guide.shape)\n",
    "    else:\n",
    "        conAdditional = sl.connect(datasetPathDatabaseVgg)\n",
    "        cur = conAdditional.cursor()\n",
    "        sql = '''SELECT BLURRED_FACE_EMB FROM FACES_BLURRED WHERE FACE_ID = ?'''\n",
    "        cur.execute(sql, [face_id])\n",
    "        image_guide = cur.fetchall()\n",
    "        #print(image_guide)\n",
    "        image_guide = pickle.loads(image_guide[0][0])\n",
    "        image_guide = image_guide.squeeze() #197 x 768\n",
    "        image_guide = image_guide[1::4] #49 x 768\n",
    "    return image_guide\n",
    "\n",
    "\n",
    "#boxBlurMin =  int(configParser.get('extractVggBlurred', 'boxBlurMin'))\n",
    "#boxBlurMax =  int(configParser.get('extractVggBlurred', 'boxBlurMax'))\n",
    "\n",
    "#gaussianBlurMin =  int(configParser.get('extractVggBlurred', 'gaussianBlurMin'))\n",
    "#gaussianBlurMax =  int(configParser.get('extractVggBlurred', 'gaussianBlurMax'))\n",
    "\n",
    "\n",
    "from PIL import Image,ImageFilter\n",
    "import random\n",
    "#import cv2\n",
    "import numpy as np\n",
    "#from matplotlib import pyplot as plt\n",
    "  \n",
    "\n",
    "\n",
    "def image_guide_preprocess_low_res_dummy(path):\n",
    "    #print(face_id)\n",
    "\n",
    "    image_guide = np.zeros((1, 768))\n",
    "    return image_guide\n",
    "\n",
    "def image_guide_preprocess_low_res(path):\n",
    "    #print(face_id)\n",
    "\n",
    "\n",
    "    if(ignore_image_guide):\n",
    "        return np.nan\n",
    "    else:\n",
    "\n",
    "        image = Image.open(path).convert('RGB')\n",
    "\n",
    "        #print(image.size)\n",
    "\n",
    "        w_s = image_size / (1+2 * 0.4)\n",
    "        h_s = image_size / (1+2 * 0.4)\n",
    "\n",
    "        image = image.crop((0.2*w_s, 0.0*h_s, 1.6*w_s, 1.4*h_s))\n",
    "\n",
    "        image = image.resize((image_size,image_size))\n",
    "\n",
    "        image = image.resize((begin_with_image_size,begin_with_image_size))\n",
    "        im = image\n",
    "        \n",
    "\n",
    "        #print('saving')\n",
    "        #image.save('opop.png')\n",
    "\n",
    "        #print(np.array(image,np.float32).shape)\n",
    "\n",
    "        pix = np.array(image, np.float32)\n",
    "        pix = np.moveaxis(pix, -1, 0)\n",
    "\n",
    "        pix = pix / 255\n",
    "        image.close()\n",
    "        im.close()\n",
    "        return pix\n",
    "    return image_guide\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def process_age(age):\n",
    "    if(random.random() < 0.2 or ignore_age or ignore_additional_attributes):\n",
    "        x = np.zeros(768)\n",
    "        x[767] = 1\n",
    "        return x\n",
    "    else:\n",
    "        try:\n",
    "            x = np.ones(768) * (age / 100.0)\n",
    "            x[767] = 0\n",
    "        except:\n",
    "            x = np.zeros(768)\n",
    "            x[767] = 1\n",
    "\n",
    "        return x\n",
    "\n",
    "def process_gender(gender):\n",
    "    if(random.random() < 0.2 or ignore_gender or ignore_additional_attributes):\n",
    "        return np.zeros(768)\n",
    "    elif(gender == 'man'):\n",
    "        return np.ones(768)\n",
    "    elif(gender == \"woman\"):\n",
    "        return np.ones(768) * -1\n",
    "    else:\n",
    "        return np.zeros(768)\n",
    "    \n",
    "# TODO\n",
    "def process_ethnicity(eth):\n",
    "    x = np.zeros(768)\n",
    "    if(random.random() < 0.2 or ignore_ethnicity or ignore_additional_attributes):\n",
    "        x = x\n",
    "    elif(eth == \"indian\"):\n",
    "        x[0] = 1\n",
    "    elif(eth == \"asian\"):\n",
    "        x[16]=1\n",
    "    elif(eth == \"latino hispanic\"):\n",
    "        x[2]=1\n",
    "    elif(eth == \"black\"):\n",
    "        x[3]=1\n",
    "    elif(eth == \"middle eastern\"):\n",
    "        x[4]=1\n",
    "    elif(eth == \"white\"):\n",
    "        x[5]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x     \n",
    "\n",
    "def process_language(lan):\n",
    "    x = np.zeros(768)\n",
    "    if(random.random() < 0.2 or ignore_language_spoken or ignore_additional_attributes):\n",
    "        x = x\n",
    "    elif(lan == \"Arabic\"):\n",
    "        x[0] = 1\n",
    "    elif(lan == \"Portuguese\"):\n",
    "        x[16]=1\n",
    "    elif(lan == \"Romansh_Sursilvan\"):\n",
    "        x[2]=1\n",
    "    elif(lan == \"Japanese\"):\n",
    "        x[3]=1\n",
    "    elif(lan == \"Ukranian\"):\n",
    "        x[4]=1\n",
    "    elif(lan == \"German\"):\n",
    "        x[5]=1   \n",
    "    elif(lan == \"Chinese_China\"):\n",
    "        x[6]=1   \n",
    "    elif(lan == \"Welsh\"):\n",
    "        x[7]=1  \n",
    "    elif(lan == \"English\"):\n",
    "        x[8]=1\n",
    "    elif(lan == \"Kabyle\"):\n",
    "        x[9]=1 \n",
    "    elif(lan == \"Kyrgyz\"):\n",
    "        x[10]=1\n",
    "    elif(lan == \"Georgian\"):\n",
    "        x[11]=1\n",
    "    elif(lan == \"Persian\"):\n",
    "        x[12]=1 \n",
    "    elif(lan == \"French\"):\n",
    "        x[13]=1\n",
    "    elif(lan == \"Interlingua\"):\n",
    "        x[14]=1\n",
    "    elif(lan == \"Swedish\"):\n",
    "        x[15]=1\n",
    "    elif(lan == \"Spanish\"):\n",
    "        x[16]=1 \n",
    "    elif(lan == \"Dhivehi\"):\n",
    "        x[17]=1\n",
    "    elif(lan == \"Kinyarwanda\"):\n",
    "        x[18]=1 \n",
    "    elif(lan == \"Tatar\"):\n",
    "        x[19]=1\n",
    "    elif(lan == \"Hakha_Chin\"):\n",
    "        x[20]=1 \n",
    "    elif(lan == \"Tamil\"):\n",
    "        x[21]=1 \n",
    "    elif(lan == \"Greek\"):\n",
    "        x[22]=1\n",
    "    elif(lan == \"Latvian\"):\n",
    "        x[23]=1 \n",
    "    elif(lan == \"Russian\"):\n",
    "        x[24]=1\n",
    "    elif(lan == \"Breton\"):\n",
    "        x[25]=1\n",
    "    elif(lan == \"Catalan\"):\n",
    "        x[26]=1    \n",
    "    elif(lan == \"Maltese\"):\n",
    "        x[27]=1 \n",
    "    elif(lan == \"Slovenian\"):\n",
    "        x[28]=1    \n",
    "    elif(lan == \"Indonesian\"):\n",
    "        x[29]=1    \n",
    "    elif(lan == \"Dutch\"):\n",
    "        x[30]=1\n",
    "    elif(lan == \"Chinese_Taiwan\"):\n",
    "        x[31]=1 \n",
    "    elif(lan == \"Sakha\"):\n",
    "        x[32]=1 \n",
    "    elif(lan == \"Polish\"):\n",
    "        x[33]=1 \n",
    "    elif(lan == \"Czech\"):\n",
    "        x[34]=1 \n",
    "    elif(lan == \"Romanian\"):\n",
    "        x[35]=1 \n",
    "    elif(lan == \"Mangolian\"):\n",
    "        x[36]=1 \n",
    "    elif(lan == \"Italian\"):\n",
    "        x[37]=1 \n",
    "    elif(lan == \"Chinese_Hongkong\"):\n",
    "        x[38]=1 \n",
    "    elif(lan == \"Estonian\"):\n",
    "        x[39]=1 \n",
    "    elif(lan == \"Basque\"):\n",
    "        x[40]=1 \n",
    "    elif(lan == \"Esperanto\"):\n",
    "        x[41]=1 \n",
    "    elif(lan == \"Frisian\"):\n",
    "        x[42]=1 \n",
    "    elif(lan == \"Turkish\"):\n",
    "        x[43]=1 \n",
    "    elif(lan == \"Chuvash\"):\n",
    "        x[44]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_input(row):\n",
    "    #print(row)\n",
    "    age = row[\"caption_a\"]\n",
    "    ethnicity=row[\"caption_e\"]\n",
    "    gender=row[\"caption_g\"]\n",
    "    language=row[\"caption_l\"]\n",
    "    speaker_emb=row[\"SPEAKER_EMB\"]\n",
    "    audio_emb=row[\"AUDIO_EMB\"]\n",
    "    audio_features=row[\"AUDIO_FEATURES\"]\n",
    "    image_guide=row[\"image_guide\"]\n",
    "    #print(image_guide)\n",
    "    #print(speaker_emb)\n",
    "    speaker_emb = np.asarray(speaker_emb, dtype=np.float32)\n",
    "    speaker_emb = speaker_emb.squeeze()\n",
    "    audio_emb = np.asarray(audio_emb, dtype=np.float32)\n",
    "    audio_emb = audio_emb.squeeze()\n",
    "    #print(speaker_emb.shape)\n",
    "    h = np.vstack((age, ethnicity))\n",
    "    h = np.vstack((h, gender))\n",
    "    h = np.vstack((h, language))\n",
    "    h = np.vstack((h, speaker_emb))\n",
    "    h = np.vstack((h, audio_emb))\n",
    "    h = np.vstack((h,audio_features))\n",
    "    h = np.vstack((h,image_guide))\n",
    "    #print('aaaaaaaaaaa')\n",
    "    #print(h.shape)\n",
    "    j = np.zeros(768)\n",
    "    j = np.tile(j,(256-h.shape[0],1))\n",
    "    h = np.vstack((h, j))\n",
    "    #print(h.shape)\n",
    "    \n",
    "    return h\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_image_path(path):\n",
    "    \n",
    "    image = Image.open(path).convert('RGB')\n",
    "\n",
    "    w_s = image_size / (1+2 * 0.4)\n",
    "    h_s = image_size / (1+2 * 0.4)\n",
    "\n",
    "    #print(image.size)\n",
    "    image = image.crop((0.2*w_s,0.0*h_s,1.6*w_s,1.4*h_s))\n",
    "    image = image.resize((image_size,image_size))\n",
    "\n",
    "    #print('saving')\n",
    "    #image.save(str(random.random()) + '.png')\n",
    "\n",
    "    \n",
    "\n",
    "    #print(np.array(image,np.float32).shape)\n",
    "    pix = np.array(image,np.float32)\n",
    "    pix = np.moveaxis(pix, -1, 0)\n",
    "    \n",
    "    pix = pix / 255\n",
    "    image.close()\n",
    "    return pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor,Compose\n",
    "import dask.dataframe as dd\n",
    "from multiprocessing import Manager,RLock,Value,Queue\n",
    "import ctypes\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    total_rows = None\n",
    "    con = None\n",
    "    lock = None\n",
    "    samples_seen =  Value('i', 0,lock=False)\n",
    "    \n",
    "    def no_of_samples_seen(self):\n",
    "        return self.samples_seen.value\n",
    "\n",
    "    def __init__(self,datasetPathDatabase_a,lock,last_sample_no):\n",
    "        self.lock = lock\n",
    "        self.samples_seen.value = last_sample_no\n",
    "        self.con = sl.connect(datasetPathDatabase_a)\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        if(self.total_rows == None):\n",
    "            self.lock.acquire(block=True)\n",
    "            data = self.con.execute(\"SELECT COUNT(*) FROM VIDEO v \")\n",
    "            count = data.fetchall()\n",
    "            self.total_rows = count[0][0]\n",
    "            data.close()\n",
    "            print(\"Found \" + str(self.total_rows) + \" data points.\")\n",
    "\n",
    "            #data = self.con.execute(\"SELECT COUNT(*) FROM VIDEO v WHERE V.TRAINED = 1\")\n",
    "            #count = data.fetchall()\n",
    "            #data.close()\n",
    "            #self.samples_seen.value = count[0][0]\n",
    "            #print(\"Starting from \" + str(self.samples_seen.value) + \" data points.\")\n",
    "            self.lock.release()\n",
    "\n",
    "        return self.total_rows\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.lock.acquire(block=True)\n",
    "        start_time = time.time()\n",
    "        sql = (\"SELECT V.ID,F.ID,V.VIDEO_PATH, V.AGE CAPTION_A, \" + \n",
    "                            \"V.ETHNICITY CAPTION_E, \" +\n",
    "                            \"lower(V.GENDER) CAPTION_G, \" +\n",
    "                                \"A.SPEAKER_EMB, \"+ (\"A.WAV_TO_VEC, \" if(audio_embs == 'wav2vec') else \"A.AUDIO_EMB2, \" if(audio_embs == 'openl3')  else \"A.PYANNOTE_TITANET, \" if(audio_embs == 'pyannoteTitaNet') else ', ') +\n",
    "                            \"A.AUDIO_FEATURES, \" +\n",
    "                            \"A.LANG CAPTION_L, \"+\n",
    "                            \"F.FACE_PATH \"+\n",
    "                            \"FROM VIDEO V \"+\n",
    "                            \"INNER JOIN AUDIO A ON V.ID = A.VIDEO_ID \" +\n",
    "                            \"INNER JOIN FACE F ON F.ID = (select ID from FACE f2 where f2.video_id = v.ID ORDER By ID limit 1 ) \" + \n",
    "                            \"WHERE AUDIO_LENGTH = \" + audio_length_used + ' ' +\n",
    "                            \"AND V.ID in (select V2.ID from VIDEO v2 WHERE V2.TRAINED IS NULL AND AUDIO_PRE IN (3,4) AND FACES_PRE = 2 ORDER BY ABS(RANDOM()) LIMIT \"+ str(1) + \")\")\n",
    "        data = self.con.execute(sql)\n",
    "        dataGotten = data.fetchall()\n",
    "        df = pd.DataFrame(dataGotten,columns = ['ID','FACE_ID','VIDEO_PATH','caption_a','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','AUDIO_FEATURES','caption_l','image_path'])\n",
    "        data.close()\n",
    "        \n",
    "        #print(\"aaaa\" + str(len(df)))\n",
    "        if(len(df) == 0):\n",
    "            print(\"Trying to get items\")\n",
    "            data = self.con.execute(sql)\n",
    "            dataGotten = data.fetchall()\n",
    "            df = pd.DataFrame(dataGotten,columns = ['ID','FACE_ID','VIDEO_PATH','caption_a','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','AUDIO_FEATURES','caption_l','image_path'])\n",
    "            data.close()\n",
    "            \n",
    "            if(len(df) == 0):\n",
    "                print(\"Trying to get items\")\n",
    "                data = self.con.execute(sql)\n",
    "                dataGotten = data.fetchall()\n",
    "                df = pd.DataFrame(dataGotten,columns = ['ID','FACE_ID','VIDEO_PATH','caption_a','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','AUDIO_FEATURES','caption_l','image_path'])\n",
    "                data.close()\n",
    "\n",
    "                if(len(df) == 0):\n",
    "                    print(\"Trying to get items\")\n",
    "                    data = self.con.execute(sql)\n",
    "                    dataGotten = data.fetchall()\n",
    "                    df = pd.DataFrame(dataGotten,columns = ['ID','FACE_ID','VIDEO_PATH','caption_a','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','AUDIO_FEATURES','caption_l','image_path'])\n",
    "                    data.close()\n",
    "\n",
    "                    if(len(df) == 0):\n",
    "                        print(\"Reintializing dataset\")\n",
    "                        sql50 = \"UPDATE VIDEO SET TRAINED = NULL\"\n",
    "                        cur50 = self.con.cursor()\n",
    "                        cur50.execute(sql50)\n",
    "                        self.con.commit()\n",
    "                        cur50.close()\n",
    "\n",
    "                        data = self.con.execute(sql)\n",
    "                        dataGotten = data.fetchall()\n",
    "                        df = pd.DataFrame(dataGotten,columns = ['ID','FACE_ID','VIDEO_PATH','caption_a','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','AUDIO_FEATURES','caption_l','image_path'])\n",
    "                        data.close()\n",
    "\n",
    "        data5 = np.array(df['ID']).astype('int')\n",
    "        data6 = []\n",
    "        for x in data5:\n",
    "            data6.append(int(x))\n",
    "        #print(len(data6))\n",
    "        #print(data6)\n",
    "        sql5 = \"UPDATE VIDEO SET TRAINED = 1 WHERE ID IN ({0})\".format(', '.join('?' for _ in data6))\n",
    "        #print(sql5)\n",
    "        cur5 = self.con.cursor()\n",
    "        cur5.execute(sql5, data6)\n",
    "        self.con.commit()\n",
    "        cur5.close()\n",
    "        self.samples_seen.value +=  1\n",
    "        print (self.samples_seen.value,end='\\r')\n",
    "\n",
    "\n",
    "        self.lock.release()\n",
    "\n",
    "        \n",
    "        \n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        df = dd.from_pandas(df, npartitions=1)\n",
    "        df[\"image_guide\"] = np.nan\n",
    "\n",
    "        #print(df.head(10))\n",
    "        data_frame = df[[\"ID\",\"FACE_ID\",\"image_path\",\"caption_a\",\"caption_e\",\"caption_g\",\"caption_l\"]]\n",
    "        data_frame['SPEAKER_EMB'] = df['SPEAKER_EMB']\n",
    "        data_frame['AUDIO_EMB'] = df['AUDIO_EMB']\n",
    "        data_frame['image_guide'] = df['image_guide']\n",
    "\n",
    "        \n",
    "        data_frame['SPEAKER_EMB'] = data_frame['SPEAKER_EMB'].apply(lambda x: speaker_emb_preprocess(x),meta=('1', 'object'))\n",
    "        data_frame['AUDIO_EMB'] = data_frame['AUDIO_EMB'].apply(lambda x: audio_emb_preprocess2(x),meta=('1', 'object'))\n",
    "        data_frame['AUDIO_FEATURES'] = data_frame['ID'].apply(lambda x: audio_features_preprocess(x),meta=('1', 'object'))\n",
    "        data_frame = data_frame.drop(['ID'], axis=1)\n",
    "        data_frame['image_guide'] = data_frame['image_path'].apply(lambda x: image_guide_preprocess_low_res_dummy(x),meta=('1', 'str'))\n",
    "        data_frame = data_frame.drop(['FACE_ID'], axis=1)\n",
    "        data_frame['caption_a'] = data_frame['caption_a'].apply(lambda x: process_age(x),meta=('1', 'object'))\n",
    "        data_frame['caption_g'] = data_frame['caption_g'].apply(lambda x: process_gender(x),meta=('1', 'object'))\n",
    "        data_frame['caption_l'] = data_frame['caption_l'].apply(lambda x: process_language(x),meta=('1', 'object'))\n",
    "        data_frame['caption_e'] = data_frame['caption_e'].apply(lambda x: process_ethnicity(x),meta=('1', 'object'))\n",
    "        data_frame['low_res_image'] = data_frame['image_path'].apply(lambda x: image_guide_preprocess_low_res(x),meta=('1', 'object'))\n",
    "\n",
    "        data_frame['INPUT'] = data_frame['SPEAKER_EMB']\n",
    "        #print(data_frame)\n",
    "\n",
    "        data_frame['INPUT'] = data_frame.apply(process_input,args=(),axis=1,meta=('1', 'object'))\n",
    "\n",
    "        #for index, row in data_frame.iterrows():\n",
    "        #    x = process_input(data_frame.loc[index,\"caption_a\"],data_frame.loc[index,\"caption_e\"]\n",
    "        #                        ,data_frame.loc[index,\"caption_g\"],\n",
    "        #                    data_frame.loc[index,\"caption_l\"],data_frame.loc[index,\"SPEAKER_EMB\"],\n",
    "        #                    data_frame.loc[index,\"AUDIO_EMB\"],data_frame.loc[index,\"AUDIO_FEATURES\"],\n",
    "        #                    data_frame.loc[index,\"image_guide\"])\n",
    "        #    x = [x]\n",
    "        #    #AADFS = AADFS\n",
    "        #    data_frame.loc[index,\"INPUT\"] = x\n",
    "\n",
    "        data_frame = data_frame.drop(['caption_e', 'caption_g','caption_l','SPEAKER_EMB','AUDIO_EMB'], axis=1)\n",
    "\n",
    "\n",
    "        data_frame['image_path'] = data_frame['image_path'].apply(lambda x: process_image_path(x),meta=('1', 'str'))\n",
    "        data_frame.compute()\n",
    "\n",
    "        part = data_frame[['INPUT','image_path','low_res_image']].partitions[0]\n",
    "\n",
    "                \n",
    "\n",
    "        input0 = np.array(part['INPUT'])\n",
    "        input0 = np.array([np.array(xi) for xi in input0])\n",
    "        input0[np.isnan(input0)] = 0\n",
    "        input0[input0 > 10] = 10\n",
    "        input0[input0 < -10] = -10\n",
    "        #print(sys.getsizeof(input))\n",
    "        \n",
    "        output = np.array(part['image_path'])\n",
    "        output = np.array([np.array(xi) for xi in output])\n",
    "        output.squeeze().shape\n",
    "        #print(sys.getsizeof(output))\n",
    "\n",
    "        input2 = np.array(part['low_res_image'])\n",
    "        input2 = np.array([np.array(xi) for xi in input2])\n",
    "        input2.squeeze()\n",
    "        #print(sys.getsizeof(input2))\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "        #print(len(procs))\n",
    "        #if(len(procs) > 0):\n",
    "        #    procs[0].join() # Wait for previous process to finish\n",
    "        #    print(\"Model trained using a batch of data...\")\n",
    "        #    prProcess,\n",
    "        input0 = torch.from_numpy(input0)\n",
    "        input0 = input0.to(torch.float)\n",
    "\n",
    "\n",
    "        input2 = torch.from_numpy(input2)\n",
    "        input2 = input2.to(torch.float)\n",
    "\n",
    "        output = torch.from_numpy(output)\n",
    "        output = output.to(torch.float)\n",
    "\n",
    "        input0 = input0.squeeze()\n",
    "        input2 = input2.squeeze()\n",
    "        output = output.squeeze()\n",
    "        self.lock.acquire(block=True)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        self.lock.release()\n",
    "        return output,input0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor,Compose\n",
    "import dask.dataframe as dd\n",
    "from multiprocessing import Manager,RLock,Value,Queue\n",
    "import ctypes\n",
    "from multiprocessing import Manager\n",
    "\n",
    "class CustomDatasetWithCache(Dataset):\n",
    "\n",
    "    total_rows = None\n",
    "    con = None\n",
    "    lock = None\n",
    "    samples_seen =  Value('i', 0,lock=False)\n",
    "    mgr = None\n",
    "    ns = None\n",
    "    cache_size = None\n",
    "\n",
    "\n",
    "\n",
    "    def no_of_samples_seen(self):\n",
    "        return self.samples_seen.value\n",
    "\n",
    "    def __init__(self,datasetPathDatabase_a,lock,last_sample_no,cache_size):\n",
    "        self.lock = lock\n",
    "        self.samples_seen.value = last_sample_no\n",
    "        self.con = sl.connect(datasetPathDatabase_a)\n",
    "        self.mgr = Manager()\n",
    "        self.ns = self.mgr.Namespace()\n",
    "        self.ns.q_frame = pd.DataFrame()\n",
    "        self.cache_size = cache_size\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        if(self.total_rows == None):\n",
    "            self.lock.acquire(block=True)\n",
    "            data = self.con.execute(\"SELECT COUNT(*) FROM VIDEO v \")\n",
    "            count = data.fetchall()\n",
    "            self.total_rows = count[0][0]\n",
    "            data.close()\n",
    "            print(\"Found \" + str(self.total_rows) + \" data points.\")\n",
    "\n",
    "            #data = self.con.execute(\"SELECT COUNT(*) FROM VIDEO v WHERE V.TRAINED = 1\")\n",
    "            #count = data.fetchall()\n",
    "            #data.close()\n",
    "            #self.samples_seen.value = count[0][0]\n",
    "            #print(\"Starting from \" + str(self.samples_seen.value) + \" data points.\")\n",
    "            self.lock.release()\n",
    "\n",
    "        return self.total_rows\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.lock.acquire(block=True)\n",
    "        start_time = time.time()\n",
    "        print(\"Cache len \" + str(len(self.ns.q_frame)))\n",
    "        if(len(self.ns.q_frame) < self.cache_size // 2):\n",
    "            print(\"Filling cache from database\")\n",
    "            sql = (\"SELECT V.ID,F.ID,V.VIDEO_PATH, V.AGE CAPTION_A, \" + \n",
    "                                \"V.ETHNICITY CAPTION_E, \" +\n",
    "                                \"lower(V.GENDER) CAPTION_G, \" +\n",
    "                                    \"A.SPEAKER_EMB, \"+ (\"A.WAV_TO_VEC, \" if(audio_embs == 'wav2vec') else \"A.AUDIO_EMB2, \" if(audio_embs == 'openl3')  else \"A.PYANNOTE_TITANET, \" if(audio_embs == 'pyannoteTitaNet') else ', ') +\n",
    "                                \"A.AUDIO_FEATURES, \" +\n",
    "                                \"A.LANG CAPTION_L, \"+\n",
    "                                \"F.FACE_PATH \"+\n",
    "                                \"FROM VIDEO V \"+\n",
    "                                \"INNER JOIN AUDIO A ON V.ID = A.VIDEO_ID \" +\n",
    "                                \"INNER JOIN FACE F ON F.ID = (select ID from FACE f2 where f2.video_id = v.ID ORDER By ID limit 1 ) \" + \n",
    "                                \"WHERE AUDIO_LENGTH = \" + audio_length_used + ' ' +\n",
    "                                \"AND V.ID in (select V2.ID from VIDEO v2 WHERE V2.TRAINED IS NULL AND AUDIO_PRE IN (3,4) AND FACES_PRE = 2 ORDER BY ABS(RANDOM()) LIMIT \"+ str(self.cache_size) + \")\")\n",
    "            data = self.con.execute(sql)\n",
    "            dataGotten = data.fetchall()\n",
    "            df = pd.DataFrame(dataGotten,columns = ['ID','FACE_ID','VIDEO_PATH','caption_a','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','AUDIO_FEATURES','caption_l','image_path'])\n",
    "            data.close()\n",
    "            \n",
    "\n",
    "\n",
    "            if(len(df) == 0):\n",
    "                print(\"Trying to get items\")\n",
    "                data = self.con.execute(sql)\n",
    "                dataGotten = data.fetchall()\n",
    "                df = pd.DataFrame(dataGotten,columns = ['ID','FACE_ID','VIDEO_PATH','caption_a','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','AUDIO_FEATURES','caption_l','image_path'])\n",
    "                data.close()\n",
    "\n",
    "                if(len(df) == 0):\n",
    "                    print(\"Reintializing dataset\")\n",
    "                    sql50 = \"UPDATE VIDEO SET TRAINED = NULL\"\n",
    "                    cur50 = self.con.cursor()\n",
    "                    cur50.execute(sql50)\n",
    "                    self.con.commit()\n",
    "                    cur50.close()\n",
    "\n",
    "                    data = self.con.execute(sql)\n",
    "                    dataGotten = data.fetchall()\n",
    "                    df = pd.DataFrame(dataGotten,columns = ['ID','FACE_ID','VIDEO_PATH','caption_a','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','AUDIO_FEATURES','caption_l','image_path'])\n",
    "                    data.close()\n",
    "\n",
    "            data5 = np.array(df['ID']).astype('int')\n",
    "            data6 = []\n",
    "            for x in data5:\n",
    "                data6.append(int(x))\n",
    "            #print(len(data6))\n",
    "            #print(data6)\n",
    "            sql5 = \"UPDATE VIDEO SET TRAINED = 1 WHERE ID IN ({0})\".format(', '.join('?' for _ in data6))\n",
    "            #print(sql5)\n",
    "            cur5 = self.con.cursor()\n",
    "            cur5.execute(sql5, data6)\n",
    "            self.con.commit()\n",
    "            cur5.close()\n",
    "            self.samples_seen.value +=  len(df)\n",
    "            print (self.samples_seen.value,end='\\r')\n",
    "            self.ns.q_frame = pd.concat([self.ns.q_frame, df], ignore_index=True)\n",
    "            print(\"After filling cache has \" + str(len(self.ns.q_frame)))\n",
    "\n",
    "        df = self.ns.q_frame.head(1)\n",
    "        self.ns.q_frame = self.ns.q_frame.iloc[1:]\n",
    "        print(\"After taking one from cache, cache has \" + str(len(self.ns.q_frame)))\n",
    "        self.lock.release()\n",
    "\n",
    "        \n",
    "        \n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        df = dd.from_pandas(df, npartitions=1)\n",
    "        df[\"image_guide\"] = np.nan\n",
    "\n",
    "        #print(df.head(10))\n",
    "        data_frame = df[[\"ID\",\"FACE_ID\",\"image_path\",\"caption_a\",\"caption_e\",\"caption_g\",\"caption_l\"]]\n",
    "        data_frame['SPEAKER_EMB'] = df['SPEAKER_EMB']\n",
    "        data_frame['AUDIO_EMB'] = df['AUDIO_EMB']\n",
    "        data_frame['image_guide'] = df['image_guide']\n",
    "\n",
    "        \n",
    "        data_frame['SPEAKER_EMB'] = data_frame['SPEAKER_EMB'].apply(lambda x: speaker_emb_preprocess(x),meta=('1', 'object'))\n",
    "        data_frame['AUDIO_EMB'] = data_frame['AUDIO_EMB'].apply(lambda x: audio_emb_preprocess2(x),meta=('1', 'object'))\n",
    "        data_frame['AUDIO_FEATURES'] = data_frame['ID'].apply(lambda x: audio_features_preprocess(x),meta=('1', 'object'))\n",
    "        data_frame = data_frame.drop(['ID'], axis=1)\n",
    "        data_frame['image_guide'] = data_frame['image_path'].apply(lambda x: image_guide_preprocess_low_res_dummy(x),meta=('1', 'str'))\n",
    "        data_frame = data_frame.drop(['FACE_ID'], axis=1)\n",
    "        data_frame['caption_a'] = data_frame['caption_a'].apply(lambda x: process_age(x),meta=('1', 'object'))\n",
    "        data_frame['caption_g'] = data_frame['caption_g'].apply(lambda x: process_gender(x),meta=('1', 'object'))\n",
    "        data_frame['caption_l'] = data_frame['caption_l'].apply(lambda x: process_language(x),meta=('1', 'object'))\n",
    "        data_frame['caption_e'] = data_frame['caption_e'].apply(lambda x: process_ethnicity(x),meta=('1', 'object'))\n",
    "        data_frame['low_res_image'] = data_frame['image_path'].apply(lambda x: image_guide_preprocess_low_res(x),meta=('1', 'object'))\n",
    "\n",
    "        data_frame['INPUT'] = data_frame['SPEAKER_EMB']\n",
    "        #print(data_frame)\n",
    "\n",
    "        data_frame['INPUT'] = data_frame.apply(process_input,args=(),axis=1,meta=('1', 'object'))\n",
    "\n",
    "        #for index, row in data_frame.iterrows():\n",
    "        #    x = process_input(data_frame.loc[index,\"caption_a\"],data_frame.loc[index,\"caption_e\"]\n",
    "        #                        ,data_frame.loc[index,\"caption_g\"],\n",
    "        #                    data_frame.loc[index,\"caption_l\"],data_frame.loc[index,\"SPEAKER_EMB\"],\n",
    "        #                    data_frame.loc[index,\"AUDIO_EMB\"],data_frame.loc[index,\"AUDIO_FEATURES\"],\n",
    "        #                    data_frame.loc[index,\"image_guide\"])\n",
    "        #    x = [x]\n",
    "        #    #AADFS = AADFS\n",
    "        #    data_frame.loc[index,\"INPUT\"] = x\n",
    "\n",
    "        data_frame = data_frame.drop(['caption_e', 'caption_g','caption_l','SPEAKER_EMB','AUDIO_EMB'], axis=1)\n",
    "\n",
    "\n",
    "        data_frame['image_path'] = data_frame['image_path'].apply(lambda x: process_image_path(x),meta=('1', 'str'))\n",
    "        data_frame.compute()\n",
    "\n",
    "        part = data_frame[['INPUT','image_path','low_res_image']].partitions[0]\n",
    "\n",
    "                \n",
    "\n",
    "        input0 = np.array(part['INPUT'])\n",
    "        input0 = np.array([np.array(xi) for xi in input0])\n",
    "        input0[np.isnan(input0)] = 0\n",
    "        input0[input0 > 10] = 10\n",
    "        input0[input0 < -10] = -10\n",
    "        #print(sys.getsizeof(input))\n",
    "        \n",
    "        output = np.array(part['image_path'])\n",
    "        output = np.array([np.array(xi) for xi in output])\n",
    "        output.squeeze().shape\n",
    "        #print(sys.getsizeof(output))\n",
    "\n",
    "        input2 = np.array(part['low_res_image'])\n",
    "        input2 = np.array([np.array(xi) for xi in input2])\n",
    "        input2.squeeze()\n",
    "        #print(sys.getsizeof(input2))\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "        #print(len(procs))\n",
    "        #if(len(procs) > 0):\n",
    "        #    procs[0].join() # Wait for previous process to finish\n",
    "        #    print(\"Model trained using a batch of data...\")\n",
    "        #    prProcess,\n",
    "        input0 = torch.from_numpy(input0)\n",
    "        input0 = input0.to(torch.float)\n",
    "\n",
    "\n",
    "        input2 = torch.from_numpy(input2)\n",
    "        input2 = input2.to(torch.float)\n",
    "\n",
    "        output = torch.from_numpy(output)\n",
    "        output = output.to(torch.float)\n",
    "\n",
    "        input0 = input0.squeeze()\n",
    "        input2 = input2.squeeze()\n",
    "        output = output.squeeze()\n",
    "        self.lock.acquire(block=True)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        self.lock.release()\n",
    "        return output,input0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(sample_no,loss_value):\n",
    "\n",
    "    if(UNET == 1):\n",
    "        my_file = Path(model_filename + '/loss_total_1.picke')\n",
    "        if my_file.is_file():\n",
    "            with open(model_filename + '/loss_total_1.picke', 'rb') as handle:\n",
    "                loss_total = pickle.load(handle)\n",
    "        else:\n",
    "            loss_total = []\n",
    "    else:\n",
    "        my_file = Path(model_filename + '/loss_total_2.picke')\n",
    "        if my_file.is_file():\n",
    "            with open(model_filename + '/loss_total_2.picke', 'rb') as handle:\n",
    "                loss_total = pickle.load(handle)\n",
    "        else:\n",
    "            loss_total = []\n",
    "    \n",
    "    #print(loss_list)\n",
    "    #print(loss_total)\n",
    "    loss_total.append([sample_no,loss_value])\n",
    "\n",
    "    if(UNET == 1):\n",
    "        with open(model_filename +'/loss_total_1.picke', 'wb') as handle:\n",
    "            pickle.dump(loss_total, handle)\n",
    "    else:\n",
    "        with open(model_filename +'/loss_total_2.picke', 'wb') as handle:\n",
    "            pickle.dump(loss_total, handle)\n",
    "    #print(loss_total)\n",
    "    fig = plt.figure()\n",
    "    x_val = [x[0] for x in loss_total]\n",
    "    y_val = [x[1] for x in loss_total]\n",
    "    plt.plot(x_val,y_val)\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Training Sample\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    if(UNET == 1):\n",
    "        if(os.path.isfile(model_filename + '/loss_1_plot.png')):\n",
    "            os.remove(model_filename + '/loss_1_plot.png')\n",
    "        fig.savefig(model_filename + '/loss_1_plot.png')\n",
    "    else:\n",
    "        if(os.path.isfile(model_filename + '/loss_2_plot.png')):\n",
    "            os.remove(model_filename + '/loss_2_plot.png')\n",
    "        fig.savefig(model_filename + '/loss_2_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    try:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(x_val[1000::],y_val[1000::]\n",
    "                ,'.',zorder=-100)\n",
    "\n",
    "        #plt.axvline(x=1000,linestyle='--',color='green',label='1000 inner epochs')\n",
    "        #plt.axvline(x=17000,linestyle='--',color='purple',label='100 inner epochs - 100000 unique samples seen' )\n",
    "        #plt.axvline(x=23100,linestyle='-.',color='black',label='1 inner epoch - end of 1st epoch ')\n",
    "        #plt.legend(bbox_to_anchor = (1.0, 1), loc = 'upper right')\n",
    "        plt.grid()\n",
    "        yhat = savgol_filter(y_val, 1000, 3)\n",
    "        plt.plot(x_val[1000::],yhat[1000::],'r')\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.xlabel(\"Training Sample\")\n",
    "        plt.ylabel(\"MSE Loss\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        if(UNET == 1):\n",
    "            if(os.path.isfile(model_filename + '/loss_zoomed_1_plot.png')):\n",
    "                os.remove(model_filename + '/loss_zoomed_1_plot.png')\n",
    "            fig.savefig(model_filename + '/loss_zoomed_1_plot.png')\n",
    "        else:\n",
    "            if(os.path.isfile(model_filename + '/loss_zoomed_2_plot.png')):\n",
    "                os.remove(model_filename + '/loss_zoomed_2_plot.png')\n",
    "            fig.savefig(model_filename + '/loss_zoomed_2_plot.png')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    except:\n",
    "        try:\n",
    "            plt.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_sample_no = 0\n",
    "if(UNET == 1):\n",
    "    my_file = Path(model_filename + '/last_sample_no1.picke')\n",
    "    if my_file.is_file():\n",
    "        with open(model_filename + '/last_sample_no1.picke', 'rb') as handle:\n",
    "            last_sample_no = pickle.load(handle)\n",
    "else:\n",
    "    my_file = Path(model_filename + '/last_sample_no2.picke')\n",
    "    if my_file.is_file():\n",
    "        with open(model_filename + '/last_sample_no2.picke', 'rb') as handle:\n",
    "            last_sample_no = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 data points.\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "my_dataset = CustomDatasetWithCache(datasetPathDatabase,RLock(),last_sample_no, cache_size = dataloader_cache_size)\n",
    "print(len(my_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataloader = DataLoader(my_dataset,batch_size = batch_size * 16,shuffle=False,num_workers=16) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n",
      "no checkpoints found to load from at /extension/checkpoint/imagen_features_low_res_asis\n",
      "Trying to get items\n",
      "Trying to get items\n",
      "Trying to get items\n",
      "Reintializing dataset\n",
      "--- 0.2801628112792969 seconds ---\n",
      "--- 0.26519179344177246 seconds ---\n",
      "--- 0.2639584541320801 seconds ---\n",
      "--- 0.1472947597503662 seconds ---\n",
      "--- 0.13481974601745605 seconds ---\n",
      "--- 0.14632248878479004 seconds ---\n",
      "--- 0.13648366928100586 seconds ---\n",
      "--- 0.15949749946594238 seconds ---\n",
      "--- 0.15762853622436523 seconds ---\n",
      "--- 0.1482377052307129 seconds ---\n",
      "--- 0.1473698616027832 seconds ---\n",
      "--- 0.13956117630004883 seconds ---\n",
      "--- 0.15233302116394043 seconds ---\n",
      "--- 0.1538379192352295 seconds ---\n",
      "--- 0.15366697311401367 seconds ---\n",
      "--- 0.14486145973205566 seconds ---\n",
      "--- 0.14405322074890137 seconds ---\n",
      "--- 0.1432185173034668 seconds ---\n",
      "--- 0.13470172882080078 seconds ---\n",
      "--- 0.15514111518859863 seconds ---\n",
      "--- 0.13709545135498047 seconds ---\n",
      "--- 0.13510799407958984 seconds ---\n",
      "--- 0.14236855506896973 seconds ---\n",
      "--- 0.13463449478149414 seconds ---\n",
      "--- 0.13342928886413574 seconds ---\n",
      "--- 0.1350879669189453 seconds ---\n",
      "--- 0.13621973991394043 seconds ---\n",
      "--- 0.13997459411621094 seconds ---\n",
      "--- 0.13235926628112793 seconds ---\n",
      "--- 0.13344621658325195 seconds ---\n",
      "--- 0.14725875854492188 seconds ---\n",
      "--- 0.14488577842712402 seconds ---\n",
      "--- 0.14847588539123535 seconds ---\n",
      "--- 0.1407163143157959 seconds ---\n",
      "--- 0.1415867805480957 seconds ---\n",
      "--- 0.1300511360168457 seconds ---\n",
      "--- 0.12943100929260254 seconds ---\n",
      "--- 0.12911105155944824 seconds ---\n",
      "Trying to get items\n",
      "Trying to get items\n",
      "Trying to get items\n",
      "Reintializing dataset\n",
      "--- 0.14537262916564941 seconds ---\n",
      "--- 0.13041234016418457 seconds ---\n",
      "--- 0.14441633224487305 seconds ---\n",
      "--- 0.13027119636535645 seconds ---\n",
      "--- 0.12945008277893066 seconds ---\n",
      "--- 0.13183045387268066 seconds ---\n",
      "--- 0.1318657398223877 seconds ---\n",
      "--- 0.12684273719787598 seconds ---\n",
      "--- 0.12693047523498535 seconds ---\n",
      "--- 0.1294400691986084 seconds ---\n",
      "--- 0.12917137145996094 seconds ---\n",
      "--- 0.12132978439331055 seconds ---\n",
      "--- 0.1442093849182129 seconds ---\n",
      "--- 0.13150334358215332 seconds ---\n",
      "--- 0.1352860927581787 seconds ---\n",
      "--- 0.12788796424865723 seconds ---\n",
      "--- 0.12187480926513672 seconds ---\n",
      "--- 0.12223243713378906 seconds ---\n",
      "--- 0.1303410530090332 seconds ---\n",
      "--- 0.1317446231842041 seconds ---\n",
      "--- 0.1399233341217041 seconds ---\n",
      "--- 0.13953518867492676 seconds ---\n",
      "--- 0.13275957107543945 seconds ---\n",
      "--- 0.13292527198791504 seconds ---\n",
      "--- 0.13230609893798828 seconds ---\n",
      "--- 0.12328624725341797 seconds ---\n",
      "--- 0.11891865730285645 seconds ---\n",
      "--- 0.12175989151000977 seconds ---\n",
      "--- 0.12986040115356445 seconds ---\n",
      "--- 0.12896323204040527 seconds ---\n",
      "--- 0.14179635047912598 seconds ---\n",
      "--- 0.13306856155395508 seconds ---\n",
      "--- 0.13303494453430176 seconds ---\n",
      "--- 0.13912200927734375 seconds ---\n",
      "--- 0.13253188133239746 seconds ---\n",
      "--- 0.14283061027526855 seconds ---\n",
      "--- 0.12636137008666992 seconds ---\n",
      "--- 0.12776923179626465 seconds ---\n",
      "--- 0.11972880363464355 seconds ---\n",
      "--- 0.12402582168579102 seconds ---\n",
      "--- 0.11581683158874512 seconds ---\n",
      "--- 0.11739993095397949 seconds ---\n",
      "loss: 0.8069720692001283\n",
      "epoch:1 from 10000\n",
      "loss: 0.7859856858849525\n",
      "epoch:2 from 10000\n",
      "loss: 0.8281670287251472\n",
      "epoch:3 from 10000\n",
      "Trying to get items\n",
      "Trying to get items\n",
      "Trying to get items\n",
      "Reintializing dataset\n",
      "--- 0.2713205814361572 seconds ---\n",
      "--- 0.25637006759643555 seconds ---\n",
      "--- 0.2545044422149658 seconds ---\n",
      "--- 0.12679433822631836 seconds ---\n",
      "--- 0.1213984489440918 seconds ---\n",
      "--- 0.12102651596069336 seconds ---\n",
      "--- 0.13219356536865234 seconds ---\n",
      "--- 0.13156390190124512 seconds ---\n",
      "--- 0.1317455768585205 seconds ---\n",
      "--- 0.13077020645141602 seconds ---\n",
      "--- 0.13158535957336426 seconds ---\n",
      "--- 0.13274168968200684 seconds ---\n",
      "--- 0.13230681419372559 seconds ---\n",
      "--- 0.13176751136779785 seconds ---\n",
      "--- 0.13099074363708496 seconds ---\n",
      "--- 0.12394833564758301 seconds ---\n",
      "--- 0.12413406372070312 seconds ---\n",
      "--- 0.12416386604309082 seconds ---\n",
      "--- 0.12718462944030762 seconds ---\n",
      "--- 0.13145828247070312 seconds ---\n",
      "--- 0.13211846351623535 seconds ---\n",
      "--- 0.12567758560180664 seconds ---\n",
      "--- 0.12530159950256348 seconds ---\n",
      "--- 0.12497210502624512 seconds ---\n",
      "--- 0.12581443786621094 seconds ---\n",
      "--- 0.12371826171875 seconds ---\n",
      "--- 0.12398171424865723 seconds ---\n",
      "--- 0.1288607120513916 seconds ---\n",
      "--- 0.12844347953796387 seconds ---\n",
      "--- 0.1286635398864746 seconds ---\n",
      "--- 0.12553143501281738 seconds ---\n",
      "--- 0.1252293586730957 seconds ---\n",
      "--- 0.12472414970397949 seconds ---\n",
      "--- 0.12542128562927246 seconds ---\n",
      "--- 0.12653517723083496 seconds ---\n",
      "--- 0.1272132396697998 seconds ---\n",
      "--- 0.13148260116577148 seconds ---\n",
      "--- 0.13118290901184082 seconds ---\n",
      "--- 0.12225484848022461 seconds ---\n",
      "Trying to get items\n",
      "Trying to get items\n",
      "Trying to get items\n",
      "Reintializing dataset\n",
      "--- 0.12751984596252441 seconds ---\n",
      "--- 0.13811683654785156 seconds ---\n",
      "--- 0.12694978713989258 seconds ---\n",
      "--- 0.1202552318572998 seconds ---\n",
      "--- 0.12191295623779297 seconds ---\n",
      "--- 0.12051701545715332 seconds ---\n",
      "--- 0.12199282646179199 seconds ---\n",
      "--- 0.12123560905456543 seconds ---\n",
      "--- 0.12214541435241699 seconds ---\n",
      "--- 0.11644506454467773 seconds ---\n",
      "--- 0.11744260787963867 seconds ---\n",
      "--- 0.11899256706237793 seconds ---\n",
      "--- 0.12004327774047852 seconds ---\n",
      "--- 0.1207742691040039 seconds ---\n",
      "--- 0.12076783180236816 seconds ---\n",
      "--- 0.1419520378112793 seconds ---\n",
      "--- 0.14426136016845703 seconds ---\n",
      "--- 0.12537121772766113 seconds ---\n",
      "--- 0.12525081634521484 seconds ---\n",
      "--- 0.12861108779907227 seconds ---\n",
      "--- 0.12896060943603516 seconds ---\n",
      "--- 0.12520217895507812 seconds ---\n",
      "--- 0.12760233879089355 seconds ---\n",
      "--- 0.13009357452392578 seconds ---\n",
      "--- 0.12466669082641602 seconds ---\n",
      "--- 0.12797784805297852 seconds ---\n",
      "--- 0.12062406539916992 seconds ---\n",
      "--- 0.12123894691467285 seconds ---\n",
      "--- 0.12065935134887695 seconds ---\n",
      "--- 0.12312531471252441 seconds ---\n",
      "--- 0.12117862701416016 seconds ---\n",
      "--- 0.12392520904541016 seconds ---\n",
      "--- 0.12485599517822266 seconds ---\n",
      "--- 0.12067580223083496 seconds ---\n",
      "--- 0.12108993530273438 seconds ---\n",
      "--- 0.12526321411132812 seconds ---\n",
      "--- 0.1262190341949463 seconds ---\n",
      "--- 0.13002252578735352 seconds ---\n",
      "--- 0.12848258018493652 seconds ---\n",
      "--- 0.3045041561126709 seconds ---\n",
      "--- 0.30307626724243164 seconds ---\n",
      "loss: 0.800429810769856\n",
      "epoch:4 from 10000\n",
      "loss: 0.7149149281904101\n",
      "epoch:5 from 10000\n",
      "loss: 0.8329859897494316\n",
      "epoch:6 from 10000\n",
      "Trying to get items\n",
      "Trying to get items\n",
      "Trying to get items\n",
      "Reintializing dataset\n",
      "--- 0.2847473621368408 seconds ---\n",
      "--- 0.27207374572753906 seconds ---\n",
      "--- 0.2752256393432617 seconds ---\n",
      "--- 0.14254355430603027 seconds ---\n",
      "--- 0.13651180267333984 seconds ---\n",
      "--- 0.1317145824432373 seconds ---\n",
      "--- 0.12724089622497559 seconds ---\n",
      "--- 0.12954092025756836 seconds ---\n",
      "--- 0.13036561012268066 seconds ---\n",
      "--- 0.13843965530395508 seconds ---\n",
      "--- 0.1263415813446045 seconds ---\n",
      "--- 0.13764548301696777 seconds ---\n",
      "--- 0.13857650756835938 seconds ---\n",
      "--- 0.1377112865447998 seconds ---\n",
      "--- 0.1359245777130127 seconds ---\n",
      "--- 0.13474059104919434 seconds ---\n",
      "--- 0.13625574111938477 seconds ---\n",
      "--- 0.14507460594177246 seconds ---\n",
      "--- 0.15491437911987305 seconds ---\n",
      "--- 0.1453871726989746 seconds ---\n",
      "--- 0.1553964614868164 seconds ---\n",
      "--- 0.14412379264831543 seconds ---\n",
      "--- 0.14670467376708984 seconds ---\n",
      "--- 0.14086318016052246 seconds ---\n",
      "--- 0.14011764526367188 seconds ---\n",
      "--- 0.1453542709350586 seconds ---\n",
      "--- 0.1394195556640625 seconds ---\n",
      "--- 0.13780593872070312 seconds ---\n",
      "--- 0.13379120826721191 seconds ---\n",
      "--- 0.13581585884094238 seconds ---\n",
      "--- 0.35549259185791016 seconds ---\n",
      "--- 0.3548142910003662 seconds ---\n",
      "--- 0.3465404510498047 seconds ---\n",
      "--- 0.13082432746887207 seconds ---\n",
      "--- 0.1312875747680664 seconds ---\n",
      "--- 0.13351011276245117 seconds ---\n",
      "--- 0.1464087963104248 seconds ---\n",
      "--- 0.14472675323486328 seconds ---\n",
      "--- 0.136948823928833 seconds ---\n",
      "Trying to get items\n",
      "Trying to get items\n",
      "Trying to get items\n",
      "Reintializing dataset\n",
      "--- 0.12790393829345703 seconds ---\n",
      "--- 0.14238977432250977 seconds ---\n",
      "--- 0.13045167922973633 seconds ---\n",
      "--- 0.13699650764465332 seconds ---\n",
      "--- 0.13603568077087402 seconds ---\n",
      "--- 0.14571213722229004 seconds ---\n",
      "--- 0.12462186813354492 seconds ---\n",
      "--- 0.12490510940551758 seconds ---\n",
      "--- 0.11952662467956543 seconds ---\n",
      "--- 0.12853384017944336 seconds ---\n",
      "--- 0.12120842933654785 seconds ---\n",
      "--- 0.12281155586242676 seconds ---\n",
      "--- 0.11917424201965332 seconds ---\n",
      "--- 0.12168145179748535 seconds ---\n",
      "--- 0.11897134780883789 seconds ---\n",
      "--- 0.1292402744293213 seconds ---\n",
      "--- 0.12694001197814941 seconds ---\n",
      "--- 0.11914253234863281 seconds ---\n",
      "--- 0.1190190315246582 seconds ---\n",
      "--- 0.1215658187866211 seconds ---\n",
      "--- 0.12049365043640137 seconds ---\n",
      "--- 0.12006020545959473 seconds ---\n",
      "--- 0.11591267585754395 seconds ---\n",
      "--- 0.11933112144470215 seconds ---\n",
      "--- 0.11637759208679199 seconds ---\n",
      "--- 0.1278667449951172 seconds ---\n",
      "--- 0.12398195266723633 seconds ---\n",
      "--- 0.12637925148010254 seconds ---\n",
      "--- 0.11675357818603516 seconds ---\n",
      "--- 0.1246786117553711 seconds ---\n",
      "--- 0.11660122871398926 seconds ---\n",
      "--- 0.12346029281616211 seconds ---\n",
      "--- 0.1250441074371338 seconds ---\n",
      "--- 0.12758302688598633 seconds ---\n",
      "--- 0.1486053466796875 seconds ---\n",
      "--- 0.13451218605041504 seconds ---\n",
      "--- 0.14133954048156738 seconds ---\n",
      "--- 0.12923383712768555 seconds ---\n",
      "--- 0.12715649604797363 seconds ---\n",
      "--- 0.12162947654724121 seconds ---\n",
      "--- 0.12101602554321289 seconds ---\n",
      "loss: 0.7887157620862126\n",
      "epoch:7 from 10000\n",
      "loss: 0.7342488020658493\n",
      "epoch:8 from 10000\n",
      "loss: 0.848586268723011\n",
      "epoch:9 from 10000\n",
      "Trying to get items\n",
      "Trying to get items\n",
      "Trying to get items\n",
      "Reintializing dataset\n",
      "--- 0.4997217655181885 seconds ---\n",
      "--- 0.48444628715515137 seconds ---\n",
      "--- 0.47275829315185547 seconds ---\n",
      "--- 0.13817191123962402 seconds ---\n",
      "--- 0.13737702369689941 seconds ---\n",
      "--- 0.13927364349365234 seconds ---\n",
      "--- 0.14330267906188965 seconds ---\n",
      "--- 0.14596128463745117 seconds ---\n",
      "--- 0.1365983486175537 seconds ---\n",
      "--- 0.12675189971923828 seconds ---\n",
      "--- 0.15675592422485352 seconds ---\n",
      "--- 0.14717507362365723 seconds ---\n",
      "--- 0.1344761848449707 seconds ---\n",
      "--- 0.14245176315307617 seconds ---\n",
      "--- 0.13474798202514648 seconds ---\n",
      "--- 0.13063359260559082 seconds ---\n",
      "--- 0.1312112808227539 seconds ---\n",
      "--- 0.1325702667236328 seconds ---\n",
      "--- 0.14269208908081055 seconds ---\n",
      "--- 0.1344289779663086 seconds ---\n",
      "--- 0.13709735870361328 seconds ---\n",
      "--- 0.1359260082244873 seconds ---\n",
      "--- 0.13731122016906738 seconds ---\n",
      "--- 0.13795852661132812 seconds ---\n",
      "--- 0.127091646194458 seconds ---\n",
      "--- 0.1695849895477295 seconds ---\n",
      "--- 0.15022659301757812 seconds ---\n",
      "--- 0.13105010986328125 seconds ---\n",
      "--- 0.14449501037597656 seconds ---\n",
      "--- 0.14673542976379395 seconds ---\n",
      "--- 0.13030099868774414 seconds ---\n",
      "--- 0.1289513111114502 seconds ---\n",
      "--- 0.12987112998962402 seconds ---\n",
      "--- 0.13177156448364258 seconds ---\n",
      "--- 0.13137435913085938 seconds ---\n",
      "--- 0.1398601531982422 seconds ---\n",
      "--- 0.12740230560302734 seconds ---\n",
      "--- 0.12334299087524414 seconds ---\n",
      "Trying to get items\n",
      "Trying to get items\n",
      "Trying to get items\n",
      "Reintializing dataset\n",
      "--- 0.1279289722442627 seconds ---\n",
      "--- 0.13594555854797363 seconds ---\n",
      "--- 0.12644696235656738 seconds ---\n",
      "--- 0.15038490295410156 seconds ---\n",
      "--- 0.12270760536193848 seconds ---\n",
      "--- 0.1245725154876709 seconds ---\n",
      "--- 0.12511563301086426 seconds ---\n",
      "--- 0.13412952423095703 seconds ---\n",
      "--- 0.12456393241882324 seconds ---\n",
      "--- 0.13122129440307617 seconds ---\n",
      "--- 0.12916207313537598 seconds ---\n",
      "--- 0.1270442008972168 seconds ---\n",
      "--- 0.12732982635498047 seconds ---\n",
      "--- 0.13130998611450195 seconds ---\n",
      "--- 0.12209296226501465 seconds ---\n",
      "--- 0.12385678291320801 seconds ---\n",
      "--- 0.14008164405822754 seconds ---\n",
      "--- 0.13552093505859375 seconds ---\n",
      "--- 0.12027692794799805 seconds ---\n",
      "--- 0.12055587768554688 seconds ---\n",
      "--- 0.13612031936645508 seconds ---\n",
      "--- 0.13562655448913574 seconds ---\n",
      "--- 0.11980271339416504 seconds ---\n",
      "--- 0.12131428718566895 seconds ---\n",
      "--- 0.12679004669189453 seconds ---\n",
      "--- 0.12648415565490723 seconds ---\n",
      "--- 0.12336897850036621 seconds ---\n",
      "--- 0.12377262115478516 seconds ---\n",
      "--- 0.12041974067687988 seconds ---\n",
      "--- 0.1188056468963623 seconds ---\n",
      "--- 0.12365055084228516 seconds ---\n",
      "--- 0.12635231018066406 seconds ---\n",
      "--- 0.12825775146484375 seconds ---\n",
      "--- 0.12781572341918945 seconds ---\n",
      "--- 0.1350095272064209 seconds ---\n",
      "--- 0.13511419296264648 seconds ---\n",
      "--- 0.1287858486175537 seconds ---\n",
      "--- 0.12856745719909668 seconds ---\n",
      "--- 0.12544035911560059 seconds ---\n",
      "--- 0.1250462532043457 seconds ---\n",
      "--- 0.12856578826904297 seconds ---\n",
      "--- 0.12688255310058594 seconds ---\n",
      "loss: 0.8743038363754749\n",
      "epoch:10 from 10000\n"
     ]
    }
   ],
   "source": [
    "from imagen_pytorch import Unet, Imagen, ImagenTrainer,NullUnet\n",
    "from imagen_pytorch.data import Dataset\n",
    "\n",
    "unet0 = NullUnet()  # add a placeholder \"null\" unet for the base unet\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = unet1_dim,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = 3,\n",
    "    layer_attns = (False, True, True, True),\n",
    "    layer_cross_attns = (False, True, True, True)\n",
    ")\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = unet2_dim,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = (2, 4, 8, 8),\n",
    "    layer_attns = (False, False, False, True),\n",
    "    layer_cross_attns = (False, False, False, True)\n",
    ")\n",
    "\n",
    "if(ignore_image_guide):\n",
    "    unets = (unet1, unet2)\n",
    "    image_sizes= (unet1_image_size, image_size)\n",
    "    unet_to_train = UNET #1\n",
    "    #start_image_or_video = None\n",
    "else:\n",
    "    unets = (unet0,unet1, unet2)\n",
    "    image_sizes= (begin_with_image_size,unet1_image_size, image_size)\n",
    "    unet_to_train = UNET + 1 #2\n",
    "    #start_image_or_video = input2[:1,:]\n",
    "\n",
    "#print(input2[:1,:])\n",
    "#print(input2)\n",
    "\n",
    "imagen = Imagen(\n",
    "    unets = unets,\n",
    "    image_sizes = image_sizes,\n",
    "    timesteps = timesteps,\n",
    "    cond_drop_prob = 0.1\n",
    ").cuda()\n",
    "\n",
    "\n",
    "\n",
    "trainer = ImagenTrainer(\n",
    "    imagen = imagen,\n",
    "    split_valid_from_train = False, # whether to split the validation dataset from the training\n",
    "    checkpoint_every = save_model_every,\n",
    "    checkpoint_path = model_filename,\n",
    "    max_checkpoints_keep = 1,\n",
    "    only_train_unet_number = unet_to_train\n",
    ").cuda()\n",
    "\n",
    "\n",
    "trainer.add_train_dataloader(my_dataloader)\n",
    "\n",
    "#if not os.path.exists(imagen_samples_folder):\n",
    "#    os.makedirs(imagen_samples_folder)\n",
    "\n",
    "#my_file = Path(model_filename)\n",
    "#if my_file.is_file():\n",
    "#    print('Using model file ' + model_filename)\n",
    "#    trainer.load(model_filename)\n",
    "\n",
    "#trainer.load_from_checkpoint_folder()\n",
    "#print(epochs)\n",
    "for i in range(epochs):\n",
    "    \n",
    "    loss = trainer.train_step(unet_number = unet_to_train,max_batch_size = batch_size)\n",
    "    print(f'loss: {loss}')\n",
    "    #print(\"At sample seen \" + str(my_dataset.no_of_samples_seen()))\n",
    "    plot_loss(my_dataset.no_of_samples_seen(),loss)\n",
    "    #print(loss_list)\n",
    "    #trainer.save_to_checkpoint_folder()\n",
    "    print('epoch:' + str(i+1) + ' from ' + str(epochs))\n",
    "    last_sample_no = my_dataset.no_of_samples_seen()\n",
    "    if(UNET == 1):\n",
    "        with open(model_filename + '/last_sample_no1.picke', 'wb') as handle:\n",
    "            pickle.dump(last_sample_no, handle)\n",
    "    else:\n",
    "        with open(model_filename + '/last_sample_no2.picke', 'wb') as handle:\n",
    "            pickle.dump(last_sample_no, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_to_checkpoint_folder()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78dccc615e96ab04385280185a87a524fe0822daf5dbad2f4bf2e7d7b28366a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
