{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNET = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "# Loading configurations\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "\n",
    "insert_amd_env_vars =  int(configParser.get('COMMON', 'insert_amd_env_vars'))\n",
    "HSA_OVERRIDE_GFX_VERSION =  configParser.get('COMMON', 'HSA_OVERRIDE_GFX_VERSION')\n",
    "ROCM_PATH =  configParser.get('COMMON', 'ROCM_PATH')\n",
    "\n",
    "if(insert_amd_env_vars != 0):\n",
    "    os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = HSA_OVERRIDE_GFX_VERSION\n",
    "    os.environ[\"ROCM_PATH\"] = ROCM_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#TODO document jupyter\n",
    "import pickle\n",
    "import configparser\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "datasetPathDatabase =  configParser.get('COMMON', 'datasetPathDatabase') + '/dataset.db'\n",
    "datasetPathDatabaseAdditional =  configParser.get('COMMON', 'datasetPathDatabase') + '/datasetAdditional.db'\n",
    "datasetPathDatabaseVgg =  configParser.get('COMMON', 'datasetPathDatabase') + '/datasetFacesBlurred.db'\n",
    "image_size =  int(configParser.get('COMMON', 'resizeImageTo'))\n",
    "begin_with_image_size = int(configParser.get('COMMON', 'begin_with_image_size'))\n",
    "unet1_dim =  int(configParser.get('COMMON', 'unet1_dim'))\n",
    "unet2_dim =  int(configParser.get('COMMON', 'unet2_dim'))\n",
    "unet1_image_size =  int(configParser.get('COMMON', 'unet1_image_size'))\n",
    "audio_embs =  str(configParser.get('COMMON', 'audio_embs'))\n",
    "audio_length_used =  configParser.get('train_imagen', 'audio_length_used') \n",
    "model_filename =  configParser.get('train_imagen', 'model_filename')\n",
    "sub_epochs=  int(configParser.get('train_imagen', 'sub_epochs') )\n",
    "inner_epochs=  int(configParser.get('train_imagen', 'inner_epochs') )\n",
    "batch_size=  int(configParser.get('train_imagen', 'batch_size') )\n",
    "timesteps= int(configParser.get('COMMON', 'timesteps') )\n",
    "sample_every=  int(configParser.get('train_imagen', 'sample_every') ) - 1\n",
    "sample_probability = int(configParser.get('train_imagen', 'sample_probability') )\n",
    "save_model_every=  int(configParser.get('train_imagen', 'save_model_every') ) - 1\n",
    "sample_every_offset=  int(configParser.get('train_imagen', 'sample_every_offset') ) - 1\n",
    "save_every_offset=  int(configParser.get('train_imagen', 'save_every_offset') ) - 1\n",
    "imagen_samples_folder = configParser.get('train_imagen', 'imagen_samples_folder') \n",
    "db_chunk = int(configParser.get('train_imagen', 'db_chunk'))\n",
    "dask_chunk = int(configParser.get('train_imagen', 'dask_chunk'))\n",
    "stop_at_no_of_samples = int(configParser.get('train_imagen', 'stop_at_no_of_samples'))\n",
    "epochs = int(configParser.get('train_imagen', 'epochs'))\n",
    "ignore_speaker_embedding = bool(int(configParser.get('train_imagen', 'ignore_speaker_embedding') ))\n",
    "ignore_speech_brain = bool(int(configParser.get('train_imagen', 'ignore_speech_brain') ))\n",
    "ignore_pyannote_titanet_speakernet = bool(int(configParser.get('train_imagen', 'ignore_pyannote_titanet_speakernet') ))\n",
    "ignore_audio_features = bool(int(configParser.get('train_imagen', 'ignore_audio_features') ))\n",
    "ignore_pyAudioAnalysis = bool(int(configParser.get('train_imagen', 'ignore_pyAudioAnalysis') ))\n",
    "ignore_librosa = bool(int(configParser.get('train_imagen', 'ignore_librosa') ))\n",
    "ignore_image_guide = bool(int(configParser.get('train_imagen', 'ignore_image_guide') ))\n",
    "ignore_additional_attributes = bool(int(configParser.get('train_imagen', 'ignore_additional_attributes') ))\n",
    "ignore_age = bool(int(configParser.get('train_imagen', 'ignore_age') ))\n",
    "ignore_gender = bool(int(configParser.get('train_imagen', 'ignore_gender') ))\n",
    "ignore_ethnicity = bool(int(configParser.get('train_imagen', 'ignore_ethnicity') ))\n",
    "ignore_language_spoken = bool(int(configParser.get('train_imagen', 'ignore_language_spoken') ))\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speaker_emb_preprocess(speaker_emb2):\n",
    "\n",
    "    if(ignore_speaker_embedding or ignore_speech_brain):\n",
    "        #print('ignore speech brain')\n",
    "        return np.zeros((1,768))\n",
    "    else:\n",
    "        speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "        #print(speaker_emb2.print)\n",
    "        speaker_emb2 = speaker_emb2.squeeze()\n",
    "        speaker_emb2 = np.pad(speaker_emb2, (288), 'constant', constant_values=(0))\n",
    "        speaker_emb2 = np.tile(speaker_emb2, (1, 1))\n",
    "        speaker_emb2 = speaker_emb2 / 200.0\n",
    "        #print(speaker_emb2.shape)\n",
    "        #speaker_emb2 = np.array(speaker_emb2).tolist()\n",
    "        #print(type(speaker_emb2))\n",
    "        return speaker_emb2\n",
    "\n",
    "\n",
    "def audio_emb_preprocess2(speaker_emb2):\n",
    "    \n",
    "    if(ignore_pyannote_titanet_speakernet or ignore_speaker_embedding):\n",
    "        #print('ignore speech brain')\n",
    "        return np.zeros((28,768))\n",
    "    else:\n",
    "        #print(speaker_emb2)\n",
    "        speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "        a = speaker_emb2 #np.zeros(shape=(24, 512))\n",
    "        a = a[-28:,:] # Actually only last 39 are the relevant ones\n",
    "        #print('au')\n",
    "        #print(a.shape)\n",
    "        b = np.zeros(shape=(a.shape[0], 768-a.shape[1]))\n",
    "        #c = np.zeros(shape=(28-a.shape[0], 768))\n",
    "        arr = np.concatenate((a, b), axis=1)\n",
    "        #arr = np.concatenate((arr, c), axis=0)\n",
    "        arr = arr / (1 if(audio_embs == 'wav2vec') else 10 if(audio_embs == 'openl3')  else 1 if(audio_embs == 'pyannoteTitaNet') else 1)\n",
    "        #print(str(arr.max()))\n",
    "        #print(arr.shape)\n",
    "        speaker_emb2 = np.array(arr)\n",
    "        return speaker_emb2\n",
    "\n",
    "def audio_features_preprocess(video_id):\n",
    "    #  79 belong to pyaudioanalysis\n",
    "    # 111 belogn to liborsa\n",
    "    if(ignore_audio_features):\n",
    "        #print('ignore speech brain')\n",
    "        return np.zeros((190,768))\n",
    "    else:\n",
    "        #print(video_id)\n",
    "        conAdditional = sl.connect(datasetPathDatabaseAdditional)\n",
    "        cur = conAdditional.cursor()\n",
    "        sql = '''SELECT AUDIO_FEATURES FROM AUDIO WHERE VIDEO_ID = ?'''\n",
    "        cur.execute(sql, [video_id])\n",
    "        audio_features = cur.fetchall()\n",
    "        #print(audio_features[0])\n",
    "        audio_features = pickle.loads(audio_features[0][0])\n",
    "        #print(audio_features.shape) # 190 x 128\n",
    "        audio_features = audio_features[0:190]\n",
    "        \n",
    "        #import sys\n",
    "        #np.set_printoptions(threshold=sys.maxsize)\n",
    "        #print(np.argwhere(audio_features == 0))\n",
    "\n",
    "        if(ignore_pyAudioAnalysis and not ignore_librosa):\n",
    "            zpa = np.zeros((190-110,128))\n",
    "            audio_features = audio_features[80:190]\n",
    "            audio_features = np.vstack((audio_features,zpa))\n",
    "            #print('ignore pyAudioAnalysis')\n",
    "            #print(audio_features.shape)\n",
    "        elif(ignore_librosa and not ignore_pyAudioAnalysis):\n",
    "            zpa = np.zeros((190-80,128))\n",
    "            audio_features = audio_features[0:80]\n",
    "            audio_features = np.vstack((audio_features,zpa))\n",
    "            #print('ignore librosa')\n",
    "            #print(audio_features.shape)\n",
    "        else:\n",
    "            audio_features = np.zeros((190,128))\n",
    "            #print('ignore pyaudioanalysis and librosa')\n",
    "\n",
    "\n",
    "        z1 = np.zeros((190,768-128))\n",
    "        audio_features = np.hstack((audio_features,z1))\n",
    "        audio_features = audio_features / 100.0\n",
    "        #print(audio_features.shape)\n",
    "    return audio_features\n",
    "\n",
    "def audio_transformer_features_preprocess(video_id):\n",
    "    #print(video_id)\n",
    "    conAdditional = sl.connect(datasetPathDatabaseAdditional)\n",
    "    cur = conAdditional.cursor()\n",
    "    sql = '''SELECT AUDIO_FEATURES FROM AUDIO_TRANSFORMER WHERE VIDEO_ID = ?'''\n",
    "    cur.execute(sql, [video_id])\n",
    "    audio_features = cur.fetchall()\n",
    "    #print(audio_features[0])\n",
    "    audio_features = pickle.loads(audio_features[0][0])\n",
    "    #print(audio_features.shape) # 514 x 768\n",
    "    audio_features = audio_features.squeeze()\n",
    "    audio_features = audio_features[0::3]#172*768\n",
    "    #z1 = np.zeros((161,768-128))\n",
    "    #audio_features = np.hstack((audio_features,z1))\n",
    "    #audio_features = audio_features / 100.0\n",
    "    return audio_features\n",
    "\n",
    "import random\n",
    "def image_guide_preprocess(face_id):\n",
    "    #print(face_id)\n",
    "\n",
    "    if(random.random() > 2):\n",
    "        image_guide = np.zeros((49,768))\n",
    "        #print(image_guide.shape)\n",
    "    else:\n",
    "        conAdditional = sl.connect(datasetPathDatabaseVgg)\n",
    "        cur = conAdditional.cursor()\n",
    "        sql = '''SELECT BLURRED_FACE_EMB FROM FACES_BLURRED WHERE FACE_ID = ?'''\n",
    "        cur.execute(sql, [face_id])\n",
    "        image_guide = cur.fetchall()\n",
    "        #print(image_guide)\n",
    "        image_guide = pickle.loads(image_guide[0][0])\n",
    "        image_guide = image_guide.squeeze() #197 x 768\n",
    "        image_guide = image_guide[1::4] #49 x 768\n",
    "    return image_guide\n",
    "\n",
    "\n",
    "#boxBlurMin =  int(configParser.get('extractVggBlurred', 'boxBlurMin'))\n",
    "#boxBlurMax =  int(configParser.get('extractVggBlurred', 'boxBlurMax'))\n",
    "\n",
    "#gaussianBlurMin =  int(configParser.get('extractVggBlurred', 'gaussianBlurMin'))\n",
    "#gaussianBlurMax =  int(configParser.get('extractVggBlurred', 'gaussianBlurMax'))\n",
    "\n",
    "\n",
    "from PIL import Image,ImageFilter\n",
    "import random\n",
    "#import cv2\n",
    "import numpy as np\n",
    "#from matplotlib import pyplot as plt\n",
    "  \n",
    "\n",
    "\n",
    "def image_guide_preprocess_low_res_dummy(path):\n",
    "    #print(face_id)\n",
    "\n",
    "    image_guide = np.zeros((1, 768))\n",
    "    return image_guide\n",
    "\n",
    "def image_guide_preprocess_low_res(path):\n",
    "    #print(face_id)\n",
    "\n",
    "\n",
    "    if(ignore_image_guide):\n",
    "        return np.nan\n",
    "    else:\n",
    "\n",
    "        image = Image.open(path).convert('RGB')\n",
    "\n",
    "        #print(image.size)\n",
    "\n",
    "        w_s = image_size / (1+2 * 0.4)\n",
    "        h_s = image_size / (1+2 * 0.4)\n",
    "\n",
    "        image = image.crop((0.2*w_s, 0.0*h_s, 1.6*w_s, 1.4*h_s))\n",
    "\n",
    "        image = image.resize((image_size,image_size))\n",
    "\n",
    "        image = image.resize((begin_with_image_size,begin_with_image_size))\n",
    "        im = image\n",
    "        \n",
    "\n",
    "        #print('saving')\n",
    "        #image.save('opop.png')\n",
    "\n",
    "        #print(np.array(image,np.float32).shape)\n",
    "\n",
    "        pix = np.array(image, np.float32)\n",
    "        pix = np.moveaxis(pix, -1, 0)\n",
    "\n",
    "        pix = pix / 255\n",
    "        image.close()\n",
    "        im.close()\n",
    "        return pix\n",
    "    return image_guide\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def process_age(age):\n",
    "    if(random.random() < 0.2 or ignore_age or ignore_additional_attributes):\n",
    "        x = np.zeros(768)\n",
    "        x[767] = 1\n",
    "        return x\n",
    "    else:\n",
    "        try:\n",
    "            x = np.ones(768) * (age / 100.0)\n",
    "            x[767] = 0\n",
    "        except:\n",
    "            x = np.zeros(768)\n",
    "            x[767] = 1\n",
    "\n",
    "        return x\n",
    "\n",
    "def process_gender(gender):\n",
    "    if(random.random() < 0.2 or ignore_gender or ignore_additional_attributes):\n",
    "        return np.zeros(768)\n",
    "    elif(gender == 'man'):\n",
    "        return np.ones(768)\n",
    "    elif(gender == \"woman\"):\n",
    "        return np.ones(768) * -1\n",
    "    else:\n",
    "        return np.zeros(768)\n",
    "    \n",
    "# TODO\n",
    "def process_ethnicity(eth):\n",
    "    x = np.zeros(768)\n",
    "    if(random.random() < 0.2 or ignore_ethnicity or ignore_additional_attributes):\n",
    "        x = x\n",
    "    elif(eth == \"indian\"):\n",
    "        x[0] = 1\n",
    "    elif(eth == \"asian\"):\n",
    "        x[16]=1\n",
    "    elif(eth == \"latino hispanic\"):\n",
    "        x[2]=1\n",
    "    elif(eth == \"black\"):\n",
    "        x[3]=1\n",
    "    elif(eth == \"middle eastern\"):\n",
    "        x[4]=1\n",
    "    elif(eth == \"white\"):\n",
    "        x[5]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x     \n",
    "\n",
    "def process_language(lan):\n",
    "    x = np.zeros(768)\n",
    "    if(random.random() < 0.2 or ignore_language_spoken or ignore_additional_attributes):\n",
    "        x = x\n",
    "    elif(lan == \"Arabic\"):\n",
    "        x[0] = 1\n",
    "    elif(lan == \"Portuguese\"):\n",
    "        x[16]=1\n",
    "    elif(lan == \"Romansh_Sursilvan\"):\n",
    "        x[2]=1\n",
    "    elif(lan == \"Japanese\"):\n",
    "        x[3]=1\n",
    "    elif(lan == \"Ukranian\"):\n",
    "        x[4]=1\n",
    "    elif(lan == \"German\"):\n",
    "        x[5]=1   \n",
    "    elif(lan == \"Chinese_China\"):\n",
    "        x[6]=1   \n",
    "    elif(lan == \"Welsh\"):\n",
    "        x[7]=1  \n",
    "    elif(lan == \"English\"):\n",
    "        x[8]=1\n",
    "    elif(lan == \"Kabyle\"):\n",
    "        x[9]=1 \n",
    "    elif(lan == \"Kyrgyz\"):\n",
    "        x[10]=1\n",
    "    elif(lan == \"Georgian\"):\n",
    "        x[11]=1\n",
    "    elif(lan == \"Persian\"):\n",
    "        x[12]=1 \n",
    "    elif(lan == \"French\"):\n",
    "        x[13]=1\n",
    "    elif(lan == \"Interlingua\"):\n",
    "        x[14]=1\n",
    "    elif(lan == \"Swedish\"):\n",
    "        x[15]=1\n",
    "    elif(lan == \"Spanish\"):\n",
    "        x[16]=1 \n",
    "    elif(lan == \"Dhivehi\"):\n",
    "        x[17]=1\n",
    "    elif(lan == \"Kinyarwanda\"):\n",
    "        x[18]=1 \n",
    "    elif(lan == \"Tatar\"):\n",
    "        x[19]=1\n",
    "    elif(lan == \"Hakha_Chin\"):\n",
    "        x[20]=1 \n",
    "    elif(lan == \"Tamil\"):\n",
    "        x[21]=1 \n",
    "    elif(lan == \"Greek\"):\n",
    "        x[22]=1\n",
    "    elif(lan == \"Latvian\"):\n",
    "        x[23]=1 \n",
    "    elif(lan == \"Russian\"):\n",
    "        x[24]=1\n",
    "    elif(lan == \"Breton\"):\n",
    "        x[25]=1\n",
    "    elif(lan == \"Catalan\"):\n",
    "        x[26]=1    \n",
    "    elif(lan == \"Maltese\"):\n",
    "        x[27]=1 \n",
    "    elif(lan == \"Slovenian\"):\n",
    "        x[28]=1    \n",
    "    elif(lan == \"Indonesian\"):\n",
    "        x[29]=1    \n",
    "    elif(lan == \"Dutch\"):\n",
    "        x[30]=1\n",
    "    elif(lan == \"Chinese_Taiwan\"):\n",
    "        x[31]=1 \n",
    "    elif(lan == \"Sakha\"):\n",
    "        x[32]=1 \n",
    "    elif(lan == \"Polish\"):\n",
    "        x[33]=1 \n",
    "    elif(lan == \"Czech\"):\n",
    "        x[34]=1 \n",
    "    elif(lan == \"Romanian\"):\n",
    "        x[35]=1 \n",
    "    elif(lan == \"Mangolian\"):\n",
    "        x[36]=1 \n",
    "    elif(lan == \"Italian\"):\n",
    "        x[37]=1 \n",
    "    elif(lan == \"Chinese_Hongkong\"):\n",
    "        x[38]=1 \n",
    "    elif(lan == \"Estonian\"):\n",
    "        x[39]=1 \n",
    "    elif(lan == \"Basque\"):\n",
    "        x[40]=1 \n",
    "    elif(lan == \"Esperanto\"):\n",
    "        x[41]=1 \n",
    "    elif(lan == \"Frisian\"):\n",
    "        x[42]=1 \n",
    "    elif(lan == \"Turkish\"):\n",
    "        x[43]=1 \n",
    "    elif(lan == \"Chuvash\"):\n",
    "        x[44]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_input(row):\n",
    "    #print(row)\n",
    "    age = row[\"caption_a\"]\n",
    "    ethnicity=row[\"caption_e\"]\n",
    "    gender=row[\"caption_g\"]\n",
    "    language=row[\"caption_l\"]\n",
    "    speaker_emb=row[\"SPEAKER_EMB\"]\n",
    "    audio_emb=row[\"AUDIO_EMB\"]\n",
    "    audio_features=row[\"AUDIO_FEATURES\"]\n",
    "    image_guide=row[\"image_guide\"]\n",
    "    #print(image_guide)\n",
    "    #print(speaker_emb)\n",
    "    speaker_emb = np.asarray(speaker_emb, dtype=np.float32)\n",
    "    speaker_emb = speaker_emb.squeeze()\n",
    "    audio_emb = np.asarray(audio_emb, dtype=np.float32)\n",
    "    audio_emb = audio_emb.squeeze()\n",
    "    #print(speaker_emb.shape)\n",
    "    h = np.vstack((age, ethnicity))\n",
    "    h = np.vstack((h, gender))\n",
    "    h = np.vstack((h, language))\n",
    "    h = np.vstack((h, speaker_emb))\n",
    "    h = np.vstack((h, audio_emb))\n",
    "    h = np.vstack((h,audio_features))\n",
    "    h = np.vstack((h,image_guide))\n",
    "    #print('aaaaaaaaaaa')\n",
    "    #print(h.shape)\n",
    "    j = np.zeros(768)\n",
    "    j = np.tile(j,(256-h.shape[0],1))\n",
    "    h = np.vstack((h, j))\n",
    "    #print(h.shape)\n",
    "    \n",
    "    return h\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_image_path(path):\n",
    "    \n",
    "    image = Image.open(path).convert('RGB')\n",
    "\n",
    "    w_s = image_size / (1+2 * 0.4)\n",
    "    h_s = image_size / (1+2 * 0.4)\n",
    "\n",
    "    #print(image.size)\n",
    "    image = image.crop((0.2*w_s,0.0*h_s,1.6*w_s,1.4*h_s))\n",
    "    image = image.resize((image_size,image_size))\n",
    "\n",
    "    #print('saving')\n",
    "    #image.save(str(random.random()) + '.png')\n",
    "\n",
    "    \n",
    "\n",
    "    #print(np.array(image,np.float32).shape)\n",
    "    pix = np.array(image,np.float32)\n",
    "    pix = np.moveaxis(pix, -1, 0)\n",
    "    \n",
    "    pix = pix / 255\n",
    "    image.close()\n",
    "    return pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import multiprocessing\n",
    "import train_imagen_batch\n",
    "import dask.dataframe as dd\n",
    "import math\n",
    "\n",
    "procs = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('-------------------- TRAINING IN EPOCH = ' + str(epoch) + ' ---------------------')\n",
    "    stop_now = False\n",
    "\n",
    "\n",
    "    while(not stop_now):\n",
    "\n",
    "        con = sl.connect(datasetPathDatabase)\n",
    "\n",
    "        print(\"Getting Data...\")\n",
    "        #import time\n",
    "        #start_time = time.time()\n",
    "        ## TODO: NEED TO CHECK F.ID CORRESPONDS TO CORRECT FACE_PATH\n",
    "\n",
    "        data = con.execute(\"SELECT V.ID,F.ID,V.VIDEO_PATH, V.AGE CAPTION_A, \" + \n",
    "                            \"V.ETHNICITY CAPTION_E, \" +\n",
    "                            \"lower(V.GENDER) CAPTION_G, \" +\n",
    "                                \"A.SPEAKER_EMB, \"+ (\"A.WAV_TO_VEC, \" if(audio_embs == 'wav2vec') else \"A.AUDIO_EMB2, \" if(audio_embs == 'openl3')  else \"A.PYANNOTE_TITANET, \" if(audio_embs == 'pyannoteTitaNet') else ', ') +\n",
    "                            \"A.AUDIO_FEATURES, \" +\n",
    "                            \"A.LANG CAPTION_L, \"+\n",
    "                            \"F.FACE_PATH \"+\n",
    "                            \"FROM VIDEO V \"+\n",
    "                            \"INNER JOIN AUDIO A ON V.ID = A.VIDEO_ID \" +\n",
    "                            \"INNER JOIN FACE F ON F.ID = (select ID from FACE f2 where f2.video_id = v.ID ORDER By ID limit 1 ) \" + \n",
    "                            \"WHERE AUDIO_LENGTH = \" + audio_length_used + ' ' +\n",
    "                            \"AND V.ID in (select V2.ID from VIDEO v2 WHERE V2.TRAINED IS NULL AND AUDIO_PRE IN (3,4) AND FACES_PRE = 2 ORDER BY ABS(RANDOM()) LIMIT \"+ str(db_chunk) + \")\" )\n",
    "        dataGotten = data.fetchall()\n",
    "\n",
    "        \n",
    "        print(\"Data Gotten\")\n",
    "        for ps in procs:\n",
    "            ps.join()\n",
    "        procs.clear()\n",
    "\n",
    "        if(len(dataGotten) == 0):\n",
    "            stop_now = True\n",
    "\n",
    "        \n",
    "\n",
    "        print(\"Preprocessing Data...\")\n",
    "        df = pd.DataFrame(dataGotten,columns = ['ID','FACE_ID','VIDEO_PATH','caption_a','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','AUDIO_FEATURES','caption_l','image_path'])\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        df = dd.from_pandas(df, npartitions=int(db_chunk/dask_chunk))\n",
    "        df[\"image_guide\"] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        #print(df.head(10))\n",
    "        data_frame = df[[\"ID\",\"FACE_ID\",\"image_path\",\"caption_a\",\"caption_e\",\"caption_g\",\"caption_l\"]]\n",
    "        data_frame['SPEAKER_EMB'] = df['SPEAKER_EMB']\n",
    "        data_frame['AUDIO_EMB'] = df['AUDIO_EMB']\n",
    "        data_frame['image_guide'] = df['image_guide']\n",
    "\n",
    "        \n",
    "        data_frame['SPEAKER_EMB'] = data_frame['SPEAKER_EMB'].apply(lambda x: speaker_emb_preprocess(x),meta=('1', 'object'))\n",
    "        data_frame['AUDIO_EMB'] = data_frame['AUDIO_EMB'].apply(lambda x: audio_emb_preprocess2(x),meta=('1', 'object'))\n",
    "        data_frame['AUDIO_FEATURES'] = data_frame['ID'].apply(lambda x: audio_features_preprocess(x),meta=('1', 'object'))\n",
    "        data_frame = data_frame.drop(['ID'], axis=1)\n",
    "        data_frame['image_guide'] = data_frame['image_path'].apply(lambda x: image_guide_preprocess_low_res_dummy(x),meta=('1', 'str'))\n",
    "        data_frame = data_frame.drop(['FACE_ID'], axis=1)\n",
    "        data_frame['caption_a'] = data_frame['caption_a'].apply(lambda x: process_age(x),meta=('1', 'object'))\n",
    "        data_frame['caption_g'] = data_frame['caption_g'].apply(lambda x: process_gender(x),meta=('1', 'object'))\n",
    "        data_frame['caption_l'] = data_frame['caption_l'].apply(lambda x: process_language(x),meta=('1', 'object'))\n",
    "        data_frame['caption_e'] = data_frame['caption_e'].apply(lambda x: process_ethnicity(x),meta=('1', 'object'))\n",
    "        data_frame['low_res_image'] = data_frame['image_path'].apply(lambda x: image_guide_preprocess_low_res(x),meta=('1', 'object'))\n",
    "\n",
    "        data_frame['INPUT'] = data_frame['SPEAKER_EMB']\n",
    "        #print(data_frame)\n",
    "\n",
    "        data_frame['INPUT'] = data_frame.apply(process_input,args=(),axis=1,meta=('1', 'object'))\n",
    "\n",
    "        #for index, row in data_frame.iterrows():\n",
    "        #    x = process_input(data_frame.loc[index,\"caption_a\"],data_frame.loc[index,\"caption_e\"]\n",
    "        #                        ,data_frame.loc[index,\"caption_g\"],\n",
    "        #                    data_frame.loc[index,\"caption_l\"],data_frame.loc[index,\"SPEAKER_EMB\"],\n",
    "        #                    data_frame.loc[index,\"AUDIO_EMB\"],data_frame.loc[index,\"AUDIO_FEATURES\"],\n",
    "        #                    data_frame.loc[index,\"image_guide\"])\n",
    "        #    x = [x]\n",
    "        #    #AADFS = AADFS\n",
    "        #    data_frame.loc[index,\"INPUT\"] = x\n",
    "\n",
    "        data_frame = data_frame.drop(['caption_e', 'caption_g','caption_l','SPEAKER_EMB','AUDIO_EMB'], axis=1)\n",
    "\n",
    "\n",
    "        data_frame['image_path'] = data_frame['image_path'].apply(lambda x: process_image_path(x),meta=('1', 'str'))\n",
    "        data_frame.compute()\n",
    "\n",
    "        #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        #raise('aa')\n",
    "        print(\"Data Preprocessed...\")\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        parts = range(data_frame.npartitions)\n",
    "\n",
    "        for sub_epoch in range(sub_epochs):\n",
    "\n",
    "            print('Training in sub epoch ' + str(sub_epoch) + ' from ' + str(sub_epochs))\n",
    "\n",
    "            for part_no in parts:\n",
    "\n",
    "                print('Training in part ' + str(part_no))\n",
    "\n",
    "                part = data_frame[['INPUT','image_path','low_res_image']].partitions[part_no]\n",
    "\n",
    "                \n",
    "\n",
    "                input0 = np.array(part['INPUT'])\n",
    "                input0 = np.array([np.array(xi) for xi in input0])\n",
    "                input0[np.isnan(input0)] = 0\n",
    "                input0[input0 > 10] = 10\n",
    "                input0[input0 < -10] = -10\n",
    "                #print(sys.getsizeof(input))\n",
    "                \n",
    "                output = np.array(part['image_path'])\n",
    "                output = np.array([np.array(xi) for xi in output])\n",
    "                output.squeeze().shape\n",
    "                #print(sys.getsizeof(output))\n",
    "\n",
    "                input2 = np.array(part['low_res_image'])\n",
    "                input2 = np.array([np.array(xi) for xi in input2])\n",
    "                input2.squeeze()\n",
    "                #print(sys.getsizeof(input2))\n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "                \n",
    "                #print(len(procs))\n",
    "                #if(len(procs) > 0):\n",
    "                #    procs[0].join() # Wait for previous process to finish\n",
    "                #    print(\"Model trained using a batch of data...\")\n",
    "                #    procs = []\n",
    "                #print(len(procs))\n",
    "\n",
    "\n",
    "                print(\"Training model ...\")\n",
    "                \n",
    "                if(np.shape(input0)[0] > 0):\n",
    "\n",
    "                    if(UNET == 1):\n",
    "                        proc = multiprocessing.Process(target=train_imagen_batch.train_batch_unet1, args=(input0,input2,output,model_filename,inner_epochs,batch_size,sample_every,save_model_every,image_size,unet1_dim,unet2_dim,timesteps,begin_with_image_size,unet1_image_size,imagen_samples_folder,sample_probability,dask_chunk,ignore_image_guide,))\n",
    "                    else:\n",
    "                        proc = multiprocessing.Process(target=train_imagen_batch.train_batch_unet2, args=(input0,input2,output,model_filename,inner_epochs,batch_size,sample_every,save_model_every,image_size,unet1_dim,unet2_dim,timesteps,begin_with_image_size,unet1_image_size,imagen_samples_folder,sample_probability,dask_chunk,ignore_image_guide,))\n",
    "                    procs.append(proc)\n",
    "                    proc.start()\n",
    "\n",
    "\n",
    "                    if(sub_epochs != 1):\n",
    "                        for ps in procs:\n",
    "                            ps.join()\n",
    "                        procs.clear()\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                    #print(\"Model trained using this batch of data...\")\n",
    "\n",
    "                    #proc = multiprocessing.Process(target=train_imagen_batch.train_batch_unet2, args=(input,output,model_filename,sub_epochs,batch_size,sample_every,save_model_every,image_size,unet1_dim,unet2_dim,))\n",
    "                    #proc.start()two_unets_pyannote_nemo\n",
    "                    #proc.join()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                    data.close()\n",
    "\n",
    "                    data5 = np.array(df['ID']).astype('int')\n",
    "                    data6 = []\n",
    "                    for x in data5:\n",
    "                        data6.append(int(x))\n",
    "                    print(len(data6))\n",
    "                    #print(data6)\n",
    "                    sql5 = \"UPDATE VIDEO SET TRAINED = 1 WHERE ID IN ({0})\".format(', '.join('?' for _ in data6))\n",
    "                    #print(sql5)\n",
    "                    cur5 = con.cursor()\n",
    "                    cur5.execute(sql5, data6)\n",
    "                    con.commit()\n",
    "                    cur5.close()\n",
    "                else:\n",
    "                    stop_now == True\n",
    "\n",
    "\n",
    "        data10 = con.execute(\"SELECT COUNT(*) FROM VIDEO v WHERE V.TRAINED = 1\")\n",
    "        dataGotten10 = data10.fetchall()\n",
    "\n",
    "\n",
    "        print('-------------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #print(dataGotten10[0])\n",
    "        if(dataGotten10[0][0] > stop_at_no_of_samples or stop_now == True):\n",
    "            for ps in procs:\n",
    "                ps.join()\n",
    "            procs.clear()\n",
    "            print('--------------- FINISHED ------------------')\n",
    "            stop_now = True\n",
    "            sql50 = \"UPDATE VIDEO SET TRAINED = NULL\"\n",
    "            cur50 = con.cursor()\n",
    "            cur50.execute(sql50)\n",
    "            con.commit()\n",
    "            cur50.close()\n",
    "            print('--------------- TRAINED COLUMN RESET ------------------')\n",
    "        \n",
    "\n",
    "\n",
    "        con.close()\n",
    "        del dataGotten\n",
    "    \n",
    "        del df\n",
    "        del data_frame\n",
    "        del input0\n",
    "        del input2\n",
    "        del output\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor,Compose\n",
    "import dask.dataframe as dd\n",
    "from multiprocessing import Manager,RLock,Value\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    total_rows = None\n",
    "    con = None\n",
    "    lock = None\n",
    "    samples_seen =  Value('i', 0)\n",
    "\n",
    "    def no_of_samples_seen(self):\n",
    "        return self.samples_seen.value\n",
    "\n",
    "    def __init__(self,datasetPathDatabase_a,lock,last_sample_no):\n",
    "        self.lock = lock\n",
    "        self.samples_seen.value = last_sample_no\n",
    "        self.con = sl.connect(datasetPathDatabase_a)\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        if(self.total_rows == None):\n",
    "            self.lock.acquire(block=True)\n",
    "            data = self.con.execute(\"SELECT COUNT(*) FROM VIDEO v \")\n",
    "            count = data.fetchall()\n",
    "            self.total_rows = count[0][0]\n",
    "            data.close()\n",
    "            print(\"Found \" + str(self.total_rows) + \" data points.\")\n",
    "\n",
    "            #data = self.con.execute(\"SELECT COUNT(*) FROM VIDEO v WHERE V.TRAINED = 1\")\n",
    "            #count = data.fetchall()\n",
    "            #data.close()\n",
    "            #self.samples_seen.value = count[0][0]\n",
    "            #print(\"Starting from \" + str(self.samples_seen.value) + \" data points.\")\n",
    "            self.lock.release()\n",
    "\n",
    "        return self.total_rows\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.lock.acquire(block=True)\n",
    "        #print(\"lock acquired on getitem p1\")\n",
    "        sql = (\"SELECT V.ID,F.ID,V.VIDEO_PATH, V.AGE CAPTION_A, \" + \n",
    "                            \"V.ETHNICITY CAPTION_E, \" +\n",
    "                            \"lower(V.GENDER) CAPTION_G, \" +\n",
    "                                \"A.SPEAKER_EMB, \"+ (\"A.WAV_TO_VEC, \" if(audio_embs == 'wav2vec') else \"A.AUDIO_EMB2, \" if(audio_embs == 'openl3')  else \"A.PYANNOTE_TITANET, \" if(audio_embs == 'pyannoteTitaNet') else ', ') +\n",
    "                            \"A.AUDIO_FEATURES, \" +\n",
    "                            \"A.LANG CAPTION_L, \"+\n",
    "                            \"F.FACE_PATH \"+\n",
    "                            \"FROM VIDEO V \"+\n",
    "                            \"INNER JOIN AUDIO A ON V.ID = A.VIDEO_ID \" +\n",
    "                            \"INNER JOIN FACE F ON F.ID = (select ID from FACE f2 where f2.video_id = v.ID ORDER By ID limit 1 ) \" + \n",
    "                            \"WHERE AUDIO_LENGTH = \" + audio_length_used + ' ' +\n",
    "                            \"AND V.ID in (select V2.ID from VIDEO v2 WHERE V2.TRAINED IS NULL AND AUDIO_PRE IN (3,4) AND FACES_PRE = 2 ORDER BY ABS(RANDOM()) LIMIT 1\"+ \")\")\n",
    "        data = self.con.execute(sql)\n",
    "        dataGotten = data.fetchall()\n",
    "        df = pd.DataFrame(dataGotten,columns = ['ID','FACE_ID','VIDEO_PATH','caption_a','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','AUDIO_FEATURES','caption_l','image_path'])\n",
    "        data.close()\n",
    "        \n",
    "        #print(\"aaaa\" + str(len(df)))\n",
    "        if(len(df) == 0):\n",
    "            print(\"Trying to get items\")\n",
    "            data = self.con.execute(sql)\n",
    "            dataGotten = data.fetchall()\n",
    "            df = pd.DataFrame(dataGotten,columns = ['ID','FACE_ID','VIDEO_PATH','caption_a','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','AUDIO_FEATURES','caption_l','image_path'])\n",
    "            data.close()\n",
    "            \n",
    "            if(len(df) == 0):\n",
    "                print(\"Trying to get items\")\n",
    "                data = self.con.execute(sql)\n",
    "                dataGotten = data.fetchall()\n",
    "                df = pd.DataFrame(dataGotten,columns = ['ID','FACE_ID','VIDEO_PATH','caption_a','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','AUDIO_FEATURES','caption_l','image_path'])\n",
    "                data.close()\n",
    "\n",
    "                if(len(df) == 0):\n",
    "                    print(\"Trying to get items\")\n",
    "                    data = self.con.execute(sql)\n",
    "                    dataGotten = data.fetchall()\n",
    "                    df = pd.DataFrame(dataGotten,columns = ['ID','FACE_ID','VIDEO_PATH','caption_a','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','AUDIO_FEATURES','caption_l','image_path'])\n",
    "                    data.close()\n",
    "\n",
    "                    if(len(df) == 0):\n",
    "                        print(\"Reintializing dataset\")\n",
    "                        sql50 = \"UPDATE VIDEO SET TRAINED = NULL\"\n",
    "                        cur50 = self.con.cursor()\n",
    "                        cur50.execute(sql50)\n",
    "                        self.con.commit()\n",
    "                        cur50.close()\n",
    "\n",
    "                        data = self.con.execute(sql)\n",
    "                        dataGotten = data.fetchall()\n",
    "                        df = pd.DataFrame(dataGotten,columns = ['ID','FACE_ID','VIDEO_PATH','caption_a','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','AUDIO_FEATURES','caption_l','image_path'])\n",
    "                        data.close()\n",
    "        #print(\"released first lock\")\n",
    "        self.lock.release()\n",
    "\n",
    "        \n",
    "        \n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        df = dd.from_pandas(df, npartitions=1)\n",
    "        df[\"image_guide\"] = np.nan\n",
    "\n",
    "        #print(df.head(10))\n",
    "        data_frame = df[[\"ID\",\"FACE_ID\",\"image_path\",\"caption_a\",\"caption_e\",\"caption_g\",\"caption_l\"]]\n",
    "        data_frame['SPEAKER_EMB'] = df['SPEAKER_EMB']\n",
    "        data_frame['AUDIO_EMB'] = df['AUDIO_EMB']\n",
    "        data_frame['image_guide'] = df['image_guide']\n",
    "\n",
    "        \n",
    "        data_frame['SPEAKER_EMB'] = data_frame['SPEAKER_EMB'].apply(lambda x: speaker_emb_preprocess(x),meta=('1', 'object'))\n",
    "        data_frame['AUDIO_EMB'] = data_frame['AUDIO_EMB'].apply(lambda x: audio_emb_preprocess2(x),meta=('1', 'object'))\n",
    "        data_frame['AUDIO_FEATURES'] = data_frame['ID'].apply(lambda x: audio_features_preprocess(x),meta=('1', 'object'))\n",
    "        data_frame = data_frame.drop(['ID'], axis=1)\n",
    "        data_frame['image_guide'] = data_frame['image_path'].apply(lambda x: image_guide_preprocess_low_res_dummy(x),meta=('1', 'str'))\n",
    "        data_frame = data_frame.drop(['FACE_ID'], axis=1)\n",
    "        data_frame['caption_a'] = data_frame['caption_a'].apply(lambda x: process_age(x),meta=('1', 'object'))\n",
    "        data_frame['caption_g'] = data_frame['caption_g'].apply(lambda x: process_gender(x),meta=('1', 'object'))\n",
    "        data_frame['caption_l'] = data_frame['caption_l'].apply(lambda x: process_language(x),meta=('1', 'object'))\n",
    "        data_frame['caption_e'] = data_frame['caption_e'].apply(lambda x: process_ethnicity(x),meta=('1', 'object'))\n",
    "        data_frame['low_res_image'] = data_frame['image_path'].apply(lambda x: image_guide_preprocess_low_res(x),meta=('1', 'object'))\n",
    "\n",
    "        data_frame['INPUT'] = data_frame['SPEAKER_EMB']\n",
    "        #print(data_frame)\n",
    "\n",
    "        data_frame['INPUT'] = data_frame.apply(process_input,args=(),axis=1,meta=('1', 'object'))\n",
    "\n",
    "        #for index, row in data_frame.iterrows():\n",
    "        #    x = process_input(data_frame.loc[index,\"caption_a\"],data_frame.loc[index,\"caption_e\"]\n",
    "        #                        ,data_frame.loc[index,\"caption_g\"],\n",
    "        #                    data_frame.loc[index,\"caption_l\"],data_frame.loc[index,\"SPEAKER_EMB\"],\n",
    "        #                    data_frame.loc[index,\"AUDIO_EMB\"],data_frame.loc[index,\"AUDIO_FEATURES\"],\n",
    "        #                    data_frame.loc[index,\"image_guide\"])\n",
    "        #    x = [x]\n",
    "        #    #AADFS = AADFS\n",
    "        #    data_frame.loc[index,\"INPUT\"] = x\n",
    "\n",
    "        data_frame = data_frame.drop(['caption_e', 'caption_g','caption_l','SPEAKER_EMB','AUDIO_EMB'], axis=1)\n",
    "\n",
    "\n",
    "        data_frame['image_path'] = data_frame['image_path'].apply(lambda x: process_image_path(x),meta=('1', 'str'))\n",
    "        data_frame.compute()\n",
    "\n",
    "        part = data_frame[['INPUT','image_path','low_res_image']].partitions[0]\n",
    "\n",
    "                \n",
    "\n",
    "        input0 = np.array(part['INPUT'])\n",
    "        input0 = np.array([np.array(xi) for xi in input0])\n",
    "        input0[np.isnan(input0)] = 0\n",
    "        input0[input0 > 10] = 10\n",
    "        input0[input0 < -10] = -10\n",
    "        #print(sys.getsizeof(input))\n",
    "        \n",
    "        output = np.array(part['image_path'])\n",
    "        output = np.array([np.array(xi) for xi in output])\n",
    "        output.squeeze().shape\n",
    "        #print(sys.getsizeof(output))\n",
    "\n",
    "        input2 = np.array(part['low_res_image'])\n",
    "        input2 = np.array([np.array(xi) for xi in input2])\n",
    "        input2.squeeze()\n",
    "        #print(sys.getsizeof(input2))\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "        #print(len(procs))\n",
    "        #if(len(procs) > 0):\n",
    "        #    procs[0].join() # Wait for previous process to finish\n",
    "        #    print(\"Model trained using a batch of data...\")\n",
    "        #    prProcess,\n",
    "        input0 = torch.from_numpy(input0)\n",
    "        input0 = input0.to(torch.float)\n",
    "\n",
    "\n",
    "        input2 = torch.from_numpy(input2)\n",
    "        input2 = input2.to(torch.float)\n",
    "\n",
    "        output = torch.from_numpy(output)\n",
    "        output = output.to(torch.float)\n",
    "\n",
    "        input0 = input0.squeeze()\n",
    "        input2 = input2.squeeze()\n",
    "        output = output.squeeze()\n",
    "\n",
    "        \n",
    "        self.lock.acquire(block=True)\n",
    "        #print(\"lock aquired on getitem p2\")\n",
    "        data5 = np.array(df['ID']).astype('int')\n",
    "        data6 = []\n",
    "        for x in data5:\n",
    "            data6.append(int(x))\n",
    "        #print(len(data6))\n",
    "        #print(data6)\n",
    "        sql5 = \"UPDATE VIDEO SET TRAINED = 1 WHERE ID IN ({0})\".format(', '.join('?' for _ in data6))\n",
    "        #print(sql5)\n",
    "        cur5 = self.con.cursor()\n",
    "        cur5.execute(sql5, data6)\n",
    "        self.con.commit()\n",
    "        cur5.close()\n",
    "        self.samples_seen.value +=  1\n",
    "        print (self.samples_seen.value,end='\\r')\n",
    "        self.lock.release()\n",
    "        #print(\"lock released on get item p2\")\n",
    "        return output,input0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(sample_no,loss_value):\n",
    "\n",
    "    if(UNET == 1):\n",
    "        my_file = Path(model_filename + '/loss_total_1.picke')\n",
    "        if my_file.is_file():\n",
    "            with open(model_filename + '/loss_total_1.picke', 'rb') as handle:\n",
    "                loss_total = pickle.load(handle)\n",
    "        else:\n",
    "            loss_total = []\n",
    "    else:\n",
    "        my_file = Path(model_filename + '/loss_total_2.picke')\n",
    "        if my_file.is_file():\n",
    "            with open(model_filename + '/loss_total_2.picke', 'rb') as handle:\n",
    "                loss_total = pickle.load(handle)\n",
    "        else:\n",
    "            loss_total = []\n",
    "    \n",
    "    #print(loss_list)\n",
    "    #print(loss_total)\n",
    "    loss_total.append([sample_no,loss_value])\n",
    "\n",
    "    if(UNET == 1):\n",
    "        with open(model_filename +'/loss_total_1.picke', 'wb') as handle:\n",
    "            pickle.dump(loss_total, handle)\n",
    "    else:\n",
    "        with open(model_filename +'/loss_total_2.picke', 'wb') as handle:\n",
    "            pickle.dump(loss_total, handle)\n",
    "    #print(loss_total)\n",
    "    fig = plt.figure()\n",
    "    x_val = [x[0] for x in loss_total]\n",
    "    y_val = [x[1] for x in loss_total]\n",
    "    plt.plot(x_val,y_val)\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Training Sample\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    if(UNET == 1):\n",
    "        if(os.path.isfile(model_filename + '/loss_1_plot.png')):\n",
    "            os.remove(model_filename + '/loss_1_plot.png')\n",
    "        fig.savefig(model_filename + '/loss_1_plot.png')\n",
    "    else:\n",
    "        if(os.path.isfile(model_filename + '/loss_2_plot.png')):\n",
    "            os.remove(model_filename + '/loss_2_plot.png')\n",
    "        fig.savefig(model_filename + '/loss_2_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    try:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(x_val[1000::],np.arange(len(y_val[1000::])) + 1000,y_val[1000::]\n",
    "                ,'.',zorder=-100)\n",
    "\n",
    "        #plt.axvline(x=1000,linestyle='--',color='green',label='1000 inner epochs')\n",
    "        #plt.axvline(x=17000,linestyle='--',color='purple',label='100 inner epochs - 100000 unique samples seen' )\n",
    "        #plt.axvline(x=23100,linestyle='-.',color='black',label='1 inner epoch - end of 1st epoch ')\n",
    "        plt.legend(bbox_to_anchor = (1.0, 1), loc = 'upper right')\n",
    "        plt.grid()\n",
    "        yhat = savgol_filter(y_val, 1000, 3)\n",
    "        plt.plot(x_val,np.arange(len(loss_total[1000::])) + 1000,yhat[1000::],'r')\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.xlabel(\"Training Sample\")\n",
    "        plt.ylabel(\"MSE Loss\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        if(UNET == 1):\n",
    "            if(os.path.isfile(model_filename + '/loss_zoomed_1_plot.png')):\n",
    "                os.remove(model_filename + '/loss_zoomed_1_plot.png')\n",
    "            fig.savefig(model_filename + '/loss_zoomed_1_plot.png')\n",
    "        else:\n",
    "            if(os.path.isfile(model_filename + '/loss_zoomed_2_plot.png')):\n",
    "                os.remove(model_filename + '/loss_zoomed_2_plot.png')\n",
    "            fig.savefig(model_filename + '/loss_zoomed_2_plot.png')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        #fig = plt.figure()\n",
    "        #smoothed = yhat[1000::]\n",
    "        #smoothed = savgol_filter(smoothed, 500, 1)\n",
    "        #price_series = pd.Series(smoothed)\n",
    "        #price_series = price_series.pct_change().to_numpy()\n",
    "        #plt.plot(np.arange(len(y_val[1000::])) + 1000,price_series,zorder=-100)\n",
    "        #plt.axhline(linestyle='--',color='red')\n",
    "        #plt.axvline(x=1000,linestyle='--',color='green',label='1000 inner epochs')\n",
    "        #plt.axvline(x=17000,linestyle='--',color='purple',label='100 inner epochs - 100000 unique samples seen' )\n",
    "        #plt.axvline(x=23100,linestyle='-.',color='black',label='1 inner epoch - end of 1st epoch ')\n",
    "        #plt.legend(loc = 'lower center')\n",
    "        #plt.grid()\n",
    "        #plt.title(\"Smoothed Rate of Change of Training Loss\")\n",
    "        #plt.xlabel(\"Training Sample (~x\" + str(int(dask_chunk)) + \")\")\n",
    "        #plt.ylabel(\"Rate of Change\")\n",
    "        #plt.show()\n",
    "\n",
    "        #if(UNET == 1):\n",
    "        #    if(os.path.isfile(model_filename + 'loss_roc_1_plot.png')):\n",
    "        #        os.remove(model_filename + 'loss_roc_1_plot.png')\n",
    "        #    fig.savefig(model_filename + 'loss_roc_1_plot.png')\n",
    "        #else:\n",
    "        #    if(os.path.isfile(model_filename + 'loss_roc_2_plot.png')):\n",
    "        #        os.remove(model_filename + 'loss_roc_2_plot.png')\n",
    "        #    fig.savefig(model_filename + 'loss_roc_2_plot.png')\n",
    "        #plt.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_sample_no = 0\n",
    "my_file = Path(model_filename + '/last_sample_no.picke')\n",
    "if my_file.is_file():\n",
    "    with open(model_filename + '/last_sample_no.picke', 'rb') as handle:\n",
    "        last_sample_no = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1092009 data points.\n",
      "1092009\n"
     ]
    }
   ],
   "source": [
    "my_dataset = CustomDataset(datasetPathDatabase,RLock(),last_sample_no)\n",
    "print(len(my_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataloader = DataLoader(my_dataset,batch_size = batch_size * 16,shuffle=False,num_workers=16) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no checkpoints found to load from at /media/gamal/Passport/checkpoint/noablation\n",
      "211\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.7698674872517586\n",
      "epoch:1 from 100000\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: '/media/gamal/Passport/checkpoint/noablation\\\\last_sample_no.picke'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/gamal/vsc/DiffusionSpeech2Face/train_imagen_all_u1.ipynb Cell 13\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gamal/vsc/DiffusionSpeech2Face/train_imagen_all_u1.ipynb#X20sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mepoch:\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m from \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(epochs))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gamal/vsc/DiffusionSpeech2Face/train_imagen_all_u1.ipynb#X20sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m last_sample_no \u001b[39m=\u001b[39m my_dataset\u001b[39m.\u001b[39mno_of_samples_seen()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gamal/vsc/DiffusionSpeech2Face/train_imagen_all_u1.ipynb#X20sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(model_filename \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mlast_sample_no.picke\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m handle:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gamal/vsc/DiffusionSpeech2Face/train_imagen_all_u1.ipynb#X20sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(last_sample_no, handle)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument: '/media/gamal/Passport/checkpoint/noablation\\\\last_sample_no.picke'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\r"
     ]
    }
   ],
   "source": [
    "from imagen_pytorch import Unet, Imagen, ImagenTrainer,NullUnet\n",
    "from imagen_pytorch.data import Dataset\n",
    "\n",
    "unet0 = NullUnet()  # add a placeholder \"null\" unet for the base unet\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = unet1_dim,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = 3,\n",
    "    layer_attns = (False, True, True, True),\n",
    "    layer_cross_attns = (False, True, True, True)\n",
    ")\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = unet2_dim,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = (2, 4, 8, 8),\n",
    "    layer_attns = (False, False, False, True),\n",
    "    layer_cross_attns = (False, False, False, True)\n",
    ")\n",
    "\n",
    "if(ignore_image_guide):\n",
    "    unets = (unet1, unet2)\n",
    "    image_sizes= (unet1_image_size, image_size)\n",
    "    unet_to_train = UNET #1\n",
    "    #start_image_or_video = None\n",
    "else:\n",
    "    unets = (unet0,unet1, unet2)\n",
    "    image_sizes= (begin_with_image_size,unet1_image_size, image_size)\n",
    "    unet_to_train = UNET + 1 #2\n",
    "    #start_image_or_video = input2[:1,:]\n",
    "\n",
    "#print(input2[:1,:])\n",
    "#print(input2)\n",
    "\n",
    "imagen = Imagen(\n",
    "    unets = unets,\n",
    "    image_sizes = image_sizes,\n",
    "    timesteps = timesteps,\n",
    "    cond_drop_prob = 0.1\n",
    ").cuda()\n",
    "\n",
    "\n",
    "\n",
    "trainer = ImagenTrainer(\n",
    "    imagen = imagen,\n",
    "    split_valid_from_train = False, # whether to split the validation dataset from the training\n",
    "    checkpoint_every = 250,\n",
    "    checkpoint_path = model_filename,\n",
    "    max_checkpoints_keep = 2,\n",
    "    only_train_unet_number = unet_to_train\n",
    ").cuda()\n",
    "\n",
    "\n",
    "trainer.add_train_dataloader(my_dataloader)\n",
    "\n",
    "#if not os.path.exists(imagen_samples_folder):\n",
    "#    os.makedirs(imagen_samples_folder)\n",
    "\n",
    "#my_file = Path(model_filename)\n",
    "#if my_file.is_file():\n",
    "#    print('Using model file ' + model_filename)\n",
    "#    trainer.load(model_filename)\n",
    "\n",
    "#trainer.load_from_checkpoint_folder()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    loss = trainer.train_step(unet_number = unet_to_train,max_batch_size = batch_size)\n",
    "    print(f'loss: {loss}')\n",
    "    #print(\"At sample seen \" + str(my_dataset.no_of_samples_seen()))\n",
    "    plot_loss(my_dataset.no_of_samples_seen(),loss)\n",
    "    #print(loss_list)\n",
    "    #trainer.save_to_checkpoint_folder()\n",
    "    print('epoch:' + str(i+1) + ' from ' + str(epochs))\n",
    "    last_sample_no = my_dataset.no_of_samples_seen()\n",
    "    with open(model_filename + '/last_sample_no.picke', 'wb') as handle:\n",
    "        pickle.dump(last_sample_no, handle)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78dccc615e96ab04385280185a87a524fe0822daf5dbad2f4bf2e7d7b28366a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
