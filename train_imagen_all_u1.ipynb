{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "# Loading configurations\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "\n",
    "insert_amd_env_vars =  int(configParser.get('COMMON', 'insert_amd_env_vars'))\n",
    "HSA_OVERRIDE_GFX_VERSION =  configParser.get('COMMON', 'HSA_OVERRIDE_GFX_VERSION')\n",
    "ROCM_PATH =  configParser.get('COMMON', 'ROCM_PATH')\n",
    "\n",
    "if(insert_amd_env_vars != 0):\n",
    "    os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = HSA_OVERRIDE_GFX_VERSION\n",
    "    os.environ[\"ROCM_PATH\"] = ROCM_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#TODO document jupyter\n",
    "import pickle\n",
    "import configparser\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "datasetPathDatabase =  configParser.get('COMMON', 'datasetPathDatabase') + '/dataset.db'\n",
    "datasetPathDatabaseAdditional =  configParser.get('COMMON', 'datasetPathDatabase') + '/datasetAdditional.db'\n",
    "datasetPathDatabaseVgg =  configParser.get('COMMON', 'datasetPathDatabase') + '/datasetFacesBlurred.db'\n",
    "image_size =  int(configParser.get('COMMON', 'resizeImageTo'))\n",
    "unet_dim =  int(configParser.get('COMMON', 'unet_dim'))\n",
    "unet1_image_size =  int(configParser.get('COMMON', 'unet1_image_size'))\n",
    "audio_embs =  str(configParser.get('COMMON', 'audio_embs'))\n",
    "audio_length_used =  configParser.get('train_imagen', 'audio_length_used') \n",
    "model_filename =  configParser.get('train_imagen', 'model_filename') + '_'  + audio_length_used +  's.pt'\n",
    "sub_epochs=  int(configParser.get('train_imagen', 'sub_epochs') )\n",
    "batch_size=  int(configParser.get('train_imagen', 'batch_size') )\n",
    "sample_every=  int(configParser.get('train_imagen', 'sample_every') ) - 1\n",
    "save_model_every=  int(configParser.get('train_imagen', 'save_model_every') ) - 1\n",
    "epochs=  int(configParser.get('train_imagen', 'epochs') )\n",
    "sample_every_offset=  int(configParser.get('train_imagen', 'sample_every_offset') ) - 1\n",
    "save_every_offset=  int(configParser.get('train_imagen', 'save_every_offset') ) - 1\n",
    "\n",
    "continue_from_epoch = int(configParser.get('train_imagen', 'continue_from_epoch'))\n",
    "continue_from_offset = int(configParser.get('train_imagen', 'continue_from_offset'))\n",
    "continue_from_epoch_and_offset_flag = int(configParser.get('train_imagen', 'continue_from_epoch_and_offset_flag'))\n",
    "db_chunk = int(configParser.get('train_imagen', 'db_chunk'))\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speaker_emb_preprocess(speaker_emb2):\n",
    "\n",
    "    speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "    speaker_emb2 = speaker_emb2.squeeze()\n",
    "    speaker_emb2 = np.pad(speaker_emb2, (288), 'constant', constant_values=(0))\n",
    "    speaker_emb2 = np.tile(speaker_emb2, (1, 1))\n",
    "    speaker_emb2 = speaker_emb2 / 200.0\n",
    "    speaker_emb2 = np.array(speaker_emb2).tolist()\n",
    "    return speaker_emb2\n",
    "\n",
    "\n",
    "def audio_emb_preprocess2(speaker_emb2):\n",
    "    speaker_emb2 = pickle.loads(speaker_emb2)\n",
    "    a = speaker_emb2 #np.zeros(shape=(24, 512))\n",
    "    a = a[-28:,:] # Actually only last 39 are the relevant ones\n",
    "    #print('au')\n",
    "    #print(a.shape)\n",
    "    b = np.zeros(shape=(a.shape[0], 768-a.shape[1]))\n",
    "    #c = np.zeros(shape=(28-a.shape[0], 768))\n",
    "    arr = np.concatenate((a, b), axis=1)\n",
    "    #arr = np.concatenate((arr, c), axis=0)\n",
    "    arr = arr / (1 if(audio_embs == 'wav2vec') else 10 if(audio_embs == 'openl3')  else 1 if(audio_embs == 'pyannoteTitaNet') else 1)\n",
    "    #print(str(arr.max()))\n",
    "    speaker_emb2 = np.array(arr).tolist()\n",
    "    return speaker_emb2\n",
    "\n",
    "def audio_features_preprocess(video_id):\n",
    "    #print(video_id)\n",
    "    conAdditional = sl.connect(datasetPathDatabaseAdditional)\n",
    "    cur = conAdditional.cursor()\n",
    "    sql = '''SELECT AUDIO_FEATURES FROM AUDIO WHERE VIDEO_ID = ?'''\n",
    "    cur.execute(sql, [video_id])\n",
    "    audio_features = cur.fetchall()\n",
    "    #print(audio_features[0])\n",
    "    audio_features = pickle.loads(audio_features[0][0])\n",
    "    #print(audio_features.shape) # 190 x 128\n",
    "    audio_features = audio_features[0:172]\n",
    "    z1 = np.zeros((172,768-128))\n",
    "    audio_features = np.hstack((audio_features,z1))\n",
    "    audio_features = audio_features / 100.0\n",
    "    return audio_features\n",
    "\n",
    "def audio_transformer_features_preprocess(video_id):\n",
    "    #print(video_id)\n",
    "    conAdditional = sl.connect(datasetPathDatabaseAdditional)\n",
    "    cur = conAdditional.cursor()\n",
    "    sql = '''SELECT AUDIO_FEATURES FROM AUDIO_TRANSFORMER WHERE VIDEO_ID = ?'''\n",
    "    cur.execute(sql, [video_id])\n",
    "    audio_features = cur.fetchall()\n",
    "    #print(audio_features[0])\n",
    "    audio_features = pickle.loads(audio_features[0][0])\n",
    "    #print(audio_features.shape) # 514 x 768\n",
    "    audio_features = audio_features.squeeze()\n",
    "    audio_features = audio_features[0::3]#172*768\n",
    "    #z1 = np.zeros((161,768-128))\n",
    "    #audio_features = np.hstack((audio_features,z1))\n",
    "    #audio_features = audio_features / 100.0\n",
    "    return audio_features\n",
    "\n",
    "import random\n",
    "def image_guide_preprocess(face_id):\n",
    "    #print(face_id)\n",
    "\n",
    "    if(random.random() > 2):\n",
    "        image_guide = np.zeros((49,768))\n",
    "        #print(image_guide.shape)\n",
    "    else:\n",
    "        conAdditional = sl.connect(datasetPathDatabaseVgg)\n",
    "        cur = conAdditional.cursor()\n",
    "        sql = '''SELECT BLURRED_FACE_EMB FROM FACES_BLURRED WHERE FACE_ID = ?'''\n",
    "        cur.execute(sql, [face_id])\n",
    "        image_guide = cur.fetchall()\n",
    "        #print(image_guide)\n",
    "        image_guide = pickle.loads(image_guide[0][0])\n",
    "        image_guide = image_guide.squeeze() #197 x 768\n",
    "        image_guide = image_guide[1::4] #49 x 768\n",
    "    return image_guide\n",
    "\n",
    "\n",
    "boxBlurMin =  int(configParser.get('extractVggBlurred', 'boxBlurMin'))\n",
    "boxBlurMax =  int(configParser.get('extractVggBlurred', 'boxBlurMax'))\n",
    "\n",
    "gaussianBlurMin =  int(configParser.get('extractVggBlurred', 'gaussianBlurMin'))\n",
    "gaussianBlurMax =  int(configParser.get('extractVggBlurred', 'gaussianBlurMax'))\n",
    "\n",
    "\n",
    "from PIL import Image,ImageFilter\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "  \n",
    "\n",
    "def make_square(im, min_size=image_size, fill_color=(0, 0, 0, 0)):\n",
    "    x, y = im.size\n",
    "    size = max(min_size, x, y)\n",
    "    new_im = Image.new('RGB', (size, size), fill_color)\n",
    "    new_im.paste(im, (int((size - x) / 2), int((size - y) / 2)))\n",
    "    im.close()\n",
    "    return new_im\n",
    "\n",
    "def image_guide_preprocess_histogram(path):\n",
    "    #print(face_id)\n",
    "\n",
    "    if(random.random() > 2):\n",
    "        image_guide = np.zeros((49,768))\n",
    "        #print(image_guide.shape)\n",
    "    else:\n",
    "\n",
    "\n",
    "        image = Image.open(path).convert('RGB')\n",
    "\n",
    "        w_s = image_size / (1+2 * 0.5)\n",
    "        h_s = image_size / (1+2 * 0.4)\n",
    "\n",
    "        image = image.crop((0.4*w_s,0.0*h_s,1.6*w_s,1.4*h_s))\n",
    "\n",
    "        image = make_square(image)\n",
    "\n",
    "\n",
    "        image= image.crop((1, 1, 127, 127))\n",
    "        im = image\n",
    "\n",
    "        blurOption = random.randint(1, 3)\n",
    "\n",
    "        if(blurOption==1):\n",
    "            imBlurred = im.filter(ImageFilter.BoxBlur(random.randint(boxBlurMin, boxBlurMax))) # 4 to 14\n",
    "        elif(blurOption==2):\n",
    "            imBlurred = im.filter(ImageFilter.GaussianBlur(random.randint(gaussianBlurMin, gaussianBlurMax))) # 2 to 6\n",
    "        elif(blurOption==3):\n",
    "            imBlurred = im.filter(ImageFilter.BoxBlur(random.randint(int(boxBlurMin/2),int(boxBlurMax/2)))).filter(\n",
    "                ImageFilter.GaussianBlur(random.randint(int(gaussianBlurMin/2),int(gaussianBlurMax/2)))) \n",
    "\n",
    "\n",
    "        M = 18\n",
    "        N = 18\n",
    "        im=np.array(im)\n",
    "        tiles = [im[x:x+M,y:y+N] for x in range(0,im.shape[0],M) for y in range(0,im.shape[1],N)]\n",
    "        pics = np.asarray(tiles)\n",
    "\n",
    "        for i in range(0,pics.shape[0]+1):\n",
    "            # Calculate histogram without mask\n",
    "            img = pics[i]\n",
    "            hist1 = cv2.calcHist([img],[0],None,[256],[0,256])\n",
    "            hist2 = cv2.calcHist([img],[1],None,[256],[0,256])\n",
    "            hist3 = cv2.calcHist([img],[2],None,[256],[0,256])\n",
    "\n",
    "            #RGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            plt.subplot(221), plt.imshow(img)\n",
    "            plt.subplot(222), plt.plot(hist1), plt.plot(hist2),plt.plot(hist3)\n",
    "            plt.xlim([0,256])\n",
    "        \n",
    "        #print('saving')\n",
    "        #image.save(str(random.random()) + '.png')\n",
    "\n",
    "        \n",
    "\n",
    "        #print(np.array(image,np.float32).shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        pix = np.array(image,np.float32)\n",
    "        pix = np.moveaxis(pix, -1, 0)\n",
    "        \n",
    "        pix = pix / 255\n",
    "        image.close()\n",
    "        im.close()\n",
    "        return pix.tolist()\n",
    "\n",
    "\n",
    "        conAdditional = sl.connect(datasetPathDatabaseVgg)\n",
    "        cur = conAdditional.cursor()\n",
    "        sql = '''SELECT BLURRED_FACE_EMB FROM FACES_BLURRED WHERE FACE_ID = ?'''\n",
    "        cur.execute(sql, [face_id])\n",
    "        image_guide = cur.fetchall()\n",
    "        #print(image_guide)\n",
    "        image_guide = pickle.loads(image_guide[0][0])\n",
    "        image_guide = image_guide.squeeze() #197 x 768\n",
    "        image_guide = image_guide[1::4] #49 x 768\n",
    "    return image_guide\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def process_age(age):\n",
    "    if(random.random() < 0.2):\n",
    "        x = np.zeros(768)\n",
    "        x[767] = 1\n",
    "        return x\n",
    "    else:\n",
    "        x = np.ones(768) * (age / 100.0)\n",
    "        x[767] = 0\n",
    "        return x\n",
    "\n",
    "def process_gender(gender):\n",
    "    if(random.random() < 0.2):\n",
    "        return np.zeros(768)\n",
    "    elif(gender == 'man'):\n",
    "        return np.ones(768)\n",
    "    elif(gender == \"woman\"):\n",
    "        return np.ones(768) * -1\n",
    "    else:\n",
    "        return np.zeros(768)\n",
    "    \n",
    "# TODO\n",
    "def process_ethnicity(eth):\n",
    "    x = np.zeros(768)\n",
    "    if(random.random() < 0.2):\n",
    "        x = x\n",
    "    elif(eth == \"indian\"):\n",
    "        x[0] = 1\n",
    "    elif(eth == \"asian\"):\n",
    "        x[16]=1\n",
    "    elif(eth == \"latino hispanic\"):\n",
    "        x[2]=1\n",
    "    elif(eth == \"black\"):\n",
    "        x[3]=1\n",
    "    elif(eth == \"middle eastern\"):\n",
    "        x[4]=1\n",
    "    elif(eth == \"white\"):\n",
    "        x[5]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x     \n",
    "\n",
    "def process_language(lan):\n",
    "    x = np.zeros(768)\n",
    "    if(random.random() < 0.2):\n",
    "        x = x\n",
    "    elif(lan == \"Arabic\"):\n",
    "        x[0] = 1\n",
    "    elif(lan == \"Portuguese\"):\n",
    "        x[16]=1\n",
    "    elif(lan == \"Romansh_Sursilvan\"):\n",
    "        x[2]=1\n",
    "    elif(lan == \"Japanese\"):\n",
    "        x[3]=1\n",
    "    elif(lan == \"Ukranian\"):\n",
    "        x[4]=1\n",
    "    elif(lan == \"German\"):\n",
    "        x[5]=1   \n",
    "    elif(lan == \"Chinese_China\"):\n",
    "        x[6]=1   \n",
    "    elif(lan == \"Welsh\"):\n",
    "        x[7]=1  \n",
    "    elif(lan == \"English\"):\n",
    "        x[8]=1\n",
    "    elif(lan == \"Kabyle\"):\n",
    "        x[9]=1 \n",
    "    elif(lan == \"Kyrgyz\"):\n",
    "        x[10]=1\n",
    "    elif(lan == \"Georgian\"):\n",
    "        x[11]=1\n",
    "    elif(lan == \"Persian\"):\n",
    "        x[12]=1 \n",
    "    elif(lan == \"French\"):\n",
    "        x[13]=1\n",
    "    elif(lan == \"Interlingua\"):\n",
    "        x[14]=1\n",
    "    elif(lan == \"Swedish\"):\n",
    "        x[15]=1\n",
    "    elif(lan == \"Spanish\"):\n",
    "        x[16]=1 \n",
    "    elif(lan == \"Dhivehi\"):\n",
    "        x[17]=1\n",
    "    elif(lan == \"Kinyarwanda\"):\n",
    "        x[18]=1 \n",
    "    elif(lan == \"Tatar\"):\n",
    "        x[19]=1\n",
    "    elif(lan == \"Hakha_Chin\"):\n",
    "        x[20]=1 \n",
    "    elif(lan == \"Tamil\"):\n",
    "        x[21]=1 \n",
    "    elif(lan == \"Greek\"):\n",
    "        x[22]=1\n",
    "    elif(lan == \"Latvian\"):\n",
    "        x[23]=1 \n",
    "    elif(lan == \"Russian\"):\n",
    "        x[24]=1\n",
    "    elif(lan == \"Breton\"):\n",
    "        x[25]=1\n",
    "    elif(lan == \"Catalan\"):\n",
    "        x[26]=1    \n",
    "    elif(lan == \"Maltese\"):\n",
    "        x[27]=1 \n",
    "    elif(lan == \"Slovenian\"):\n",
    "        x[28]=1    \n",
    "    elif(lan == \"Indonesian\"):\n",
    "        x[29]=1    \n",
    "    elif(lan == \"Dutch\"):\n",
    "        x[30]=1\n",
    "    elif(lan == \"Chinese_Taiwan\"):\n",
    "        x[31]=1 \n",
    "    elif(lan == \"Sakha\"):\n",
    "        x[32]=1 \n",
    "    elif(lan == \"Polish\"):\n",
    "        x[33]=1 \n",
    "    elif(lan == \"Czech\"):\n",
    "        x[34]=1 \n",
    "    elif(lan == \"Romanian\"):\n",
    "        x[35]=1 \n",
    "    elif(lan == \"Mangolian\"):\n",
    "        x[36]=1 \n",
    "    elif(lan == \"Italian\"):\n",
    "        x[37]=1 \n",
    "    elif(lan == \"Chinese_Hongkong\"):\n",
    "        x[38]=1 \n",
    "    elif(lan == \"Estonian\"):\n",
    "        x[39]=1 \n",
    "    elif(lan == \"Basque\"):\n",
    "        x[40]=1 \n",
    "    elif(lan == \"Esperanto\"):\n",
    "        x[41]=1 \n",
    "    elif(lan == \"Frisian\"):\n",
    "        x[42]=1 \n",
    "    elif(lan == \"Turkish\"):\n",
    "        x[43]=1 \n",
    "    elif(lan == \"Chuvash\"):\n",
    "        x[44]=1 \n",
    "    else:\n",
    "        x = x\n",
    "    return x       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_input(age,ethnicity,gender,language,speaker_emb,audio_emb,audio_features,image_guide):\n",
    "    speaker_emb = np.asarray(speaker_emb, dtype=np.float32)\n",
    "    speaker_emb = speaker_emb.squeeze()\n",
    "    audio_emb = np.asarray(audio_emb, dtype=np.float32)\n",
    "    audio_emb = audio_emb.squeeze()\n",
    "    #print(speaker_emb.shape)\n",
    "    h = np.vstack((age, ethnicity))\n",
    "    h = np.vstack((h, gender))\n",
    "    h = np.vstack((h, language))\n",
    "    h = np.vstack((h, speaker_emb))\n",
    "    h = np.vstack((h, audio_emb))\n",
    "    h = np.vstack((h,audio_features))\n",
    "    h = np.vstack((h,image_guide))\n",
    "    #print('aaaaaaaaaaa')\n",
    "    #print(h.shape)\n",
    "    j = np.zeros(768)\n",
    "    j = np.tile(j,(256-h.shape[0],1))\n",
    "    h = np.vstack((h, j))\n",
    "    #print(h.shape)\n",
    "    return h.tolist()\n",
    "\n",
    "\n",
    "\n",
    "def process_image_path(path):\n",
    "    \n",
    "    image = Image.open(path).convert('RGB')\n",
    "\n",
    "    w_s = image_size / (1+2 * 0.5)\n",
    "    h_s = image_size / (1+2 * 0.4)\n",
    "\n",
    "    image = image.crop((0.4*w_s,0.0*h_s,1.6*w_s,1.4*h_s))\n",
    "\n",
    "    image = make_square(image)\n",
    "    #print('saving')\n",
    "    #image.save(str(random.random()) + '.png')\n",
    "\n",
    "    \n",
    "\n",
    "    #print(np.array(image,np.float32).shape)\n",
    "    pix = np.array(image,np.float32)\n",
    "    pix = np.moveaxis(pix, -1, 0)\n",
    "    \n",
    "    pix = pix / 255\n",
    "    image.close()\n",
    "    return pix.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in epoch 1\n",
      "Training in offset 0\n",
      "Getting Data...\n",
      "Data Gotten\n",
      "Preprocessing Data...\n",
      "Data Preprocessed...\n",
      "Training model using another batch of data...\n",
      "Training Unet No. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2023-09-19 16:08:13.576529: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-19 16:08:13.790814: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 427 samples and validating with randomly splitted 11 samples\n",
      "valid loss: 0.7085637860000134\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600acb62ebcd47a48bd27316c70f6c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3dac2a712640b6a3ecb8d908c74f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to imagen_live128_24s.pt\n",
      "valid loss: 0.06832483224570751\n",
      "checkpoint saved to imagen_live128_24s.pt\n",
      "Training in offset 500\n",
      "Getting Data...\n",
      "Data Gotten\n",
      "Preprocessing Data...\n",
      "Data Preprocessed...\n",
      "Training model using another batch of data...\n",
      "Training Unet No. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2023-09-19 16:12:52.712387: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-19 16:12:52.783130: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 425 samples and validating with randomly splitted 11 samples\n",
      "Using model file imagen_live128_24s.pt\n",
      "checkpoint loaded from imagen_live128_24s.pt\n",
      "valid loss: 0.03969157510437071\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94d0feaaae34313a5c7ef6a9eb6b059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89ccf44bca94da6b8b4b66118b5f068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to imagen_live128_24s.pt\n",
      "valid loss: 0.02645968785509467\n",
      "checkpoint saved to imagen_live128_24s.pt\n",
      "Training in offset 1000\n",
      "Getting Data...\n",
      "Data Gotten\n",
      "Preprocessing Data...\n",
      "Data Preprocessed...\n",
      "Training model using another batch of data...\n",
      "Training Unet No. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2023-09-19 16:17:52.893311: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-19 16:17:52.960967: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 428 samples and validating with randomly splitted 11 samples\n",
      "Using model file imagen_live128_24s.pt\n",
      "checkpoint loaded from imagen_live128_24s.pt\n",
      "valid loss: 0.0220615261932835\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf3d1f01fd94825bb0f85d44879d3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c20000d83af4e5d930c98d6d832e123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to imagen_live128_24s.pt\n",
      "valid loss: 0.022718440159223974\n",
      "checkpoint saved to imagen_live128_24s.pt\n",
      "Training in offset 1500\n",
      "Getting Data...\n",
      "Data Gotten\n",
      "Preprocessing Data...\n",
      "Data Preprocessed...\n",
      "Training model using another batch of data...\n",
      "Training Unet No. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2023-09-19 16:22:37.284258: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-19 16:22:37.353736: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 220 samples and validating with randomly splitted 6 samples\n",
      "Using model file imagen_live128_24s.pt\n",
      "checkpoint loaded from imagen_live128_24s.pt\n",
      "valid loss: 0.01734262309037149\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30849a08ed414dcdbb6f8893865dcd3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efb3d4c829447ab896ce157069d72f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to imagen_live128_24s.pt\n",
      "valid loss: 0.02397124469280243\n",
      "checkpoint saved to imagen_live128_24s.pt\n",
      "Training in offset 2000\n",
      "Getting Data...\n",
      "Data Gotten\n",
      "Preprocessing Data...\n",
      "Data Preprocessed...\n",
      "Training model using another batch of data...\n",
      "Training Unet No. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2023-09-19 16:27:28.485240: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-19 16:27:28.579632: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 428 samples and validating with randomly splitted 11 samples\n",
      "Using model file imagen_live128_24s.pt\n",
      "checkpoint loaded from imagen_live128_24s.pt\n",
      "valid loss: 0.016525695857126266\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f71787aca94cbabfeb3f37e2f70e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51d8150bf1e4b6aaf4678197181a656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to imagen_live128_24s.pt\n",
      "valid loss: 0.012658487423323095\n",
      "checkpoint saved to imagen_live128_24s.pt\n",
      "Training in offset 2500\n",
      "Getting Data...\n",
      "Data Gotten\n",
      "Preprocessing Data...\n",
      "Data Preprocessed...\n",
      "Training model using another batch of data...\n",
      "Training Unet No. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2023-09-19 16:33:02.007760: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-19 16:33:02.213935: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 425 samples and validating with randomly splitted 11 samples\n",
      "Using model file imagen_live128_24s.pt\n",
      "checkpoint loaded from imagen_live128_24s.pt\n",
      "valid loss: 0.017484284238889813\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d744338f5604456af601cec9ec29173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36edb54baebc4550b59c0814270574e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to imagen_live128_24s.pt\n",
      "valid loss: 0.013557759928517044\n",
      "checkpoint saved to imagen_live128_24s.pt\n",
      "Training in offset 3000\n",
      "Getting Data...\n",
      "Data Gotten\n",
      "Preprocessing Data...\n",
      "Data Preprocessed...\n",
      "Training model using another batch of data...\n",
      "Training Unet No. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2023-09-19 16:37:59.761450: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-19 16:37:59.941013: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 428 samples and validating with randomly splitted 11 samples\n",
      "Using model file imagen_live128_24s.pt\n",
      "checkpoint loaded from imagen_live128_24s.pt\n",
      "valid loss: 0.015541567699983716\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5834a4faf714e14a8f943bbee027398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd8a37737a04387b61b2d7818f3a2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to imagen_live128_24s.pt\n",
      "valid loss: 0.015125842299312353\n",
      "checkpoint saved to imagen_live128_24s.pt\n",
      "Training in offset 3500\n",
      "Getting Data...\n",
      "Data Gotten\n",
      "Preprocessing Data...\n",
      "Data Preprocessed...\n",
      "Training model using another batch of data...\n",
      "Training Unet No. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2023-09-19 16:41:53.829737: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-19 16:41:54.031261: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 220 samples and validating with randomly splitted 6 samples\n",
      "Using model file imagen_live128_24s.pt\n",
      "checkpoint loaded from imagen_live128_24s.pt\n",
      "valid loss: 0.017779161222279072\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee05da39949245358b497c365f90199a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0907efa9ffc14c77b0d741308aed39c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to imagen_live128_24s.pt\n",
      "valid loss: 0.009965404286049306\n",
      "checkpoint saved to imagen_live128_24s.pt\n",
      "Training in offset 4000\n",
      "Getting Data...\n",
      "Data Gotten\n",
      "Preprocessing Data...\n",
      "Data Preprocessed...\n",
      "Training model using another batch of data...\n",
      "Training Unet No. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2023-09-19 16:46:18.547520: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-19 16:46:18.732830: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 428 samples and validating with randomly splitted 11 samples\n",
      "Using model file imagen_live128_24s.pt\n",
      "checkpoint loaded from imagen_live128_24s.pt\n",
      "valid loss: 0.012299847483518533\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b88a31240694ac8a541dc26c5146492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14a0dfadbc24f9896388e29ec7667b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to imagen_live128_24s.pt\n",
      "valid loss: 0.011421007569879293\n",
      "checkpoint saved to imagen_live128_24s.pt\n",
      "Training in offset 4500\n",
      "Getting Data...\n",
      "Data Gotten\n",
      "Preprocessing Data...\n",
      "Data Preprocessed...\n",
      "Training model using another batch of data...\n",
      "Training Unet No. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2023-09-19 16:51:47.685922: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-19 16:51:47.760269: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 425 samples and validating with randomly splitted 11 samples\n",
      "Using model file imagen_live128_24s.pt\n",
      "checkpoint loaded from imagen_live128_24s.pt\n",
      "valid loss: 0.015883915009908378\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d265bfb87e74854b8e548a364cfafb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd18ec6b817d4a4ea777e7ad12230ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to imagen_live128_24s.pt\n",
      "valid loss: 0.011074930429458618\n",
      "checkpoint saved to imagen_live128_24s.pt\n",
      "Training in offset 5000\n",
      "Getting Data...\n",
      "Data Gotten\n",
      "Preprocessing Data...\n",
      "Data Preprocessed...\n",
      "Training model using another batch of data...\n",
      "Training Unet No. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2023-09-19 16:57:19.172682: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-19 16:57:19.370823: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 428 samples and validating with randomly splitted 11 samples\n",
      "Using model file imagen_live128_24s.pt\n",
      "checkpoint loaded from imagen_live128_24s.pt\n",
      "valid loss: 0.011650875268969685\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2594466571ed48b8a98b08f01dfbccbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b9404cd7ac493699cee37da33dec12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to imagen_live128_24s.pt\n",
      "valid loss: 0.0072651938535273075\n",
      "checkpoint saved to imagen_live128_24s.pt\n",
      "Training in offset 5500\n",
      "Getting Data...\n",
      "Data Gotten\n",
      "Preprocessing Data...\n",
      "Data Preprocessed...\n",
      "Training model using another batch of data...\n",
      "Training Unet No. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2023-09-19 17:00:59.196593: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-19 17:00:59.373216: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 220 samples and validating with randomly splitted 6 samples\n",
      "Using model file imagen_live128_24s.pt\n",
      "checkpoint loaded from imagen_live128_24s.pt\n",
      "valid loss: 0.009400245733559132\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2caf8a303a024153895126395849caae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc1196129bd4a1ba7e0216b1415534d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to imagen_live128_24s.pt\n",
      "valid loss: 0.011020963545888662\n",
      "checkpoint saved to imagen_live128_24s.pt\n",
      "Training in offset 6000\n",
      "Getting Data...\n",
      "Data Gotten\n",
      "Preprocessing Data...\n",
      "Data Preprocessed...\n",
      "Training model using another batch of data...\n",
      "Training Unet No. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2023-09-19 17:06:30.925787: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-19 17:06:31.036610: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 428 samples and validating with randomly splitted 11 samples\n",
      "Using model file imagen_live128_24s.pt\n",
      "checkpoint loaded from imagen_live128_24s.pt\n",
      "valid loss: 0.015730672050267458\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6fde09db2246d7ba70055ef7e6f3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673fa46c2fe5451ca20a6904ccb24685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to imagen_live128_24s.pt\n",
      "valid loss: 0.011410501494538039\n",
      "checkpoint saved to imagen_live128_24s.pt\n",
      "Training in offset 6500\n",
      "Getting Data...\n",
      "Data Gotten\n",
      "Preprocessing Data...\n",
      "Data Preprocessed...\n",
      "Training model using another batch of data...\n",
      "Training Unet No. 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2023-09-19 17:12:01.793981: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-19 17:12:01.882832: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 425 samples and validating with randomly splitted 11 samples\n",
      "Using model file imagen_live128_24s.pt\n",
      "checkpoint loaded from imagen_live128_24s.pt\n",
      "valid loss: 0.011931030079722404\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc7c6ec68f949248f8721303a117f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d7f54a2abb40c19b602325af67a4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to imagen_live128_24s.pt\n",
      "valid loss: 0.010583127907011658\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import train_imagen_batch\n",
    "\n",
    "procs = []\n",
    "\n",
    "range_of_epochs = range(1,epochs + 1)\n",
    "\n",
    "if(continue_from_epoch_and_offset_flag != 0):\n",
    "    \n",
    "    range_of_epochs = range(continue_from_epoch, epochs + 1)\n",
    "\n",
    "offset_zero_flag = 0\n",
    "\n",
    "for epoch in range_of_epochs:\n",
    "\n",
    "    print(\"Training in epoch \" + str(epoch))\n",
    "\n",
    "    offset = 0\n",
    "\n",
    "    if(continue_from_epoch_and_offset_flag != 0 and offset_zero_flag == 0):\n",
    "        offset = continue_from_offset\n",
    "        offset_zero_flag = 1\n",
    "\n",
    "    while(True):\n",
    "        con = sl.connect(datasetPathDatabase)\n",
    "        print(\"Training in offset \" + str(offset))\n",
    "\n",
    "        print(\"Getting Data...\")\n",
    "        ## TODO: NEED TO CHECK F.ID CORRESPONDS TO CORRECT FACE_PATH\n",
    "\n",
    "        data = con.execute(\"SELECT V.ID,F.ID,V.VIDEO_PATH, V.AGE CAPTION_A, \" + \n",
    "                            \"V.ETHNICITY CAPTION_E, \" +\n",
    "                            \"lower(V.GENDER) CAPTION_G, \" +\n",
    "                            \"A.SPEAKER_EMB, \"+ (\"A.WAV_TO_VEC, \" if(audio_embs == 'wav2vec') else \"A.AUDIO_EMB2, \" if(audio_embs == 'openl3')  else \"A.PYANNOTE_TITANET, \" if(audio_embs == 'pyannoteTitaNet') else ', ') +\n",
    "                            \"A.AUDIO_FEATURES, \" +\n",
    "                            \"A.LANG CAPTION_L, \"+\n",
    "                            \"F.FACE_PATH \"+\n",
    "                            \"FROM VIDEO V \"+\n",
    "                            \"INNER JOIN AUDIO A ON V.ID = A.VIDEO_ID \" +\n",
    "                            \"INNER JOIN FACE F ON F.ID = (select ID from FACE f2 where f2.video_id = v.ID ORDER By ID limit 1 ) \" + \n",
    "                            \"WHERE AUDIO_PRE = 4 AND FACES_PRE = 2 AND AUDIO_LENGTH = \" + audio_length_used + ' ' +\n",
    "                            \"AND V.ID in (select V2.ID from VIDEO v2 order by V2.ID LIMIT \"+ str(db_chunk) +\" OFFSET \" + str(offset) + \")\")\n",
    "        dataGotten = data.fetchall()\n",
    "\n",
    "        \n",
    "        print(\"Data Gotten\")\n",
    "\n",
    "        if(len(dataGotten) == 0):\n",
    "            break\n",
    "\n",
    "        \n",
    "\n",
    "        print(\"Preprocessing Data...\")\n",
    "        df = pd.DataFrame(dataGotten,columns = ['ID','FACE_ID','VIDEO_PATH','caption_a','caption_e','caption_g','SPEAKER_EMB','AUDIO_EMB','AUDIO_FEATURES','caption_l','image_path'])\n",
    "        df[\"image_guide\"] = np.nan\n",
    "\n",
    "\n",
    "        #print(df.head(10))\n",
    "        data_frame = df[[\"ID\",\"FACE_ID\",\"image_path\",\"caption_a\",\"caption_e\",\"caption_g\",\"caption_l\"]]\n",
    "        data_frame['SPEAKER_EMB'] = df['SPEAKER_EMB']\n",
    "        data_frame['AUDIO_EMB'] = df['AUDIO_EMB']\n",
    "        data_frame['image_guide'] = df['image_guide']\n",
    "\n",
    "        \n",
    "        data_frame['SPEAKER_EMB'] = data_frame['SPEAKER_EMB'].apply(lambda x: speaker_emb_preprocess(x))\n",
    "        data_frame['AUDIO_EMB'] = data_frame['AUDIO_EMB'].apply(lambda x: audio_emb_preprocess2(x))\n",
    "        data_frame['AUDIO_FEATURES'] = data_frame['ID'].apply(lambda x: audio_features_preprocess(x))\n",
    "        data_frame = data_frame.drop(['ID'], axis=1)\n",
    "        data_frame['image_guide'] = data_frame['FACE_ID'].apply(lambda x: image_guide_preprocess(x))\n",
    "        data_frame = data_frame.drop(['FACE_ID'], axis=1)\n",
    "        data_frame['caption_a'] = data_frame['caption_a'].apply(lambda x: process_age(x))\n",
    "        data_frame['caption_g'] = data_frame['caption_g'].apply(lambda x: process_gender(x))\n",
    "        data_frame['caption_l'] = data_frame['caption_l'].apply(lambda x: process_language(x))\n",
    "        data_frame['caption_e'] = data_frame['caption_e'].apply(lambda x: process_ethnicity(x))\n",
    "\n",
    "        data_frame['INPUT'] = data_frame['SPEAKER_EMB']\n",
    "\n",
    "        for index, row in data_frame.iterrows():\n",
    "            x = process_input(data_frame.loc[index,\"caption_a\"],data_frame.loc[index,\"caption_e\"]\n",
    "                              ,data_frame.loc[index,\"caption_g\"],\n",
    "                            data_frame.loc[index,\"caption_l\"],data_frame.loc[index,\"SPEAKER_EMB\"],\n",
    "                            data_frame.loc[index,\"AUDIO_EMB\"],data_frame.loc[index,\"AUDIO_FEATURES\"],\n",
    "                            data_frame.loc[index,\"image_guide\"])\n",
    "            x = [x]\n",
    "            #AADFS = AADFS\n",
    "            data_frame.loc[index,\"INPUT\"] = x\n",
    "\n",
    "        data_frame = data_frame.drop(['caption_e', 'caption_g','caption_l','SPEAKER_EMB','AUDIO_EMB'], axis=1)\n",
    "\n",
    "\n",
    "        data_frame['image_path'] = data_frame['image_path'].apply(lambda x: process_image_path(x))\n",
    "\n",
    "        input = data_frame['INPUT'].to_numpy()\n",
    "        input = np.array([np.array(xi) for xi in input])\n",
    "        input[np.isnan(input)] = 0\n",
    "        input[input > 10] = 10\n",
    "        input[input < -10] = -10\n",
    "\n",
    "        \n",
    "        output = data_frame['image_path'].to_numpy()\n",
    "        output = np.array([np.array(xi) for xi in output])\n",
    "        output.squeeze().shape\n",
    "\n",
    "        print(\"Data Preprocessed...\")\n",
    "\n",
    "        \n",
    "        #print(len(procs))\n",
    "        #if(len(procs) > 0):\n",
    "        #    procs[0].join() # Wait for previous process to finish\n",
    "        #    print(\"Model trained using a batch of data...\")\n",
    "        #    procs = []\n",
    "        #print(len(procs))\n",
    "\n",
    "        print(\"Training model using another batch of data...\")\n",
    "        proc = multiprocessing.Process(target=train_imagen_batch.train_batch_unet1, args=(input,output,model_filename,sub_epochs,batch_size,sample_every,save_model_every,image_size,unet_dim,unet1_image_size,))\n",
    "        #procs.append(proc)\n",
    "        proc.start()\n",
    "        proc.join()\n",
    "\n",
    "        #print(\"Model trained using this batch of data...\")\n",
    "\n",
    "        #proc = multiprocessing.Process(target=train_imagen_batch.train_batch_unet2, args=(input,output,model_filename,sub_epochs,batch_size,sample_every,save_model_every,image_size,unet1_dim,unet2_dim,))\n",
    "        #proc.start()two_unets_pyannote_nemo\n",
    "        #proc.join()\n",
    "        offset = offset + db_chunk\n",
    "\n",
    "        \n",
    "        data.close()\n",
    "        con.close()\n",
    "        del dataGotten\n",
    "        del df\n",
    "        del data_frame\n",
    "        del input\n",
    "        del output\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78dccc615e96ab04385280185a87a524fe0822daf5dbad2f4bf2e7d7b28366a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
