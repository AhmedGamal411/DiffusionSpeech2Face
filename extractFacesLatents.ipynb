{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n",
      "env: ROCM_PATH=/opt/rocm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 10:04:10.949222: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 10:04:11.092137: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-20 10:04:12.223885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 10:04:12.238487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 10:04:12.238590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "#https://towardsdatascience.com/generating-new-faces-with-variational-autoencoders-d13cfcb5f0a8\n",
    "%env HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true\n",
    "%env ROCM_PATH=/opt/rocm\n",
    "import tensorflow as tf\n",
    "tf.test.is_built_with_rocm()\n",
    "# document export HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.utils import plot_model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 10:04:22.878294: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 10:04:22.879052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 10:04:22.879174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 10:04:22.879220: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 10:04:23.293680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 10:04:23.293806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 10:04:23.293910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 10:04:23.293964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 169 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-03-20 10:04:23.300130: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2023-03-20 10:04:23.335232: W tensorflow/c/c_api.cc:291] Operation '{name:'encoder_conv_3/bias/Assign' id:276 op device:{requested: '', assigned: ''} def:{{{node encoder_conv_3/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](encoder_conv_3/bias, encoder_conv_3/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-03-20 10:04:23.474501: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_conv_1_1/kernel/Assign' id:602 op device:{requested: '', assigned: ''} def:{{{node decoder_conv_1_1/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](decoder_conv_1_1/kernel, decoder_conv_1_1/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-03-20 10:04:23.576825: W tensorflow/c/c_api.cc:291] Operation '{name:'log_var_1/bias/Assign' id:881 op device:{requested: '', assigned: ''} def:{{{node log_var_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](log_var_1/bias, log_var_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "vae_model = keras.models.load_model(\"vae_model_celeba.h5\", compile=False)\n",
    "vae_decoder = keras.models.load_model(\"vae_decoder_celeba.h5\", compile=False)\n",
    "vae_encoder = keras.models.load_model(\"vae_encoder_celeba.h5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " encoder_conv_0 (Conv2D)        (None, 64, 64, 32)   896         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 64, 64, 32)   0           ['encoder_conv_0[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_conv_1 (Conv2D)        (None, 32, 32, 64)   18496       ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 32, 32, 64)   0           ['encoder_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_conv_2 (Conv2D)        (None, 16, 16, 64)   36928       ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 16, 16, 64)   0           ['encoder_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_conv_3 (Conv2D)        (None, 8, 8, 64)     36928       ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 8, 8, 64)     0           ['encoder_conv_3[0][0]']         \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 4096)         0           ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " mu (Dense)                     (None, 200)          819400      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " log_var (Dense)                (None, 200)          819400      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " encoder_output (Lambda)        (None, 200)          0           ['mu[0][0]',                     \n",
      "                                                                  'log_var[0][0]']                \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, 128, 128, 3)  916483      ['encoder_output[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,648,531\n",
      "Trainable params: 2,648,531\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n",
      "Got chunk of faces from database. Extracting latent features...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m face_img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(face_img)\n\u001b[1;32m     33\u001b[0m face_img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(face_img,\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m latent_rep \u001b[39m=\u001b[39m vae_encoder\u001b[39m.\u001b[39;49mpredict(face_img)\n\u001b[1;32m     35\u001b[0m latent_rep \u001b[39m=\u001b[39m latent_rep[\u001b[39m0\u001b[39m]\n\u001b[1;32m     36\u001b[0m latent_rep_pickled \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mdumps(latent_rep)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/keras/engine/training_v1.py:1057\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_call_args(\u001b[39m\"\u001b[39m\u001b[39mpredict\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1056\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m-> 1057\u001b[0m \u001b[39mreturn\u001b[39;00m func\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m   1058\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1059\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m   1060\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1061\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1062\u001b[0m     steps\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m   1063\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1064\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1065\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1066\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1067\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py:801\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.predict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    797\u001b[0m batch_size \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m_validate_or_infer_batch_size(batch_size, steps, x)\n\u001b[1;32m    798\u001b[0m x, _, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m_standardize_user_data(\n\u001b[1;32m    799\u001b[0m     x, check_steps\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, steps_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msteps\u001b[39m\u001b[39m\"\u001b[39m, steps\u001b[39m=\u001b[39msteps\n\u001b[1;32m    800\u001b[0m )\n\u001b[0;32m--> 801\u001b[0m \u001b[39mreturn\u001b[39;00m predict_loop(\n\u001b[1;32m    802\u001b[0m     model,\n\u001b[1;32m    803\u001b[0m     x,\n\u001b[1;32m    804\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    805\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    806\u001b[0m     steps\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m    807\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    808\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py:421\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m callbacks\u001b[39m.\u001b[39m_call_batch_hook(\n\u001b[1;32m    417\u001b[0m     mode, \u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m, batch_index, batch_logs\n\u001b[1;32m    418\u001b[0m )\n\u001b[1;32m    420\u001b[0m \u001b[39m# Get outputs.\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m batch_outs \u001b[39m=\u001b[39m f(ins_batch)\n\u001b[1;32m    422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_outs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    423\u001b[0m     batch_outs \u001b[39m=\u001b[39m [batch_outs]\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/keras/backend.py:4581\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4571\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callable_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   4573\u001b[0m     \u001b[39mor\u001b[39;00m feed_arrays \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feed_arrays\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4577\u001b[0m     \u001b[39mor\u001b[39;00m session \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_session\n\u001b[1;32m   4578\u001b[0m ):\n\u001b[1;32m   4579\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[0;32m-> 4581\u001b[0m fetched \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_callable_fn(\u001b[39m*\u001b[39;49marray_vals, run_metadata\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_metadata)\n\u001b[1;32m   4582\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetches) :])\n\u001b[1;32m   4583\u001b[0m output_structure \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mpack_sequence_as(\n\u001b[1;32m   4584\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_structure,\n\u001b[1;32m   4585\u001b[0m     fetched[: \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs)],\n\u001b[1;32m   4586\u001b[0m     expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   4587\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/tensorflow/python/client/session.py:1481\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1479\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1480\u001b[0m   run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1481\u001b[0m   ret \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39;49mTF_SessionRunCallable(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49m_session,\n\u001b[1;32m   1482\u001b[0m                                          \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle, args,\n\u001b[1;32m   1483\u001b[0m                                          run_metadata_ptr)\n\u001b[1;32m   1484\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[1;32m   1485\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TODO document jupyter\n",
    "import pickle\n",
    "import configparser\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "datasetPathDatabase =  configParser.get('COMMON', 'datasetPathDatabase') + '/dataset.db'\n",
    "p =  configParser.get('extractFacesLatents', 'dbChunk')\n",
    "\n",
    "# TODO Better display of progress and handling of exceptions\n",
    "contLoop = True # Flag to continue to get chunks of videos from database\n",
    "con = sl.connect(datasetPathDatabase)\n",
    "offset = 0\n",
    "while(contLoop):\n",
    "    data = con.execute(\"SELECT ID,FACE_PATH FROM FACE WHERE LATENT_REP IS NULL ORDER BY ID ASC LIMIT \" + p + \" OFFSET \" + str(offset))\n",
    "    contLoop = False\n",
    "    offset = offset + int(p)\n",
    "    print(\"Got chunk of faces from database. Extracting latent features...\")\n",
    "    dataGotten = data.fetchall()\n",
    "    for id,facePath in dataGotten:\n",
    "        contLoop = True\n",
    "        face_img = Image.open(facePath)\n",
    "        face_img.load() # required for png.split()\n",
    "        face_img2 = Image.new(\"RGB\", face_img.size, (255, 255, 255))\n",
    "        face_img2.paste(face_img, mask=face_img.split()[3]) # 3 is the alpha channel\n",
    "        face_img = face_img2\n",
    "        face_img = np.array(face_img)\n",
    "        face_img = np.expand_dims(face_img,0)\n",
    "        latent_rep = vae_encoder.predict(face_img)\n",
    "        latent_rep = latent_rep[0]\n",
    "        latent_rep_pickled = pickle.dumps(latent_rep)\n",
    "\n",
    "        # Insert latent representations into database\n",
    "        sql = '''UPDATE FACE SET LATENT_REP = ? WHERE ID = ?'''\n",
    "\n",
    "        \n",
    "        cur = con.cursor()\n",
    "        data = [latent_rep_pickled,id]\n",
    "        cur.execute(sql, data)\n",
    "        con.commit()\n",
    "        cur.close()\n",
    "\n",
    "\n",
    "con.close()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/NZOCucECvDs_62.732333_65.732333_face_1.png'),\n",
       " (2,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/NZOCucECvDs_62.732333_65.732333_face_2.png'),\n",
       " (3,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/NZOCucECvDs_62.732333_65.732333_face_3.png'),\n",
       " (4,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/1uizi6R-PMU_116.920000_120.000000_face_1.png'),\n",
       " (5,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/1uizi6R-PMU_116.920000_120.000000_face_2.png'),\n",
       " (6,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/1uizi6R-PMU_116.920000_120.000000_face_3.png'),\n",
       " (7,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/4ipH_P0io9o_79.160000_82.480000_face_1.png'),\n",
       " (8,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/4ipH_P0io9o_79.160000_82.480000_face_2.png'),\n",
       " (9,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/4ipH_P0io9o_79.160000_82.480000_face_3.png'),\n",
       " (10,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/x2brYLv1HJk_146.533000_149.933000_face_1.png'),\n",
       " (11,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/x2brYLv1HJk_146.533000_149.933000_face_2.png'),\n",
       " (12,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/x2brYLv1HJk_146.533000_149.933000_face_3.png'),\n",
       " (13,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/q1552sXvKNc_230.480256_239.989756_face_1.png'),\n",
       " (14,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/q1552sXvKNc_230.480256_239.989756_face_2.png'),\n",
       " (15,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/q1552sXvKNc_230.480256_239.989756_face_3.png'),\n",
       " (16,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-2pPayrmmPI_90.000000_99.733333_face_1.png'),\n",
       " (17,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-2pPayrmmPI_90.000000_99.733333_face_2.png'),\n",
       " (18,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-2pPayrmmPI_90.000000_99.733333_face_3.png'),\n",
       " (19,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/aOWfern0vHI_254.462822_261.594744_face_1.png'),\n",
       " (20,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/aOWfern0vHI_254.462822_261.594744_face_2.png'),\n",
       " (21,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/aOWfern0vHI_254.462822_261.594744_face_3.png'),\n",
       " (22,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-3YxIpmrKe4_210.001467_215.924044_face_1.png'),\n",
       " (23,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-3YxIpmrKe4_210.001467_215.924044_face_2.png'),\n",
       " (24,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-3YxIpmrKe4_210.001467_215.924044_face_3.png'),\n",
       " (25,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/cxgPVppBov0_230.480000_236.280000_face_1.png'),\n",
       " (26,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/cxgPVppBov0_230.480000_236.280000_face_2.png'),\n",
       " (27,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/cxgPVppBov0_230.480000_236.280000_face_3.png'),\n",
       " (28,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/xgeVospNNGk_99.733333_102.733333_face_1.png'),\n",
       " (29,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/xgeVospNNGk_99.733333_102.733333_face_2.png'),\n",
       " (30,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/xgeVospNNGk_99.733333_102.733333_face_3.png'),\n",
       " (31,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/FOrctLP1xXc_190.160000_197.880000_face_1.png'),\n",
       " (32,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/FOrctLP1xXc_190.160000_197.880000_face_2.png'),\n",
       " (33,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/FOrctLP1xXc_190.160000_197.880000_face_3.png'),\n",
       " (34,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/ewkFvB89_AE_278.244000_281.414000_face_1.png'),\n",
       " (35,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/ewkFvB89_AE_278.244000_281.414000_face_2.png'),\n",
       " (36,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/ewkFvB89_AE_278.244000_281.414000_face_3.png'),\n",
       " (37,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/XWvpySr7_cY_120.080000_132.240000_face_1.png'),\n",
       " (38,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/XWvpySr7_cY_120.080000_132.240000_face_2.png'),\n",
       " (39,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/XWvpySr7_cY_120.080000_132.240000_face_3.png'),\n",
       " (40,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/w9YkmluWDg4_169.702867_179.979800_face_1.png'),\n",
       " (41,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/w9YkmluWDg4_169.702867_179.979800_face_2.png'),\n",
       " (42,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/w9YkmluWDg4_169.702867_179.979800_face_3.png'),\n",
       " (43,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/xncsi6RR8Ss_16.880000_30.000000_face_1.png'),\n",
       " (44,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/xncsi6RR8Ss_16.880000_30.000000_face_2.png'),\n",
       " (45,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/xncsi6RR8Ss_16.880000_30.000000_face_3.png'),\n",
       " (46,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/ufuRTqz29no_278.866667_283.900000_face_1.png'),\n",
       " (47,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/ufuRTqz29no_278.866667_283.900000_face_2.png'),\n",
       " (48,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/ufuRTqz29no_278.866667_283.900000_face_3.png'),\n",
       " (49,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/K14x8wwT2zM_294.466667_300.000000_face_1.png'),\n",
       " (50,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/K14x8wwT2zM_294.466667_300.000000_face_2.png'),\n",
       " (51,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/K14x8wwT2zM_294.466667_300.000000_face_3.png'),\n",
       " (52,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-124IQ-v7-Y_100.480000_105.400000_face_1.png'),\n",
       " (53,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-124IQ-v7-Y_100.480000_105.400000_face_2.png'),\n",
       " (54,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-124IQ-v7-Y_100.480000_105.400000_face_3.png'),\n",
       " (55,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/4CRhkc3MhRo_288.800000_292.040000_face_1.png'),\n",
       " (56,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/4CRhkc3MhRo_288.800000_292.040000_face_2.png'),\n",
       " (57,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/4CRhkc3MhRo_288.800000_292.040000_face_3.png'),\n",
       " (58,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/upkSgdSdm9k_161.313000_173.972000_face_1.png'),\n",
       " (59,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/upkSgdSdm9k_161.313000_173.972000_face_2.png'),\n",
       " (60,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/upkSgdSdm9k_161.313000_173.972000_face_3.png'),\n",
       " (61,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe0_47.360000_51.120000_face_1.png'),\n",
       " (62,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe0_47.360000_51.120000_face_2.png'),\n",
       " (63,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/zoNyZZXbOe0_47.360000_51.120000_face_3.png'),\n",
       " (64,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/LChyZrli7LM_182.280000_188.040000_face_1.png'),\n",
       " (65,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/LChyZrli7LM_182.280000_188.040000_face_2.png'),\n",
       " (66,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/LChyZrli7LM_182.280000_188.040000_face_3.png'),\n",
       " (67,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/D4ET00DHmS4_6.266667_12.000000_face_1.png'),\n",
       " (68,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/D4ET00DHmS4_6.266667_12.000000_face_2.png'),\n",
       " (69,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/D4ET00DHmS4_6.266667_12.000000_face_3.png'),\n",
       " (70,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/sa-cXFFo32g_110.210000_115.182000_face_1.png'),\n",
       " (71,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/sa-cXFFo32g_110.210000_115.182000_face_2.png'),\n",
       " (72,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/sa-cXFFo32g_110.210000_115.182000_face_3.png'),\n",
       " (73,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/y7b2Uj0BLXc_14.560000_17.560000_face_1.png'),\n",
       " (74,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/y7b2Uj0BLXc_14.560000_17.560000_face_2.png'),\n",
       " (75,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/y7b2Uj0BLXc_14.560000_17.560000_face_3.png'),\n",
       " (76,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/OiY3ci4HXgg_186.110900_191.149267_face_1.png'),\n",
       " (77,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/OiY3ci4HXgg_186.110900_191.149267_face_2.png'),\n",
       " (78,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/OiY3ci4HXgg_186.110900_191.149267_face_3.png'),\n",
       " (79,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/4OlwnCgH9xc_92.692600_96.029267_face_1.png'),\n",
       " (80,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/4OlwnCgH9xc_92.692600_96.029267_face_2.png'),\n",
       " (81,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/4OlwnCgH9xc_92.692600_96.029267_face_3.png'),\n",
       " (82,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/vkRSM5lLlec_105.160000_119.800000_face_1.png'),\n",
       " (83,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/vkRSM5lLlec_105.160000_119.800000_face_2.png'),\n",
       " (84,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/vkRSM5lLlec_105.160000_119.800000_face_3.png'),\n",
       " (85,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/geHHiHydbOU_286.452833_294.994700_face_1.png'),\n",
       " (86,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/geHHiHydbOU_286.452833_294.994700_face_2.png'),\n",
       " (87,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/geHHiHydbOU_286.452833_294.994700_face_3.png'),\n",
       " (88,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-wuxbgMRIWs_30.030000_36.745044_face_1.png'),\n",
       " (89,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-wuxbgMRIWs_30.030000_36.745044_face_2.png'),\n",
       " (90,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-wuxbgMRIWs_30.030000_36.745044_face_3.png'),\n",
       " (91,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvbs_234.067167_238.571667_face_1.png'),\n",
       " (92,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvbs_234.067167_238.571667_face_2.png'),\n",
       " (93,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/Hgj_vrVuvbs_234.067167_238.571667_face_3.png'),\n",
       " (94,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/bR80JEaW4LY_86.878456_89.964878_face_1.png'),\n",
       " (95,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/bR80JEaW4LY_86.878456_89.964878_face_2.png'),\n",
       " (96,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/bR80JEaW4LY_86.878456_89.964878_face_3.png'),\n",
       " (97,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/VvcwAGkSy2o_240.200000_253.366667_face_1.png'),\n",
       " (98,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/VvcwAGkSy2o_240.200000_253.366667_face_2.png'),\n",
       " (99,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/VvcwAGkSy2o_240.200000_253.366667_face_3.png'),\n",
       " (100,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZo_180.013167_184.142289_face_1.png'),\n",
       " (101,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZo_180.013167_184.142289_face_2.png'),\n",
       " (102,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZo_180.013167_184.142289_face_3.png'),\n",
       " (103,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZo_270.270000_278.319711_face_1.png'),\n",
       " (104,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZo_270.270000_278.319711_face_2.png'),\n",
       " (105,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-1n-Uy-SZZo_270.270000_278.319711_face_3.png'),\n",
       " (106,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/30I8ejHtchI_60.760000_63.760000_face_1.png'),\n",
       " (107,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/30I8ejHtchI_60.760000_63.760000_face_2.png'),\n",
       " (108,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/30I8ejHtchI_60.760000_63.760000_face_3.png'),\n",
       " (109,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-6Q8rfdxYoM_197.600000_204.200000_face_1.png'),\n",
       " (110,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-6Q8rfdxYoM_197.600000_204.200000_face_2.png'),\n",
       " (111,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/-6Q8rfdxYoM_197.600000_204.200000_face_3.png'),\n",
       " (112,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/8zFuvALoSgU_170.169000_176.142000_face_1.png'),\n",
       " (113,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/8zFuvALoSgU_170.169000_176.142000_face_2.png'),\n",
       " (114,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/8zFuvALoSgU_170.169000_176.142000_face_3.png'),\n",
       " (115,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/McOz-H57DOo_128.378467_134.384533_face_1.png'),\n",
       " (116,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/McOz-H57DOo_128.378467_134.384533_face_2.png'),\n",
       " (117,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/McOz-H57DOo_128.378467_134.384533_face_3.png'),\n",
       " (118,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/S8ya8TH0Fhc_60.000000_70.733333_face_1.png'),\n",
       " (119,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/S8ya8TH0Fhc_60.000000_70.733333_face_2.png'),\n",
       " (120,\n",
       "  '/home/gamal/Datasets/Dataset1/Faces/S8ya8TH0Fhc_60.000000_70.733333_face_3.png')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataGotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = vae_encoder.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 200)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0843052 , -0.1574214 ,  0.96014076, -0.01608606,  0.37075156,\n",
       "        2.7769132 , -1.194191  ,  0.8174098 , -1.2320787 , -0.26595482,\n",
       "        1.5342367 ,  1.5997851 , -1.3255134 ,  0.51969504,  0.38224915,\n",
       "        1.715023  , -0.20995013,  0.4509715 , -0.34549168, -1.4251034 ,\n",
       "       -0.7809226 , -0.7464825 , -1.8087181 ,  0.03291734, -0.66174364,\n",
       "       -0.5666951 , -1.0904781 , -0.17445779, -0.29315504,  1.0310547 ,\n",
       "       -0.40406942,  1.0277227 , -0.31162962,  1.1208587 , -0.4127219 ,\n",
       "       -0.8519272 , -0.2411303 , -2.1992898 ,  0.7975864 , -0.34818193,\n",
       "        1.8494238 , -1.5860696 ,  1.528455  , -1.1227158 ,  0.7889575 ,\n",
       "        4.2257195 , -2.103981  , -0.27558464, -0.9730452 , -2.1764283 ,\n",
       "       -1.2047594 ,  1.3019958 , -0.08131307,  0.9376823 , -1.6207539 ,\n",
       "        1.3475494 ,  1.0325551 ,  0.25274947, -0.34661037,  0.23892787,\n",
       "       -0.13960503, -0.01966438,  0.69003373, -1.181275  ,  0.77673805,\n",
       "        0.09425893, -2.7011867 , -0.5522225 , -1.0743376 , -1.1565216 ,\n",
       "        0.66673815,  0.27158937,  1.5197184 ,  0.14486414,  0.6404372 ,\n",
       "        1.0196056 , -0.86201483,  0.25106457, -0.8117353 ,  0.49410105,\n",
       "       -0.40389776, -0.8641347 ,  0.49041563, -0.97693396,  0.4951906 ,\n",
       "       -0.5437141 , -0.511065  ,  2.3717232 , -0.16331768,  1.0369139 ,\n",
       "       -1.1473608 , -2.428545  , -1.3192562 , -1.01924   , -0.24039875,\n",
       "       -0.6821546 ,  0.16475268,  1.4948955 ,  0.8640911 ,  0.51707757,\n",
       "       -0.3609952 ,  0.5560901 , -0.8223319 , -1.7822555 ,  0.98635745,\n",
       "       -0.0752519 , -1.0688524 , -0.00601067,  0.01731715,  0.14547221,\n",
       "        1.1143373 , -0.939827  , -2.0724924 , -0.8791814 ,  0.8800576 ,\n",
       "        1.0978346 , -4.6935463 ,  1.1234984 ,  1.5647014 , -1.285024  ,\n",
       "        0.6584078 ,  0.39674333, -2.5160832 ,  1.7859313 , -0.12670134,\n",
       "        0.162983  ,  2.3198466 ,  0.83023125,  0.27312985,  2.2986748 ,\n",
       "       -2.1266677 , -1.1610645 , -0.4327457 , -0.16615847, -0.19639188,\n",
       "        0.04778794, -1.2731749 ,  0.95967555, -0.37596104,  1.2106811 ,\n",
       "        1.0231892 ,  0.6007466 , -0.03507247,  2.4079704 ,  0.7527112 ,\n",
       "       -0.20586562,  1.1418204 , -1.6248528 , -0.5225341 , -0.22721794,\n",
       "       -1.1510135 ,  0.5421943 ,  1.0049372 , -1.7303038 , -2.847191  ,\n",
       "        0.2411018 , -0.03022631,  0.5087832 ,  2.7748818 , -0.74773157,\n",
       "        0.12614648,  2.197372  , -1.7105571 ,  0.7005884 ,  0.95970774,\n",
       "       -0.42898634, -0.31811255,  0.36005792,  0.7285487 , -0.98268014,\n",
       "       -0.39392596,  1.1223409 , -0.78187597,  0.6527305 ,  0.6200126 ,\n",
       "        0.11165036, -0.07414015, -0.39165246, -1.8914969 ,  0.9160813 ,\n",
       "        0.8915732 , -0.3745466 ,  1.910657  ,  1.5175396 ,  1.8125328 ,\n",
       "       -0.43959668, -1.2090912 ,  0.27431825, -0.9422703 ,  0.5677498 ,\n",
       "       -0.6483595 ,  0.8554053 ,  0.07261603,  0.22835349, -1.1922642 ,\n",
       "       -0.577156  , -0.8912283 ,  0.24202605,  1.7821909 ,  0.29940373],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latents[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
