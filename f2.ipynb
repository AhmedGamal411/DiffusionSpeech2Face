{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 19:29:38.138856: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You do not have pycocotools installed, so KerasCV pycoco metrics are not available. Please run `pip install pycocotools`.\n",
      "You do not have pyococotools installed, so the `PyCOCOCallback` API is not available.\n",
      "You do not have Waymo Open Dataset installed, so KerasCV Waymo metrics are not available.\n"
     ]
    }
   ],
   "source": [
    "from textwrap import wrap\n",
    "import os\n",
    "\n",
    "import keras_cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "from keras_cv.models.stable_diffusion.clip_tokenizer import SimpleTokenizer\n",
    "from keras_cv.models.stable_diffusion.diffusion_model import DiffusionModel\n",
    "from keras_cv.models.stable_diffusion.image_encoder import ImageEncoder\n",
    "from keras_cv.models.stable_diffusion.noise_scheduler import NoiseScheduler\n",
    "from keras_cv.models.stable_diffusion.text_encoder import TextEncoder\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://huggingface.co/datasets/sayakpaul/pokemon-blip-original-version/resolve/main/pokemon_dataset.tar.gz\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "URL fetch failure on https://huggingface.co/datasets/sayakpaul/pokemon-blip-original-version/resolve/main/pokemon_dataset.tar.gz: None -- EOF occurred in violation of protocol (_ssl.c:1131)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLEOFError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/urllib/request.py:1354\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1354\u001b[0m     h\u001b[39m.\u001b[39;49mrequest(req\u001b[39m.\u001b[39;49mget_method(), req\u001b[39m.\u001b[39;49mselector, req\u001b[39m.\u001b[39;49mdata, headers,\n\u001b[1;32m   1355\u001b[0m               encode_chunked\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mhas_header(\u001b[39m'\u001b[39;49m\u001b[39mTransfer-encoding\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m   1356\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/http/client.py:1256\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1256\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/http/client.py:1302\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1301\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1302\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/http/client.py:1251\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1251\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/http/client.py:1011\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1011\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1013\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1014\u001b[0m \n\u001b[1;32m   1015\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/http/client.py:951\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 951\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    952\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/http/client.py:1425\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1423\u001b[0m     server_hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[0;32m-> 1425\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context\u001b[39m.\u001b[39;49mwrap_socket(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msock,\n\u001b[1;32m   1426\u001b[0m                                       server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/ssl.py:500\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    496\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    497\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    498\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    501\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    502\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    503\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    504\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    505\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    506\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    507\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    508\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/ssl.py:1040\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1040\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1041\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/ssl.py:1309\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1309\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1310\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[0;31mSSLEOFError\u001b[0m: EOF occurred in violation of protocol (_ssl.c:1131)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/keras/utils/data_utils.py:300\u001b[0m, in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     urlretrieve(origin, fpath, DLProgbar())\n\u001b[1;32m    301\u001b[0m \u001b[39mexcept\u001b[39;00m urllib\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mHTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/keras/utils/data_utils.py:84\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m response \u001b[39m=\u001b[39m urlopen(url, data)\n\u001b[1;32m     85\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m fd:\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/urllib/request.py:222\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 222\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/urllib/request.py:531\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m     meth \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 531\u001b[0m     response \u001b[39m=\u001b[39m meth(req, response)\n\u001b[1;32m    533\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/urllib/request.py:640\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m):\n\u001b[0;32m--> 640\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49merror(\n\u001b[1;32m    641\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m'\u001b[39;49m, request, response, code, msg, hdrs)\n\u001b[1;32m    643\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    562\u001b[0m args \u001b[39m=\u001b[39m (\u001b[39mdict\u001b[39m, proto, meth_name) \u001b[39m+\u001b[39m args\n\u001b[0;32m--> 563\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    564\u001b[0m \u001b[39mif\u001b[39;00m result:\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/urllib/request.py:502\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 502\u001b[0m result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/urllib/request.py:755\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[0;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[1;32m    753\u001b[0m fp\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 755\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49mopen(new, timeout\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    524\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[0;32m--> 525\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[1;32m    527\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/urllib/request.py:542\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    541\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[0;32m--> 542\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[1;32m    543\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[1;32m    544\u001b[0m \u001b[39mif\u001b[39;00m result:\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/urllib/request.py:502\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 502\u001b[0m result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/urllib/request.py:1397\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1396\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttps_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[0;32m-> 1397\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPSConnection, req,\n\u001b[1;32m   1398\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context, check_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_hostname)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/urllib/request.py:1357\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1357\u001b[0m     \u001b[39mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1358\u001b[0m r \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mgetresponse()\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error EOF occurred in violation of protocol (_ssl.c:1131)>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_path \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mget_file(\n\u001b[1;32m      2\u001b[0m     origin\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhttps://huggingface.co/datasets/sayakpaul/pokemon-blip-original-version/resolve/main/pokemon_dataset.tar.gz\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     untar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m data_frame \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_path, \u001b[39m\"\u001b[39m\u001b[39mdata.csv\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      8\u001b[0m data_frame[\u001b[39m\"\u001b[39m\u001b[39mimage_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m data_frame[\u001b[39m\"\u001b[39m\u001b[39mimage_path\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\n\u001b[1;32m      9\u001b[0m     \u001b[39mlambda\u001b[39;00m x: os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_path, x)\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/keras/utils/data_utils.py:304\u001b[0m, in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(error_msg\u001b[39m.\u001b[39mformat(origin, e\u001b[39m.\u001b[39mcode, e\u001b[39m.\u001b[39mmsg))\n\u001b[1;32m    303\u001b[0m     \u001b[39mexcept\u001b[39;00m urllib\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mURLError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(error_msg\u001b[39m.\u001b[39mformat(origin, e\u001b[39m.\u001b[39merrno, e\u001b[39m.\u001b[39mreason))\n\u001b[1;32m    305\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m    306\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(fpath):\n",
      "\u001b[0;31mException\u001b[0m: URL fetch failure on https://huggingface.co/datasets/sayakpaul/pokemon-blip-original-version/resolve/main/pokemon_dataset.tar.gz: None -- EOF occurred in violation of protocol (_ssl.c:1131)"
     ]
    }
   ],
   "source": [
    "data_path = tf.keras.utils.get_file(\n",
    "    origin=\"https://huggingface.co/datasets/sayakpaul/pokemon-blip-original-version/resolve/main/pokemon_dataset.tar.gz\",\n",
    "    untar=True,\n",
    ")\n",
    "\n",
    "data_frame = pd.read_csv(os.path.join(data_path, \"data.csv\"))\n",
    "\n",
    "data_frame[\"image_path\"] = data_frame[\"image_path\"].apply(\n",
    "    lambda x: os.path.join(data_path, x)\n",
    ")\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The padding token and maximum prompt length are specific to the text encoder.\n",
    "# If you're using a different text encoder be sure to change them accordingly.\n",
    "PADDING_TOKEN = 49407\n",
    "MAX_PROMPT_LENGTH = 77\n",
    "\n",
    "# Load the tokenizer.\n",
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "#  Method to tokenize and pad the tokens.\n",
    "def process_text(caption):\n",
    "    tokens = tokenizer.encode(caption)\n",
    "    tokens = tokens + [PADDING_TOKEN] * (MAX_PROMPT_LENGTH - len(tokens))\n",
    "    return np.array(tokens)\n",
    "\n",
    "\n",
    "# Collate the tokenized captions into an array.\n",
    "tokenized_texts = np.empty((len(data_frame), MAX_PROMPT_LENGTH))\n",
    "\n",
    "all_captions = list(data_frame[\"caption\"].values)\n",
    "for i, caption in enumerate(all_captions):\n",
    "    tokenized_texts[i] = process_text(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "POS_IDS = tf.convert_to_tensor([list(range(MAX_PROMPT_LENGTH))], dtype=tf.int32)\n",
    "\n",
    "augmenter = keras_cv.layers.Augmenter(\n",
    "    layers=[\n",
    "        keras_cv.layers.CenterCrop(RESOLUTION, RESOLUTION),\n",
    "        keras_cv.layers.RandomFlip(),\n",
    "        tf.keras.layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "    ]\n",
    ")\n",
    "text_encoder = TextEncoder(MAX_PROMPT_LENGTH)\n",
    "\n",
    "\n",
    "def process_image(image_path, tokenized_text):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_png(image, 3)\n",
    "    image = tf.image.resize(image, (RESOLUTION, RESOLUTION))\n",
    "    return image, tokenized_text\n",
    "\n",
    "\n",
    "def apply_augmentation(image_batch, token_batch):\n",
    "    return augmenter(image_batch), token_batch\n",
    "\n",
    "\n",
    "def run_text_encoder(image_batch, token_batch):\n",
    "    return (\n",
    "        image_batch,\n",
    "        token_batch,\n",
    "        text_encoder([token_batch, POS_IDS], training=False),\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_dict(image_batch, token_batch, encoded_text_batch):\n",
    "    return {\n",
    "        \"images\": image_batch,\n",
    "        \"tokens\": token_batch,\n",
    "        \"encoded_text\": encoded_text_batch,\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_dataset(image_paths, tokenized_texts, batch_size=1):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, tokenized_texts))\n",
    "    dataset = dataset.shuffle(batch_size * 10)\n",
    "    dataset = dataset.map(process_image, num_parallel_calls=AUTO).batch(batch_size)\n",
    "    dataset = dataset.map(apply_augmentation, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.map(run_text_encoder, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.map(prepare_dict, num_parallel_calls=AUTO)\n",
    "    return dataset.prefetch(AUTO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset.\n",
    "training_dataset = prepare_dataset(\n",
    "    np.array(data_frame[\"image_path\"]), tokenized_texts, batch_size=4\n",
    ")\n",
    "\n",
    "# Take a sample batch and investigate.\n",
    "sample_batch = next(iter(training_dataset))\n",
    "\n",
    "for k in sample_batch:\n",
    "    print(k, sample_batch[k].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i in range(3):\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.imshow((sample_batch[\"images\"][i] + 1) / 2)\n",
    "\n",
    "    text = tokenizer.decode(sample_batch[\"tokens\"][i].numpy().squeeze())\n",
    "    text = text.replace(\"<|startoftext|>\", \"\")\n",
    "    text = text.replace(\"<|endoftext|>\", \"\")\n",
    "    text = \"\\n\".join(wrap(text, 12))\n",
    "    plt.title(text, fontsize=15)\n",
    "\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(tf.keras.Model):\n",
    "    # Reference:\n",
    "    # https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusion_model,\n",
    "        vae,\n",
    "        noise_scheduler,\n",
    "        use_mixed_precision=False,\n",
    "        max_grad_norm=1.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.diffusion_model = diffusion_model\n",
    "        self.vae = vae\n",
    "        self.noise_scheduler = noise_scheduler\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.use_mixed_precision = use_mixed_precision\n",
    "        self.vae.trainable = False\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images = inputs[\"images\"]\n",
    "        encoded_text = inputs[\"encoded_text\"]\n",
    "        batch_size = tf.shape(images)[0]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Project image into the latent space and sample from it.\n",
    "            latents = self.sample_from_encoder_outputs(self.vae(images, training=False))\n",
    "            # Know more about the magic number here:\n",
    "            # https://keras.io/examples/generative/fine_tune_via_textual_inversion/\n",
    "            latents = latents * 0.18215\n",
    "\n",
    "            # Sample noise that we'll add to the latents.\n",
    "            noise = tf.random.normal(tf.shape(latents))\n",
    "\n",
    "            # Sample a random timestep for each image.\n",
    "            timesteps = tnp.random.randint(\n",
    "                0, self.noise_scheduler.train_timesteps, (batch_size,)\n",
    "            )\n",
    "\n",
    "            # Add noise to the latents according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process).\n",
    "            noisy_latents = self.noise_scheduler.add_noise(\n",
    "                tf.cast(latents, noise.dtype), noise, timesteps\n",
    "            )\n",
    "\n",
    "            # Get the target for loss depending on the prediction type\n",
    "            # just the sampled noise for now.\n",
    "            target = noise  # noise_schedule.predict_epsilon == True\n",
    "\n",
    "            # Predict the noise residual and compute loss.\n",
    "            timestep_embedding = tf.map_fn(\n",
    "                lambda t: self.get_timestep_embedding(t), timesteps, dtype=tf.float32\n",
    "            )\n",
    "            timestep_embedding = tf.squeeze(timestep_embedding, 1)\n",
    "            model_pred = self.diffusion_model(\n",
    "                [noisy_latents, timestep_embedding, encoded_text], training=True\n",
    "            )\n",
    "            loss = self.compiled_loss(target, model_pred)\n",
    "            if self.use_mixed_precision:\n",
    "                loss = self.optimizer.get_scaled_loss(loss)\n",
    "\n",
    "        # Update parameters of the diffusion model.\n",
    "        trainable_vars = self.diffusion_model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        if self.use_mixed_precision:\n",
    "            gradients = self.optimizer.get_unscaled_gradients(gradients)\n",
    "        gradients = [tf.clip_by_norm(g, self.max_grad_norm) for g in gradients]\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def get_timestep_embedding(self, timestep, dim=320, max_period=10000):\n",
    "        half = dim // 2\n",
    "        log_max_preiod = tf.math.log(tf.cast(max_period, tf.float32))\n",
    "        freqs = tf.math.exp(\n",
    "            -log_max_preiod * tf.range(0, half, dtype=tf.float32) / half\n",
    "        )\n",
    "        args = tf.convert_to_tensor([timestep], dtype=tf.float32) * freqs\n",
    "        embedding = tf.concat([tf.math.cos(args), tf.math.sin(args)], 0)\n",
    "        embedding = tf.reshape(embedding, [1, -1])\n",
    "        return embedding\n",
    "\n",
    "    def sample_from_encoder_outputs(self, outputs):\n",
    "        mean, logvar = tf.split(outputs, 2, axis=-1)\n",
    "        logvar = tf.clip_by_value(logvar, -30.0, 20.0)\n",
    "        std = tf.exp(0.5 * logvar)\n",
    "        sample = tf.random.normal(tf.shape(mean), dtype=mean.dtype)\n",
    "        return mean + std * sample\n",
    "\n",
    "    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n",
    "        # Overriding this method will allow us to use the `ModelCheckpoint`\n",
    "        # callback directly with this trainer class. In this case, it will\n",
    "        # only checkpoint the `diffusion_model` since that's what we're training\n",
    "        # during fine-tuning.\n",
    "        self.diffusion_model.save_weights(\n",
    "            filepath=filepath,\n",
    "            overwrite=overwrite,\n",
    "            save_format=save_format,\n",
    "            options=options,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable mixed-precision training if the underlying GPU has tensor cores.\n",
    "USE_MP = False\n",
    "if USE_MP:\n",
    "    keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "image_encoder = ImageEncoder(RESOLUTION, RESOLUTION)\n",
    "diffusion_ft_trainer = Trainer(\n",
    "    diffusion_model=DiffusionModel(RESOLUTION, RESOLUTION, MAX_PROMPT_LENGTH),\n",
    "    # Remove the top layer from the encoder, which cuts off the variance and only\n",
    "    # returns the mean.\n",
    "    vae=tf.keras.Model(\n",
    "        image_encoder.input,\n",
    "        image_encoder.layers[-2].output,\n",
    "    ),\n",
    "    noise_scheduler=NoiseScheduler(),\n",
    "    use_mixed_precision=USE_MP,\n",
    ")\n",
    "\n",
    "# These hyperparameters come from this tutorial by Hugging Face:\n",
    "# https://huggingface.co/docs/diffusers/training/text2image\n",
    "lr = 1e-5\n",
    "beta_1, beta_2 = 0.9, 0.999\n",
    "weight_decay = (1e-2,)\n",
    "epsilon = 1e-08\n",
    "\n",
    "optimizer = tf.keras.optimizers.experimental.AdamW(\n",
    "    learning_rate=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    beta_1=beta_1,\n",
    "    beta_2=beta_2,\n",
    "    epsilon=epsilon,\n",
    ")\n",
    "diffusion_ft_trainer.compile(optimizer=optimizer, loss=\"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "ckpt_path = \"finetuned_stable_diffusion.h5\"\n",
    "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    ckpt_path,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "diffusion_ft_trainer.fit(training_dataset, epochs=epochs, callbacks=[ckpt_callback])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
