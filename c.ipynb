{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamal/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#TODO document jupyter\n",
    "import torch\n",
    "import pickle\n",
    "import configparser\n",
    "import sqlite3 as sl\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'configuration.txt'\n",
    "configParser.read(configFilePath)\n",
    "datasetPathDatabase =  configParser.get('COMMON', 'datasetPathDatabase') + '/dataset.db'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sl.connect(datasetPathDatabase)\n",
    "data = con.execute(\"SELECT V.ID, V.VIDEO_PATH, V.AGE, V.ETHNICITY, V.GENDER, A.SPEAKER_EMB, A.LANG, F.FACE_PATH  FROM VIDEO V INNER JOIN AUDIO A ON V.ID = A.VIDEO_ID INNER JOIN FACE F ON V.ID = F.VIDEO_ID\")\n",
    "dataGotten = data.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.DataFrame(dataGotten,columns = ['ID','VIDEO_PATH','AGE','ETHNICITY','GENDER','SPEAKER_EMB','LANG','FACE_PATH'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [[[tensor(4.8478, device='cuda:0'), tensor(2.4...\n",
      "Name: SPEAKER_EMB, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['SPEAKER_EMB'] = df['SPEAKER_EMB'].apply(lambda x:pickle.loads(x))\n",
    "print(df.head(1)['SPEAKER_EMB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 192])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df.head(1)['SPEAKER_EMB'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# now we expand to size (7, 11) by appending a row of 0s at pos 0 and pos 6, \n",
    "# and a column of 0s at pos 10\n",
    "#df['SPEAKER_EMB'] = df['SPEAKER_EMB'].apply(lambda x:print(x.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source = torch.rand((1,1,192))\n",
    "# now we expand to size (7, 11) by appending a row of 0s at pos 0 and pos 6, \n",
    "# and a column of 0s at pos 10\n",
    "# (4, 256, 768)\n",
    "#print(source.shape)\n",
    "#result = F.pad(input=source, pad=(1, 575), mode='constant', value=0)\n",
    "#print(result.shape)\n",
    "#result2 = result.unsqueeze(1).repeat(1,1,1,1)\n",
    "#print(result2.shape)\n",
    "#result3 = result2.squeeze()\n",
    "#print(result3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# now we expand to size (7, 11) by appending a row of 0s at pos 0 and pos 6, \n",
    "# and a column of 0s at pos 10\n",
    "with torch.no_grad():\n",
    "    df['SPEAKER_EMB'] = df['SPEAKER_EMB'].apply(lambda x:F.pad(input=x, pad=(1, 575), mode='constant', value=0) )\n",
    "    df['SPEAKER_EMB'] = df['SPEAKER_EMB'].apply(lambda x:x.unsqueeze(1).repeat(1,1,2,1) )\n",
    "    df['SPEAKER_EMB'] = df['SPEAKER_EMB'].apply(lambda x:x.squeeze() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)['SPEAKER_EMB'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_arr = df.head(1)['SPEAKER_EMB'][0].cpu().detach().numpy()\n",
    "np_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SPEAKER_EMB'] = df['SPEAKER_EMB'].apply(lambda x:x.cpu().detach().numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.head(1)['SPEAKER_EMB'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "face_path = df.head(1)['FACE_PATH'][0]\n",
    "im = Image.open(face_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(im).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.load() # required for png.split()\n",
    "\n",
    "im2 = Image.new(\"RGB\", im.size, (255, 255, 255))\n",
    "im2.paste(im, mask=im.split()[3]) # 3 is the alpha channel\n",
    "im2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im3 = np.array(im2)\n",
    "np.array(im3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 32, 32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im4 = np.rollaxis(im3,2)\n",
    "im4.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torchvision.transforms as T\n",
    "#from torchvision.io import read_image\n",
    "#x = T.ToPILImage()(im4)\n",
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImage(face_path):\n",
    "    im = Image.open(face_path)\n",
    "    im.load() # required for png.split()\n",
    "\n",
    "    im2 = Image.new(\"RGB\", im.size, (255, 255, 255))\n",
    "    im2.paste(im, mask=im.split()[3]) # 3 is the alpha channel\n",
    "    im3 = np.array(im2)\n",
    "    im4 = np.rollaxis(im3,2)\n",
    "    return im4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IMAGE'] = df['FACE_PATH'].apply(lambda x:getImage(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 32, 32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)['IMAGE'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.       , 4.8477864, 2.436525 , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 4.8477864, 2.436525 , ..., 0.       , 0.       ,\n",
       "        0.       ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)['SPEAKER_EMB'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109037/3490873246.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1673752754831/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  input_tensor = torch.tensor(df['SPEAKER_EMB']).cuda()\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input_tensor = torch.tensor(df['SPEAKER_EMB']).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1008, 2, 768])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  4.8478,  2.4365,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  4.8478,  2.4365,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  4.8478,  2.4365,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  4.8478,  2.4365,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  4.8478,  2.4365,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  4.8478,  2.4365,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0000,  0.0887, -5.2107,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0887, -5.2107,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0887, -5.2107,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0887, -5.2107,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0887, -5.2107,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0887, -5.2107,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor = torch.tensor(df['IMAGE']).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1008, 3, 32, 32])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1008, 3, 32, 32])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from imagen_pytorch import Unet, Imagen\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n"
     ]
    }
   ],
   "source": [
    "unet1 = Unet(\n",
    "    dim = 16,\n",
    "    cond_dim = 256,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = 3,\n",
    "    layer_attns = (False, True, True, True),\n",
    "    layer_cross_attns = (False, True, True, True)\n",
    ")\n",
    "\n",
    "#unet2 = Unet(\n",
    "#    dim = 32,\n",
    "#    cond_dim = 512,\n",
    "#    dim_mults = (1, 2, 4, 8),\n",
    "#    num_resnet_blocks = (2, 4, 8, 8),\n",
    "#    layer_attns = (False, False, False, True),\n",
    "#    layer_cross_attns = (False, False, False, True)\n",
    "#)\n",
    "\n",
    "# imagen, which contains the unets above (base unet and super resoluting ones)\n",
    "\n",
    "imagen = Imagen(\n",
    "    unets = (unet1),#, unet2),\n",
    "    image_sizes = (32),\n",
    "    timesteps = 100,\n",
    "    cond_drop_prob = 0.1\n",
    ").cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1008, 2, 768])\n",
      "torch.Size([1008, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor.shape)\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_embeds = torch.randn(4, 128, 768).cuda()\n",
    "#print(text_embeds.shape)\n",
    "\n",
    "#images = torch.randn(4, 3, 256, 256).cuda()\n",
    "#print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1008, 2, 768])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'max_batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#for i in (1):#, 2):\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#    loss = imagen(images, text_embeds = text_embeds, unet_number = 1)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#    loss.backward()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m loss \u001b[39m=\u001b[39m imagen(output_tensor, text_embeds \u001b[39m=\u001b[39;49m input_tensor, unet_number \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,max_batch_size \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m<@beartype(imagen_pytorch.imagen_pytorch.Imagen.forward) at 0x7fa2cc1890d0>:65\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(__beartype_func, __beartype_conf, __beartype_get_violation, __beartype_object_140337186050432, __beartype_getrandbits, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/imagen_pytorch.py:2705\u001b[0m, in \u001b[0;36mImagen.forward\u001b[0;34m(self, images, unet, texts, text_embeds, text_masks, unet_number, cond_images, **kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m         lowres_aug_times \u001b[39m=\u001b[39m repeat(lowres_aug_time, \u001b[39m'\u001b[39m\u001b[39m1 -> b\u001b[39m\u001b[39m'\u001b[39m, b \u001b[39m=\u001b[39m b)\n\u001b[1;32m   2703\u001b[0m images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize_to(images, target_image_size, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mframes_to_resize_kwargs(target_frame_size))\n\u001b[0;32m-> 2705\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_losses(unet, images, times, text_embeds \u001b[39m=\u001b[39;49m text_embeds, text_mask \u001b[39m=\u001b[39;49m text_masks, cond_images \u001b[39m=\u001b[39;49m cond_images, noise_scheduler \u001b[39m=\u001b[39;49m noise_scheduler, lowres_cond_img \u001b[39m=\u001b[39;49m lowres_cond_img, lowres_aug_times \u001b[39m=\u001b[39;49m lowres_aug_times, pred_objective \u001b[39m=\u001b[39;49m pred_objective, p2_loss_weight_gamma \u001b[39m=\u001b[39;49m p2_loss_weight_gamma, random_crop_size \u001b[39m=\u001b[39;49m random_crop_size, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m<@beartype(imagen_pytorch.imagen_pytorch.Imagen.p_losses) at 0x7fa2cc180f70>:35\u001b[0m, in \u001b[0;36mp_losses\u001b[0;34m(__beartype_func, __beartype_conf, __beartype_get_violation, __beartype_object_140337186050432, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/ds2f_m_i/lib/python3.8/site-packages/imagen_pytorch/imagen_pytorch.py:2578\u001b[0m, in \u001b[0;36mImagen.p_losses\u001b[0;34m(self, unet, x_start, times, noise_scheduler, lowres_cond_img, lowres_aug_times, text_embeds, text_mask, cond_images, noise, times_next, pred_objective, p2_loss_weight_gamma, random_crop_size, **kwargs)\u001b[0m\n\u001b[1;32m   2574\u001b[0m         unet_kwargs \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39munet_kwargs, \u001b[39m'\u001b[39m\u001b[39mself_cond\u001b[39m\u001b[39m'\u001b[39m: x_start}\n\u001b[1;32m   2576\u001b[0m \u001b[39m# get prediction\u001b[39;00m\n\u001b[0;32m-> 2578\u001b[0m pred \u001b[39m=\u001b[39m unet\u001b[39m.\u001b[39;49mforward(\n\u001b[1;32m   2579\u001b[0m     x_noisy,\n\u001b[1;32m   2580\u001b[0m     noise_cond,\n\u001b[1;32m   2581\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munet_kwargs\n\u001b[1;32m   2582\u001b[0m )\n\u001b[1;32m   2584\u001b[0m \u001b[39m# prediction objective\u001b[39;00m\n\u001b[1;32m   2586\u001b[0m \u001b[39mif\u001b[39;00m pred_objective \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnoise\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'max_batch_size'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#for i in (1):#, 2):\n",
    "#    loss = imagen(images, text_embeds = text_embeds, unet_number = 1)\n",
    "#    loss.backward()\n",
    "loss = imagen(output_tensor, text_embeds = input_tensor, unet_number = 1,max_batch_size = 1)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 100/100 [00:15<00:00,  6.33it/s]\n",
      "1it [00:15, 15.98s/it]\n"
     ]
    }
   ],
   "source": [
    "images = imagen.sample(text_embeds=input_tensor, cond_scale = 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(eog:69516): Gtk-WARNING **: 11:03:52.638: Could not load a pixbuf from icon theme.\n",
      "This may indicate that pixbuf loaders or the mime database could not be found.\n"
     ]
    }
   ],
   "source": [
    "im_numpy = images.cpu().numpy()\n",
    "im1 = np.transpose(im_numpy[0], (1,2,0))\n",
    "im1f  = (im1 * 255 / np.max(im1)).astype('uint8')\n",
    "x = T.ToPILImage()(im1f)\n",
    "x.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2f_m_i",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
